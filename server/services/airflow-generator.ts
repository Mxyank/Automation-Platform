interface Task {
  id: string;
  name: string;
  type: string;
  operator: string;
  pythonCallable?: string;
  bashCommand?: string;
  sqlQuery?: string;
  dependencies: string[];
}

interface AirflowConfig {
  dagId: string;
  description: string;
  schedule: string;
  startDate: string;
  catchup: boolean;
  maxActiveRuns: number;
  retries: number;
  retryDelay: number;
  owner: string;
  email: string;
  emailOnFailure: boolean;
  emailOnRetry: boolean;
  tags: string[];
  tasks: Task[];
  defaultArgs: {
    depends_on_past: boolean;
    wait_for_downstream: boolean;
  };
}

export function generateAirflowDAG(config: AirflowConfig): string {
  const {
    dagId,
    description,
    schedule,
    startDate,
    catchup,
    maxActiveRuns,
    retries,
    retryDelay,
    owner,
    email,
    emailOnFailure,
    emailOnRetry,
    tags,
    tasks,
    defaultArgs
  } = config;

  const sections: string[] = [];

  const operators = new Set<string>();
  tasks.forEach(task => {
    operators.add(task.operator);
  });

  const imports = [
    'from datetime import datetime, timedelta',
    'from airflow import DAG'
  ];

  if (operators.has('PythonOperator') || operators.has('BranchPythonOperator')) {
    imports.push('from airflow.operators.python import PythonOperator, BranchPythonOperator');
  }
  if (operators.has('BashOperator')) {
    imports.push('from airflow.operators.bash import BashOperator');
  }
  if (operators.has('EmptyOperator')) {
    imports.push('from airflow.operators.empty import EmptyOperator');
  }
  if (operators.has('PostgresOperator')) {
    imports.push('from airflow.providers.postgres.operators.postgres import PostgresOperator');
  }
  if (operators.has('SnowflakeOperator') || operators.has('S3ToSnowflakeOperator')) {
    imports.push('from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator');
    if (operators.has('S3ToSnowflakeOperator')) {
      imports.push('from airflow.providers.snowflake.transfers.s3_to_snowflake import S3ToSnowflakeOperator');
    }
  }
  if (operators.has('BigQueryOperator')) {
    imports.push('from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator');
  }
  if (operators.has('SparkSubmitOperator')) {
    imports.push('from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator');
  }
  if (operators.has('TriggerDagRunOperator')) {
    imports.push('from airflow.operators.trigger_dagrun import TriggerDagRunOperator');
  }

  sections.push(`"""
${description || `Airflow DAG: ${dagId}`}

Generated by CloudForge DevOps Platform
Generated: ${new Date().toISOString()}

DAG ID: ${dagId}
Schedule: ${schedule}
Owner: ${owner || 'airflow'}
"""

${imports.join('\n')}
`);

  const emailList = email ? `['${email}']` : '[]';
  
  sections.push(`
# Default arguments for all tasks
default_args = {
    'owner': '${owner || 'airflow'}',
    'depends_on_past': ${defaultArgs?.depends_on_past ? 'True' : 'False'},
    'wait_for_downstream': ${defaultArgs?.wait_for_downstream ? 'True' : 'False'},
    'email': ${emailList},
    'email_on_failure': ${emailOnFailure ? 'True' : 'False'},
    'email_on_retry': ${emailOnRetry ? 'True' : 'False'},
    'retries': ${retries || 2},
    'retry_delay': timedelta(minutes=${retryDelay || 5}),
}
`);

  const pythonFunctions: string[] = [];
  tasks.forEach(task => {
    if (task.operator === 'PythonOperator' || task.operator === 'BranchPythonOperator') {
      const funcName = task.pythonCallable || task.name;
      pythonFunctions.push(`
def ${funcName}(**kwargs):
    """
    Task: ${task.name}
    
    TODO: Implement your ${task.operator === 'BranchPythonOperator' ? 'branching logic' : 'task logic'} here
    
    Available kwargs:
        - ds: execution date as string (YYYY-MM-DD)
        - ds_nodash: execution date without dashes
        - ts: execution timestamp
        - execution_date: datetime object
        - prev_execution_date: previous execution date
        - next_execution_date: next execution date
        - dag: the DAG object
        - task: the Task object
        - task_instance: TaskInstance object (ti)
        - params: user-defined params from DAG definition
        - var: Airflow Variables
        - conn: Airflow Connections
    
    Example using XCom:
        # Push value to XCom
        kwargs['ti'].xcom_push(key='my_key', value='my_value')
        
        # Pull value from XCom
        value = kwargs['ti'].xcom_pull(task_ids='previous_task', key='my_key')
    """
    print(f"Executing ${task.name}...")
    
    # Your implementation here
    ${task.operator === 'BranchPythonOperator' ? `
    # Return the task_id to execute next
    # return 'task_id_if_true'
    # return 'task_id_if_false'
    pass` : `
    # Example: Process data
    # data = fetch_data()
    # result = transform_data(data)
    # return result
    pass`}
`);
    }
  });

  if (pythonFunctions.length > 0) {
    sections.push(`
# ============================================================
# Python Callables for PythonOperator Tasks
# ============================================================
${pythonFunctions.join('\n')}
`);
  }

  const tagsStr = tags && tags.length > 0 ? `[${tags.map(t => `'${t}'`).join(', ')}]` : "['data-pipeline']";
  const startDateFormatted = startDate || new Date().toISOString().split('T')[0];
  const [year, month, day] = startDateFormatted.split('-');

  sections.push(`
# ============================================================
# DAG Definition
# ============================================================

with DAG(
    dag_id='${dagId || 'my_dag'}',
    default_args=default_args,
    description='${(description || '').replace(/'/g, "\\'")}',
    schedule_interval='${schedule || '@daily'}',
    start_date=datetime(${year}, ${parseInt(month)}, ${parseInt(day)}),
    catchup=${catchup ? 'True' : 'False'},
    max_active_runs=${maxActiveRuns || 1},
    tags=${tagsStr},
) as dag:
`);

  const taskDefinitions: string[] = [];
  const taskIdMap = new Map<string, string>();
  
  tasks.forEach(task => {
    taskIdMap.set(task.id, task.name);
  });

  tasks.forEach(task => {
    let taskDef = '';
    const taskVarName = task.name.replace(/-/g, '_');

    switch (task.operator) {
      case 'PythonOperator':
        taskDef = `
    ${taskVarName} = PythonOperator(
        task_id='${task.name}',
        python_callable=${task.pythonCallable || task.name},
        provide_context=True,
    )`;
        break;

      case 'BranchPythonOperator':
        taskDef = `
    ${taskVarName} = BranchPythonOperator(
        task_id='${task.name}',
        python_callable=${task.pythonCallable || task.name},
        provide_context=True,
    )`;
        break;

      case 'BashOperator':
        const bashCmd = (task.bashCommand || 'echo "Hello World"').replace(/'/g, "\\'");
        taskDef = `
    ${taskVarName} = BashOperator(
        task_id='${task.name}',
        bash_command='${bashCmd}',
    )`;
        break;

      case 'EmptyOperator':
        taskDef = `
    ${taskVarName} = EmptyOperator(
        task_id='${task.name}',
    )`;
        break;

      case 'PostgresOperator':
        const pgSql = (task.sqlQuery || 'SELECT 1;').replace(/'/g, "\\'");
        taskDef = `
    ${taskVarName} = PostgresOperator(
        task_id='${task.name}',
        postgres_conn_id='postgres_default',  # Configure in Airflow Connections
        sql='''${pgSql}''',
    )`;
        break;

      case 'SnowflakeOperator':
        const sfSql = (task.sqlQuery || 'SELECT CURRENT_TIMESTAMP();').replace(/'/g, "\\'");
        taskDef = `
    ${taskVarName} = SnowflakeOperator(
        task_id='${task.name}',
        snowflake_conn_id='snowflake_default',  # Configure in Airflow Connections
        sql='''${sfSql}''',
        warehouse='COMPUTE_WH',  # Update with your warehouse
    )`;
        break;

      case 'S3ToSnowflakeOperator':
        taskDef = `
    ${taskVarName} = S3ToSnowflakeOperator(
        task_id='${task.name}',
        snowflake_conn_id='snowflake_default',
        s3_keys=['your-file.csv'],  # Update with your S3 keys
        table='YOUR_TABLE',  # Update with your table name
        schema='PUBLIC',
        stage='YOUR_STAGE',  # Update with your Snowflake stage
        file_format='(TYPE=CSV, FIELD_DELIMITER=",")',
    )`;
        break;

      case 'BigQueryOperator':
        const bqSql = (task.sqlQuery || 'SELECT 1').replace(/'/g, "\\'");
        taskDef = `
    ${taskVarName} = BigQueryInsertJobOperator(
        task_id='${task.name}',
        gcp_conn_id='google_cloud_default',  # Configure in Airflow Connections
        configuration={
            'query': {
                'query': '''${bqSql}''',
                'useLegacySql': False,
            }
        },
    )`;
        break;

      case 'SparkSubmitOperator':
        taskDef = `
    ${taskVarName} = SparkSubmitOperator(
        task_id='${task.name}',
        application='/path/to/your/spark_app.py',  # Update with your Spark app path
        conn_id='spark_default',  # Configure in Airflow Connections
        application_args=['--arg1', 'value1'],  # Update with your arguments
        conf={
            'spark.executor.memory': '2g',
            'spark.driver.memory': '1g',
        },
    )`;
        break;

      case 'TriggerDagRunOperator':
        taskDef = `
    ${taskVarName} = TriggerDagRunOperator(
        task_id='${task.name}',
        trigger_dag_id='target_dag_id',  # Update with target DAG ID
        wait_for_completion=True,
        poke_interval=60,
    )`;
        break;

      default:
        taskDef = `
    ${taskVarName} = EmptyOperator(
        task_id='${task.name}',
    )`;
    }

    taskDefinitions.push(taskDef);
  });

  sections.push(`    # ========================================
    # Task Definitions
    # ========================================
${taskDefinitions.join('\n')}
`);

  const dependencies: string[] = [];
  tasks.forEach(task => {
    if (task.dependencies && task.dependencies.length > 0) {
      const taskVarName = task.name.replace(/-/g, '_');
      task.dependencies.forEach(depId => {
        const depName = taskIdMap.get(depId);
        if (depName) {
          const depVarName = depName.replace(/-/g, '_');
          dependencies.push(`    ${depVarName} >> ${taskVarName}`);
        }
      });
    }
  });

  if (dependencies.length > 0) {
    sections.push(`
    # ========================================
    # Task Dependencies
    # ========================================
${dependencies.join('\n')}
`);
  } else if (tasks.length > 1) {
    const taskNames = tasks.map(t => t.name.replace(/-/g, '_'));
    sections.push(`
    # ========================================
    # Task Dependencies
    # No dependencies defined - tasks will run in parallel
    # To define sequential execution, uncomment below:
    # ========================================
    # ${taskNames.join(' >> ')}
`);
  }

  sections.push(`

# ============================================================
# DAG Documentation
# ============================================================
"""
How to use this DAG:

1. Save this file to your Airflow dags folder (usually ~/airflow/dags/)

2. Configure required connections in Airflow UI:
   - Go to Admin -> Connections
   - Add connections for any operators that require them:
     * postgres_default: PostgreSQL connection
     * snowflake_default: Snowflake connection
     * google_cloud_default: Google Cloud connection
     * spark_default: Spark connection

3. Configure required variables (if any):
   - Go to Admin -> Variables
   - Add any required variables

4. Enable the DAG in Airflow UI

5. Trigger manually or wait for scheduled run

Monitoring:
- View task logs in Airflow UI
- Check task instance details for execution info
- Use XCom for passing data between tasks

Troubleshooting:
- Check task logs for errors
- Verify connections are configured correctly
- Ensure all dependencies are installed
- Check scheduler logs if DAG doesn't appear

Best Practices:
- Use meaningful task IDs
- Keep tasks atomic and idempotent
- Use XCom sparingly for small data
- Set appropriate retries and retry_delay
- Use pools to limit concurrent tasks
- Add documentation and comments
"""
`);

  return sections.join('');
}
