var __defProp = Object.defineProperty;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __esm = (fn, res) => function __init() {
  return fn && (res = (0, fn[__getOwnPropNames(fn)[0]])(fn = 0)), res;
};
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};

// shared/schema.ts
var schema_exports = {};
__export(schema_exports, {
  adminActivityLog: () => adminActivityLog,
  adminActivityLogRelations: () => adminActivityLogRelations,
  domainConfigs: () => domainConfigs,
  domainConfigsRelations: () => domainConfigsRelations,
  impersonationLogs: () => impersonationLogs,
  impersonationLogsRelations: () => impersonationLogsRelations,
  incidentMessages: () => incidentMessages,
  incidentMessagesRelations: () => incidentMessagesRelations,
  incidents: () => incidents,
  incidentsRelations: () => incidentsRelations,
  insertAdminActivityLogSchema: () => insertAdminActivityLogSchema,
  insertDomainConfigSchema: () => insertDomainConfigSchema,
  insertIncidentMessageSchema: () => insertIncidentMessageSchema,
  insertIncidentSchema: () => insertIncidentSchema,
  insertProjectSchema: () => insertProjectSchema,
  insertPushSubscriptionSchema: () => insertPushSubscriptionSchema,
  insertSecurityLogSchema: () => insertSecurityLogSchema,
  insertSiteSettingSchema: () => insertSiteSettingSchema,
  insertSubscriptionSchema: () => insertSubscriptionSchema,
  insertUsageSchema: () => insertUsageSchema,
  insertUserSchema: () => insertUserSchema,
  payments: () => payments,
  paymentsRelations: () => paymentsRelations,
  projects: () => projects,
  projectsRelations: () => projectsRelations,
  pushSubscriptions: () => pushSubscriptions,
  pushSubscriptionsRelations: () => pushSubscriptionsRelations,
  securityLogs: () => securityLogs,
  securityLogsRelations: () => securityLogsRelations,
  siteSettings: () => siteSettings,
  subscriptions: () => subscriptions,
  subscriptionsRelations: () => subscriptionsRelations,
  systemMetrics: () => systemMetrics,
  usage: () => usage,
  usageRelations: () => usageRelations,
  users: () => users,
  usersRelations: () => usersRelations
});
import { pgTable, text, serial, integer, boolean, timestamp, jsonb } from "drizzle-orm/pg-core";
import { createInsertSchema } from "drizzle-zod";
import { relations } from "drizzle-orm";
var users, domainConfigs, siteSettings, incidents, incidentMessages, projects, usage, payments, subscriptions, securityLogs, pushSubscriptions, systemMetrics, adminActivityLog, impersonationLogs, usersRelations, domainConfigsRelations, incidentsRelations, incidentMessagesRelations, subscriptionsRelations, projectsRelations, usageRelations, paymentsRelations, securityLogsRelations, pushSubscriptionsRelations, adminActivityLogRelations, impersonationLogsRelations, insertUserSchema, insertProjectSchema, insertUsageSchema, insertSubscriptionSchema, insertSecurityLogSchema, insertPushSubscriptionSchema, insertAdminActivityLogSchema, insertDomainConfigSchema, insertSiteSettingSchema, insertIncidentSchema, insertIncidentMessageSchema;
var init_schema = __esm({
  "shared/schema.ts"() {
    "use strict";
    users = pgTable("users", {
      id: serial("id").primaryKey(),
      username: text("username").notNull().unique(),
      email: text("email").notNull().unique(),
      password: text("password"),
      // Make password optional for OAuth users
      credits: integer("credits").default(2).notNull(),
      // New users get 2 free credits
      razorpayCustomerId: text("razorpay_customer_id"),
      googleId: text("google_id").unique(),
      // Google OAuth ID
      avatar: text("avatar"),
      // Profile picture from Google
      provider: text("provider").default("local").notNull(),
      // 'local' or 'google'
      isPremium: boolean("is_premium").default(false).notNull(),
      // Premium subscription status
      premiumExpiresAt: timestamp("premium_expires_at"),
      // Premium expiration date
      hasSeenTour: boolean("has_seen_tour").default(false).notNull(),
      // Track if user has seen the onboarding tour
      isAdmin: boolean("is_admin").default(false).notNull(),
      // Admin role for platform management
      adminGrantedBy: text("admin_granted_by"),
      // Email of admin who granted access
      adminGrantedAt: timestamp("admin_granted_at"),
      // When admin access was granted
      primaryDomain: text("primary_domain").default("devops").notNull(),
      // 'devops', 'data-engineering', 'cybersecurity'
      isBanned: boolean("is_banned").default(false).notNull(),
      // User ban status
      bannedAt: timestamp("banned_at"),
      // When user was banned
      banReason: text("ban_reason"),
      // Reason for ban
      canImpersonate: boolean("can_impersonate").default(false).notNull(),
      // Can switch to other user accounts
      lastActiveAt: timestamp("last_active_at"),
      // Last activity timestamp
      createdAt: timestamp("created_at").defaultNow().notNull()
    });
    domainConfigs = pgTable("domain_configs", {
      id: serial("id").primaryKey(),
      domain: text("domain").notNull().unique(),
      // 'devops', 'dataeng', 'security'
      isEnabled: boolean("is_enabled").default(false).notNull(),
      comingSoonMessage: text("coming_soon_message"),
      updatedBy: integer("updated_by").references(() => users.id, { onDelete: "set null" }),
      updatedAt: timestamp("updated_at").defaultNow().notNull()
    });
    siteSettings = pgTable("site_settings", {
      id: serial("id").primaryKey(),
      key: text("key").notNull().unique(),
      value: boolean("value").default(false).notNull(),
      label: text("label").notNull(),
      description: text("description"),
      updatedBy: integer("updated_by").references(() => users.id, { onDelete: "set null" }),
      updatedAt: timestamp("updated_at").defaultNow().notNull()
    });
    incidents = pgTable("incidents", {
      id: serial("id").primaryKey(),
      userId: integer("user_id").notNull().references(() => users.id, { onDelete: "cascade" }),
      title: text("title").notNull(),
      description: text("description").notNull(),
      category: text("category").notNull(),
      // 'bug', 'abuse', 'billing', 'feature', 'other'
      priority: text("priority").default("medium").notNull(),
      // 'low', 'medium', 'high', 'critical'
      status: text("status").default("open").notNull(),
      // 'open', 'in_progress', 'resolved', 'closed', 'reopened'
      assigneeId: integer("assignee_id").references(() => users.id, { onDelete: "set null" }),
      resolution: text("resolution"),
      // Resolution notes from admin
      createdAt: timestamp("created_at").defaultNow().notNull(),
      updatedAt: timestamp("updated_at").defaultNow().notNull(),
      closedAt: timestamp("closed_at")
    });
    incidentMessages = pgTable("incident_messages", {
      id: serial("id").primaryKey(),
      incidentId: integer("incident_id").notNull().references(() => incidents.id, { onDelete: "cascade" }),
      authorId: integer("author_id").notNull().references(() => users.id, { onDelete: "cascade" }),
      authorRole: text("author_role").notNull(),
      // 'user', 'admin'
      message: text("message").notNull(),
      createdAt: timestamp("created_at").defaultNow().notNull()
    });
    projects = pgTable("projects", {
      id: serial("id").primaryKey(),
      userId: integer("user_id").notNull().references(() => users.id, { onDelete: "cascade" }),
      name: text("name").notNull(),
      type: text("type").notNull(),
      // 'api', 'docker', 'ci-cd'
      config: jsonb("config").notNull(),
      generatedCode: text("generated_code"),
      status: text("status").default("draft").notNull(),
      // 'draft', 'active', 'deploying'
      createdAt: timestamp("created_at").defaultNow().notNull(),
      updatedAt: timestamp("updated_at").defaultNow().notNull()
    });
    usage = pgTable("usage", {
      id: serial("id").primaryKey(),
      userId: integer("user_id").notNull().references(() => users.id, { onDelete: "cascade" }),
      featureName: text("feature_name").notNull(),
      // 'api_generation', 'docker_generation', 'ai_assistance'
      usedCount: integer("used_count").default(0).notNull(),
      lastUsed: timestamp("last_used").defaultNow().notNull()
    });
    payments = pgTable("payments", {
      id: serial("id").primaryKey(),
      userId: integer("user_id").notNull().references(() => users.id, { onDelete: "cascade" }),
      razorpayPaymentId: text("razorpay_payment_id"),
      amount: integer("amount").notNull(),
      // in paise
      credits: integer("credits").notNull(),
      status: text("status").notNull(),
      // 'pending', 'completed', 'failed'
      createdAt: timestamp("created_at").defaultNow().notNull()
    });
    subscriptions = pgTable("subscriptions", {
      id: serial("id").primaryKey(),
      userId: integer("user_id").notNull().references(() => users.id, { onDelete: "cascade" }),
      planType: text("plan_type").notNull(),
      // 'ai-pro-monthly', 'ai-pro-annual'
      status: text("status").notNull().default("active"),
      // 'active', 'cancelled', 'expired'
      razorpaySubscriptionId: text("razorpay_subscription_id"),
      startDate: timestamp("start_date").defaultNow().notNull(),
      endDate: timestamp("end_date").notNull(),
      autoRenew: boolean("auto_renew").default(true).notNull(),
      createdAt: timestamp("created_at").defaultNow().notNull()
    });
    securityLogs = pgTable("security_logs", {
      id: serial("id").primaryKey(),
      userId: integer("user_id").references(() => users.id, { onDelete: "set null" }),
      action: text("action").notNull(),
      // 'login', 'logout', 'failed_login', 'password_change', 'api_access', 'admin_action'
      ipAddress: text("ip_address"),
      userAgent: text("user_agent"),
      details: jsonb("details"),
      // Additional context
      severity: text("severity").default("info").notNull(),
      // 'info', 'warning', 'critical'
      createdAt: timestamp("created_at").defaultNow().notNull()
    });
    pushSubscriptions = pgTable("push_subscriptions", {
      id: serial("id").primaryKey(),
      userId: integer("user_id").notNull().references(() => users.id, { onDelete: "cascade" }),
      endpoint: text("endpoint").notNull(),
      p256dh: text("p256dh").notNull(),
      // Public key
      auth: text("auth").notNull(),
      // Auth secret
      createdAt: timestamp("created_at").defaultNow().notNull()
    });
    systemMetrics = pgTable("system_metrics", {
      id: serial("id").primaryKey(),
      metricName: text("metric_name").notNull(),
      metricValue: text("metric_value").notNull(),
      labels: jsonb("labels"),
      recordedAt: timestamp("recorded_at").defaultNow().notNull()
    });
    adminActivityLog = pgTable("admin_activity_log", {
      id: serial("id").primaryKey(),
      adminId: integer("admin_id").notNull().references(() => users.id, { onDelete: "cascade" }),
      action: text("action").notNull(),
      // 'grant_admin', 'revoke_admin', 'view_users', 'view_logs', etc.
      targetUserId: integer("target_user_id").references(() => users.id, { onDelete: "set null" }),
      details: jsonb("details"),
      createdAt: timestamp("created_at").defaultNow().notNull()
    });
    impersonationLogs = pgTable("impersonation_logs", {
      id: serial("id").primaryKey(),
      impersonatorId: integer("impersonator_id").notNull().references(() => users.id, { onDelete: "cascade" }),
      targetUserId: integer("target_user_id").notNull().references(() => users.id, { onDelete: "cascade" }),
      reason: text("reason"),
      action: text("action").notNull(),
      // 'start', 'end'
      ipAddress: text("ip_address"),
      userAgent: text("user_agent"),
      createdAt: timestamp("created_at").defaultNow().notNull()
    });
    usersRelations = relations(users, ({ many }) => ({
      projects: many(projects),
      usage: many(usage),
      payments: many(payments),
      subscriptions: many(subscriptions),
      securityLogs: many(securityLogs),
      pushSubscriptions: many(pushSubscriptions),
      adminActivityLogs: many(adminActivityLog),
      incidents: many(incidents),
      incidentMessages: many(incidentMessages)
    }));
    domainConfigsRelations = relations(domainConfigs, ({ one }) => ({
      updatedByUser: one(users, {
        fields: [domainConfigs.updatedBy],
        references: [users.id]
      })
    }));
    incidentsRelations = relations(incidents, ({ one, many }) => ({
      user: one(users, {
        fields: [incidents.userId],
        references: [users.id]
      }),
      assignee: one(users, {
        fields: [incidents.assigneeId],
        references: [users.id]
      }),
      messages: many(incidentMessages)
    }));
    incidentMessagesRelations = relations(incidentMessages, ({ one }) => ({
      incident: one(incidents, {
        fields: [incidentMessages.incidentId],
        references: [incidents.id]
      }),
      author: one(users, {
        fields: [incidentMessages.authorId],
        references: [users.id]
      })
    }));
    subscriptionsRelations = relations(subscriptions, ({ one }) => ({
      user: one(users, {
        fields: [subscriptions.userId],
        references: [users.id]
      })
    }));
    projectsRelations = relations(projects, ({ one }) => ({
      user: one(users, {
        fields: [projects.userId],
        references: [users.id]
      })
    }));
    usageRelations = relations(usage, ({ one }) => ({
      user: one(users, {
        fields: [usage.userId],
        references: [users.id]
      })
    }));
    paymentsRelations = relations(payments, ({ one }) => ({
      user: one(users, {
        fields: [payments.userId],
        references: [users.id]
      })
    }));
    securityLogsRelations = relations(securityLogs, ({ one }) => ({
      user: one(users, {
        fields: [securityLogs.userId],
        references: [users.id]
      })
    }));
    pushSubscriptionsRelations = relations(pushSubscriptions, ({ one }) => ({
      user: one(users, {
        fields: [pushSubscriptions.userId],
        references: [users.id]
      })
    }));
    adminActivityLogRelations = relations(adminActivityLog, ({ one }) => ({
      admin: one(users, {
        fields: [adminActivityLog.adminId],
        references: [users.id]
      }),
      targetUser: one(users, {
        fields: [adminActivityLog.targetUserId],
        references: [users.id]
      })
    }));
    impersonationLogsRelations = relations(impersonationLogs, ({ one }) => ({
      impersonator: one(users, {
        fields: [impersonationLogs.impersonatorId],
        references: [users.id]
      }),
      targetUser: one(users, {
        fields: [impersonationLogs.targetUserId],
        references: [users.id]
      })
    }));
    insertUserSchema = createInsertSchema(users).pick({
      username: true,
      email: true,
      password: true
    });
    insertProjectSchema = createInsertSchema(projects).pick({
      name: true,
      type: true,
      config: true,
      generatedCode: true,
      status: true
    });
    insertUsageSchema = createInsertSchema(usage).pick({
      featureName: true
    });
    insertSubscriptionSchema = createInsertSchema(subscriptions).pick({
      planType: true,
      endDate: true
    });
    insertSecurityLogSchema = createInsertSchema(securityLogs).pick({
      userId: true,
      action: true,
      ipAddress: true,
      userAgent: true,
      details: true,
      severity: true
    });
    insertPushSubscriptionSchema = createInsertSchema(pushSubscriptions).pick({
      userId: true,
      endpoint: true,
      p256dh: true,
      auth: true
    });
    insertAdminActivityLogSchema = createInsertSchema(adminActivityLog).pick({
      adminId: true,
      action: true,
      targetUserId: true,
      details: true
    });
    insertDomainConfigSchema = createInsertSchema(domainConfigs).pick({
      domain: true,
      isEnabled: true,
      comingSoonMessage: true
    });
    insertSiteSettingSchema = createInsertSchema(siteSettings).pick({
      key: true,
      value: true,
      label: true,
      description: true
    });
    insertIncidentSchema = createInsertSchema(incidents).pick({
      title: true,
      description: true,
      category: true,
      priority: true
    });
    insertIncidentMessageSchema = createInsertSchema(incidentMessages).pick({
      incidentId: true,
      message: true
    });
  }
});

// server/db.ts
import pg from "pg";
import { drizzle } from "drizzle-orm/node-postgres";
var pool, db;
var init_db = __esm({
  "server/db.ts"() {
    "use strict";
    init_schema();
    pool = new pg.Pool({
      connectionString: process.env.DATABASE_URL,
      ssl: { rejectUnauthorized: false },
      // ğŸ”’ Neon + Docker tuning
      max: 3,
      connectionTimeoutMillis: 1e4,
      idleTimeoutMillis: 3e4
    });
    db = drizzle({ client: pool, schema: schema_exports });
  }
});

// server/logger.ts
import { createWriteStream, existsSync, mkdirSync } from "fs";
import { join } from "path";
var Logger, logger;
var init_logger = __esm({
  "server/logger.ts"() {
    "use strict";
    Logger = class {
      appLogStream;
      errorLogStream;
      accessLogStream;
      isServerless;
      constructor() {
        this.isServerless = process.env.VERCEL === "1" || process.env.NODE_ENV === "production";
        if (!this.isServerless) {
          try {
            const logsDir = join(process.cwd(), "logs");
            if (!existsSync(logsDir)) {
              mkdirSync(logsDir, { recursive: true });
            }
            this.appLogStream = createWriteStream(join(logsDir, "app.log"), { flags: "a" });
            this.errorLogStream = createWriteStream(join(logsDir, "error.log"), { flags: "a" });
            this.accessLogStream = createWriteStream(join(logsDir, "access.log"), { flags: "a" });
          } catch (error) {
            console.warn("Failed to initialize file logging (likely read-only fs), falling back to console only.");
            this.isServerless = true;
          }
        }
      }
      formatLog(entry) {
        const { timestamp: timestamp2, level, message, data, userId, endpoint, method, statusCode, responseTime } = entry;
        let logString = `[${timestamp2}] ${level}: ${message}`;
        if (userId) logString += ` | User: ${userId}`;
        if (method && endpoint) logString += ` | ${method} ${endpoint}`;
        if (statusCode) logString += ` | Status: ${statusCode}`;
        if (responseTime) logString += ` | ${responseTime}ms`;
        if (data) logString += ` | Data: ${JSON.stringify(data)}`;
        return logString + "\n";
      }
      writeLog(stream, entry) {
        const logString = this.formatLog(entry);
        if (!this.isServerless && stream) {
          stream.write(logString);
        }
        const colorCode = {
          ["ERROR" /* ERROR */]: "\x1B[31m",
          ["WARN" /* WARN */]: "\x1B[33m",
          ["INFO" /* INFO */]: "\x1B[36m",
          ["DEBUG" /* DEBUG */]: "\x1B[90m"
        };
        const consoleMethod = entry.level === "ERROR" /* ERROR */ ? console.error : entry.level === "WARN" /* WARN */ ? console.warn : console.log;
        consoleMethod(`${colorCode[entry.level]}${logString.trim()}\x1B[0m`);
      }
      info(message, data, userId) {
        const entry = {
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          level: "INFO" /* INFO */,
          message,
          data,
          userId
        };
        this.writeLog(this.appLogStream, entry);
      }
      error(message, error, userId) {
        const entry = {
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          level: "ERROR" /* ERROR */,
          message,
          data: error?.stack || error,
          userId
        };
        this.writeLog(this.errorLogStream, entry);
        this.writeLog(this.appLogStream, entry);
      }
      warn(message, data, userId) {
        const entry = {
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          level: "WARN" /* WARN */,
          message,
          data,
          userId
        };
        this.writeLog(this.appLogStream, entry);
      }
      debug(message, data, userId) {
        const entry = {
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          level: "DEBUG" /* DEBUG */,
          message,
          data,
          userId
        };
        this.writeLog(this.appLogStream, entry);
      }
      // Access logging for HTTP requests
      access(method, endpoint, statusCode, responseTime, userId, data) {
        const entry = {
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          level: "INFO" /* INFO */,
          message: "HTTP Request",
          method,
          endpoint,
          statusCode,
          responseTime,
          userId,
          data
        };
        this.writeLog(this.accessLogStream, entry);
      }
      // Database operation logging
      database(operation, table, data, userId) {
        this.info(`Database ${operation} on ${table}`, data, userId);
      }
      // API generation logging
      apiGeneration(projectName, framework, success, userId, error) {
        if (success) {
          this.info(`API generated successfully: ${projectName} (${framework})`, { projectName, framework }, userId);
        } else {
          this.error(`API generation failed: ${projectName} (${framework})`, error, userId);
        }
      }
      // AI service logging
      aiRequest(prompt, model, success, userId, error) {
        if (success) {
          this.info(`AI request successful: ${model}`, { prompt: prompt.substring(0, 100) + "..." }, userId);
        } else {
          this.error(`AI request failed: ${model}`, error, userId);
        }
      }
      // Payment logging
      payment(action, amount, currency, success, userId, error) {
        if (success) {
          this.info(`Payment ${action} successful`, { amount, currency }, userId);
        } else {
          this.error(`Payment ${action} failed`, error, userId);
        }
      }
      // Authentication logging
      auth(action, username, success, ip, error) {
        if (success) {
          this.info(`Authentication ${action} successful: ${username}`, { ip });
        } else {
          this.warn(`Authentication ${action} failed: ${username}`, { error: error?.message, ip });
        }
      }
    };
    logger = new Logger();
  }
});

// server/redis.ts
import { createClient } from "redis";
var RedisService, redis;
var init_redis = __esm({
  "server/redis.ts"() {
    "use strict";
    init_logger();
    RedisService = class {
      client;
      connected = false;
      connectionAttempted = false;
      errorLogged = false;
      constructor() {
        const redisUrl = process.env.REDIS_URL;
        this.client = createClient({
          url: redisUrl || "redis://localhost:6379",
          socket: {
            connectTimeout: 2e3,
            reconnectStrategy: (retries) => {
              if (retries > 1) {
                return false;
              }
              return 500;
            }
          }
        });
        this.setupEventHandlers();
      }
      setupEventHandlers() {
        this.client.on("connect", () => {
          logger.info("Redis client connected");
          this.connected = true;
          this.errorLogged = false;
        });
        this.client.on("ready", () => {
          logger.info("Redis client ready");
        });
        this.client.on("error", () => {
          this.connected = false;
        });
        this.client.on("end", () => {
          this.connected = false;
        });
      }
      async connect() {
        if (this.connectionAttempted) return;
        this.connectionAttempted = true;
        try {
          await this.client.connect();
        } catch (error) {
          if (!this.errorLogged) {
            logger.warn("Redis unavailable - running without cache (this is normal in development)");
            this.errorLogged = true;
          }
        }
      }
      async disconnect() {
        if (this.connected) {
          await this.client.disconnect();
        }
      }
      isConnected() {
        return this.connected;
      }
      // Cache user data
      async cacheUser(userId, userData, ttl = 3600) {
        if (!this.connected) return;
        try {
          const key = `user:${userId}`;
          await this.client.setEx(key, ttl, JSON.stringify(userData));
          logger.info(`Cached user data for user ${userId}`);
        } catch (error) {
          logger.error("Failed to cache user data", error);
        }
      }
      async getCachedUser(userId) {
        if (!this.connected) return null;
        try {
          const key = `user:${userId}`;
          const cached = await this.client.get(key);
          if (cached) {
            logger.info(`Retrieved cached user data for user ${userId}`);
            return JSON.parse(cached);
          }
          return null;
        } catch (error) {
          logger.error("Failed to retrieve cached user data", error);
          return null;
        }
      }
      // Cache project data
      async cacheUserProjects(userId, projects2, ttl = 1800) {
        if (!this.connected) return;
        try {
          const key = `projects:user:${userId}`;
          await this.client.setEx(key, ttl, JSON.stringify(projects2));
          logger.info(`Cached projects for user ${userId}`);
        } catch (error) {
          logger.error("Failed to cache projects", error);
        }
      }
      async getCachedUserProjects(userId) {
        if (!this.connected) return null;
        try {
          const key = `projects:user:${userId}`;
          const cached = await this.client.get(key);
          if (cached) {
            logger.info(`Retrieved cached projects for user ${userId}`);
            return JSON.parse(cached);
          }
          return null;
        } catch (error) {
          logger.error("Failed to retrieve cached projects", error);
          return null;
        }
      }
      // Cache usage data
      async cacheUserUsage(userId, usage2, ttl = 900) {
        if (!this.connected) return;
        try {
          const key = `usage:user:${userId}`;
          await this.client.setEx(key, ttl, JSON.stringify(usage2));
          logger.info(`Cached usage data for user ${userId}`);
        } catch (error) {
          logger.error("Failed to cache usage data", error);
        }
      }
      async getCachedUserUsage(userId) {
        if (!this.connected) return null;
        try {
          const key = `usage:user:${userId}`;
          const cached = await this.client.get(key);
          if (cached) {
            logger.info(`Retrieved cached usage data for user ${userId}`);
            return JSON.parse(cached);
          }
          return null;
        } catch (error) {
          logger.error("Failed to retrieve cached usage data", error);
          return null;
        }
      }
      // General purpose caching
      async set(key, value, ttl = 3600) {
        if (!this.connected) return;
        try {
          const serialized = typeof value === "string" ? value : JSON.stringify(value);
          await this.client.setEx(key, ttl, serialized);
          logger.info(`Set cache key: ${key}`);
        } catch (error) {
          logger.error("Failed to set cache", error);
        }
      }
      async get(key) {
        if (!this.connected) return null;
        try {
          const cached = await this.client.get(key);
          if (cached) {
            logger.info(`Retrieved cache key: ${key}`);
            try {
              return JSON.parse(cached);
            } catch {
              return cached;
            }
          }
          return null;
        } catch (error) {
          logger.error("Failed to get cache", error);
          return null;
        }
      }
      async delete(key) {
        if (!this.connected) return;
        try {
          await this.client.del(key);
          logger.info(`Deleted cache key: ${key}`);
        } catch (error) {
          logger.error("Failed to delete cache", error);
        }
      }
      // Clear user-related cache when data changes
      async clearUserCache(userId) {
        if (!this.connected) return;
        const keys = [
          `user:${userId}`,
          `projects:user:${userId}`,
          `usage:user:${userId}`
        ];
        for (const key of keys) {
          await this.delete(key);
        }
        logger.info(`Cleared all cache for user ${userId}`);
      }
      // Get Redis client for advanced operations
      getClient() {
        return this.client;
      }
    };
    redis = new RedisService();
  }
});

// server/storage.ts
import { eq, and, desc, count, gte, like, or, sum } from "drizzle-orm";
var DatabaseStorage, storage;
var init_storage = __esm({
  "server/storage.ts"() {
    "use strict";
    init_schema();
    init_db();
    init_redis();
    DatabaseStorage = class {
      async getUser(id) {
        const cachedUser = await redis.getCachedUser(id);
        if (cachedUser) {
          return cachedUser;
        }
        const [user] = await db.select().from(users).where(eq(users.id, id));
        if (user) {
          await redis.cacheUser(id, user, 3600);
        }
        return user || void 0;
      }
      async getUserByEmail(email) {
        const [user] = await db.select().from(users).where(eq(users.email, email));
        return user || void 0;
      }
      async getUserByUsername(username) {
        const [user] = await db.select().from(users).where(eq(users.username, username));
        return user || void 0;
      }
      async createUser(insertUser) {
        const [user] = await db.insert(users).values(insertUser).returning();
        return user;
      }
      async getUserByGoogleId(googleId) {
        const [user] = await db.select().from(users).where(eq(users.googleId, googleId));
        return user || void 0;
      }
      async createGoogleUser(profile) {
        const baseUsername = profile.emails[0].value.split("@")[0];
        let username = baseUsername;
        let counter = 1;
        while (await this.getUserByUsername(username)) {
          username = `${baseUsername}${counter}`;
          counter++;
        }
        const [user] = await db.insert(users).values({
          username,
          email: profile.emails[0].value,
          googleId: profile.id,
          avatar: profile.photos[0]?.value,
          provider: "google",
          password: null
          // OAuth users don't have passwords
        }).returning();
        return user;
      }
      async updateUserCredits(userId, credits) {
        const [user] = await db.update(users).set({ credits }).where(eq(users.id, userId)).returning();
        await redis.clearUserCache(userId);
        return user;
      }
      async updateUserTourStatus(userId, hasSeenTour) {
        const [user] = await db.update(users).set({ hasSeenTour }).where(eq(users.id, userId)).returning();
        await redis.clearUserCache(userId);
        return user;
      }
      async updateUserDomain(userId, domain) {
        const [user] = await db.update(users).set({ primaryDomain: domain }).where(eq(users.id, userId)).returning();
        await redis.clearUserCache(userId);
        return user;
      }
      async getUserCount() {
        const result = await db.select().from(users);
        return result.length;
      }
      // Projects
      async createProject(project) {
        const [newProject] = await db.insert(projects).values(project).returning();
        await redis.clearUserCache(project.userId);
        return newProject;
      }
      async getProjectsByUserId(userId) {
        const cachedProjects = await redis.getCachedUserProjects(userId);
        if (cachedProjects) {
          return cachedProjects;
        }
        const userProjects = await db.select().from(projects).where(eq(projects.userId, userId)).orderBy(desc(projects.createdAt));
        if (userProjects.length > 0) {
          await redis.cacheUserProjects(userId, userProjects, 1800);
        }
        return userProjects;
      }
      async getProject(id) {
        const [project] = await db.select().from(projects).where(eq(projects.id, id));
        return project || void 0;
      }
      async updateProject(id, updates) {
        const [project] = await db.update(projects).set({ ...updates, updatedAt: /* @__PURE__ */ new Date() }).where(eq(projects.id, id)).returning();
        return project;
      }
      // Usage tracking
      async getUsage(userId, featureName) {
        const [usageRecord] = await db.select().from(usage).where(and(eq(usage.userId, userId), eq(usage.featureName, featureName)));
        return usageRecord || void 0;
      }
      async incrementUsage(userId, featureName) {
        const existingUsage = await this.getUsage(userId, featureName);
        if (existingUsage) {
          const [updated] = await db.update(usage).set({
            usedCount: existingUsage.usedCount + 1,
            lastUsed: /* @__PURE__ */ new Date()
          }).where(eq(usage.id, existingUsage.id)).returning();
          return updated;
        } else {
          const [newUsage] = await db.insert(usage).values({
            userId,
            featureName,
            usedCount: 1
          }).returning();
          return newUsage;
        }
      }
      // Payments
      async createPayment(payment) {
        const [newPayment] = await db.insert(payments).values(payment).returning();
        return newPayment;
      }
      async updatePaymentStatus(paymentId, status) {
        await db.update(payments).set({ status }).where(eq(payments.razorpayPaymentId, paymentId));
      }
      // Subscriptions
      async getActiveSubscription(userId) {
        const [subscription] = await db.select().from(subscriptions).where(and(
          eq(subscriptions.userId, userId),
          eq(subscriptions.status, "active")
        )).orderBy(desc(subscriptions.endDate));
        if (subscription && new Date(subscription.endDate) > /* @__PURE__ */ new Date()) {
          return subscription;
        }
        return void 0;
      }
      async createSubscription(subscription) {
        const [newSubscription] = await db.insert(subscriptions).values(subscription).returning();
        await this.updateUserPremiumStatus(subscription.userId, true, subscription.endDate);
        return newSubscription;
      }
      async cancelSubscription(subscriptionId) {
        const [subscription] = await db.update(subscriptions).set({ status: "cancelled", autoRenew: false }).where(eq(subscriptions.id, subscriptionId)).returning();
        return subscription;
      }
      async updateUserPremiumStatus(userId, isPremium, expiresAt) {
        const [user] = await db.update(users).set({ isPremium, premiumExpiresAt: expiresAt }).where(eq(users.id, userId)).returning();
        await redis.clearUserCache(userId);
        return user;
      }
      // Admin methods
      async getAllUsers() {
        return await db.select().from(users).orderBy(desc(users.createdAt));
      }
      async getAdminUsers() {
        return await db.select().from(users).where(eq(users.isAdmin, true));
      }
      async grantAdminAccess(userId, grantedByEmail) {
        const [user] = await db.update(users).set({
          isAdmin: true,
          adminGrantedBy: grantedByEmail,
          adminGrantedAt: /* @__PURE__ */ new Date()
        }).where(eq(users.id, userId)).returning();
        await redis.clearUserCache(userId);
        return user;
      }
      async revokeAdminAccess(userId) {
        const [user] = await db.update(users).set({
          isAdmin: false,
          adminGrantedBy: null,
          adminGrantedAt: null
        }).where(eq(users.id, userId)).returning();
        await redis.clearUserCache(userId);
        return user;
      }
      // Security logs
      async createSecurityLog(log2) {
        const [newLog] = await db.insert(securityLogs).values(log2).returning();
        return newLog;
      }
      async getSecurityLogs(limit = 100) {
        return await db.select().from(securityLogs).orderBy(desc(securityLogs.createdAt)).limit(limit);
      }
      async getSecurityLogsByUser(userId, limit = 50) {
        return await db.select().from(securityLogs).where(eq(securityLogs.userId, userId)).orderBy(desc(securityLogs.createdAt)).limit(limit);
      }
      // Admin activity log
      async createAdminActivityLog(log2) {
        const [newLog] = await db.insert(adminActivityLog).values(log2).returning();
        return newLog;
      }
      async getAdminActivityLogs(limit = 100) {
        return await db.select().from(adminActivityLog).orderBy(desc(adminActivityLog.createdAt)).limit(limit);
      }
      // Platform stats for admin
      async getPlatformStats() {
        const [userStats] = await db.select({ count: count() }).from(users);
        const [projectStats] = await db.select({ count: count() }).from(projects);
        const [paymentStats] = await db.select({ count: count() }).from(payments);
        const [premiumStats] = await db.select({ count: count() }).from(users).where(eq(users.isPremium, true));
        const [adminStats] = await db.select({ count: count() }).from(users).where(eq(users.isAdmin, true));
        const [subscriptionStats] = await db.select({ count: count() }).from(subscriptions);
        const [paymentAmountStats] = await db.select({ total: sum(payments.amount) }).from(payments);
        return {
          totalUsers: userStats.count,
          totalProjects: projectStats.count,
          totalPayments: paymentStats.count,
          premiumUsers: premiumStats.count,
          adminCount: adminStats.count,
          totalSubscriptions: subscriptionStats.count,
          totalPaymentAmount: Number(paymentAmountStats.total) || 0
        };
      }
      // Get all payments for admin
      async getAllPayments(limit = 100) {
        return await db.select().from(payments).orderBy(desc(payments.createdAt)).limit(limit);
      }
      // Get all subscriptions for admin
      async getAllSubscriptions(limit = 100) {
        return await db.select().from(subscriptions).orderBy(desc(subscriptions.createdAt)).limit(limit);
      }
      // Domain config methods
      async getDomainConfigs() {
        return await db.select().from(domainConfigs);
      }
      async getDomainConfig(domain) {
        const [config] = await db.select().from(domainConfigs).where(eq(domainConfigs.domain, domain));
        return config || void 0;
      }
      async updateDomainConfig(domain, isEnabled, message, updatedBy) {
        const existing = await this.getDomainConfig(domain);
        if (existing) {
          const [updated] = await db.update(domainConfigs).set({ isEnabled, comingSoonMessage: message, updatedBy, updatedAt: /* @__PURE__ */ new Date() }).where(eq(domainConfigs.domain, domain)).returning();
          return updated;
        } else {
          const [created] = await db.insert(domainConfigs).values({ domain, isEnabled, comingSoonMessage: message, updatedBy }).returning();
          return created;
        }
      }
      // Site settings methods
      async getSiteSettings() {
        return await db.select().from(siteSettings);
      }
      async getSiteSetting(key) {
        const [setting] = await db.select().from(siteSettings).where(eq(siteSettings.key, key));
        return setting || void 0;
      }
      async updateSiteSetting(key, value, updatedBy) {
        const [updated] = await db.update(siteSettings).set({ value, updatedBy, updatedAt: /* @__PURE__ */ new Date() }).where(eq(siteSettings.key, key)).returning();
        return updated;
      }
      // User ban methods
      async banUser(userId, reason) {
        const [user] = await db.update(users).set({ isBanned: true, bannedAt: /* @__PURE__ */ new Date(), banReason: reason }).where(eq(users.id, userId)).returning();
        await redis.clearUserCache(userId);
        return user;
      }
      async unbanUser(userId) {
        const [user] = await db.update(users).set({ isBanned: false, bannedAt: null, banReason: null }).where(eq(users.id, userId)).returning();
        await redis.clearUserCache(userId);
        return user;
      }
      async updateUserCreditsByEmail(email, credits) {
        const user = await this.getUserByEmail(email);
        if (!user) return void 0;
        return await this.updateUserCredits(user.id, credits);
      }
      async updateLastActive(userId) {
        await db.update(users).set({ lastActiveAt: /* @__PURE__ */ new Date() }).where(eq(users.id, userId));
        await redis.clearUserCache(userId);
      }
      // Enhanced stats
      async getActiveUsersCount(since) {
        const [result] = await db.select({ count: count() }).from(users).where(gte(users.lastActiveAt, since));
        return result.count;
      }
      async searchUsers(query) {
        return await db.select().from(users).where(or(
          like(users.email, `%${query}%`),
          like(users.username, `%${query}%`)
        )).orderBy(desc(users.createdAt)).limit(50);
      }
      // Incident/Helpdesk methods
      async createIncident(userId, data) {
        const [incident] = await db.insert(incidents).values({ ...data, userId }).returning();
        return incident;
      }
      async getIncidentsByUser(userId) {
        return await db.select().from(incidents).where(eq(incidents.userId, userId)).orderBy(desc(incidents.createdAt));
      }
      async getAllIncidents(status) {
        if (status) {
          return await db.select().from(incidents).where(eq(incidents.status, status)).orderBy(desc(incidents.createdAt));
        }
        return await db.select().from(incidents).orderBy(desc(incidents.createdAt));
      }
      async getIncident(id) {
        const [incident] = await db.select().from(incidents).where(eq(incidents.id, id));
        return incident || void 0;
      }
      async updateIncidentStatus(id, status, resolution, assigneeId) {
        const updates = { status, updatedAt: /* @__PURE__ */ new Date() };
        if (resolution) updates.resolution = resolution;
        if (assigneeId) updates.assigneeId = assigneeId;
        if (status === "closed" || status === "resolved") updates.closedAt = /* @__PURE__ */ new Date();
        const [incident] = await db.update(incidents).set(updates).where(eq(incidents.id, id)).returning();
        return incident;
      }
      async addIncidentMessage(incidentId, authorId, authorRole, message) {
        const [msg] = await db.insert(incidentMessages).values({ incidentId, authorId, authorRole, message }).returning();
        await db.update(incidents).set({ updatedAt: /* @__PURE__ */ new Date() }).where(eq(incidents.id, incidentId));
        return msg;
      }
      async getIncidentMessages(incidentId) {
        return await db.select().from(incidentMessages).where(eq(incidentMessages.incidentId, incidentId)).orderBy(incidentMessages.createdAt);
      }
      async getIncidentStats() {
        const [open] = await db.select({ count: count() }).from(incidents).where(eq(incidents.status, "open"));
        const [inProgress] = await db.select({ count: count() }).from(incidents).where(eq(incidents.status, "in_progress"));
        const [resolved] = await db.select({ count: count() }).from(incidents).where(or(eq(incidents.status, "resolved"), eq(incidents.status, "closed")));
        const [total] = await db.select({ count: count() }).from(incidents);
        return { open: open.count, inProgress: inProgress.count, resolved: resolved.count, total: total.count };
      }
    };
    storage = new DatabaseStorage();
  }
});

// server/auth.ts
import passport from "passport";
import { Strategy as LocalStrategy } from "passport-local";
import { Strategy as GoogleStrategy } from "passport-google-oauth20";
import session from "express-session";
import { scrypt, randomBytes, timingSafeEqual } from "crypto";
import { promisify } from "util";
import connectPg from "connect-pg-simple";
import { RedisStore } from "connect-redis";
async function hashPassword(password) {
  const salt = randomBytes(16).toString("hex");
  const buf = await scryptAsync(password, salt, 64);
  return `${buf.toString("hex")}.${salt}`;
}
async function comparePasswords(supplied, stored) {
  const [hashed, salt] = stored.split(".");
  const hashedBuf = Buffer.from(hashed, "hex");
  const suppliedBuf = await scryptAsync(supplied, salt, 64);
  return timingSafeEqual(hashedBuf, suppliedBuf);
}
function setupAuth(app) {
  let sessionStore;
  try {
    if (redis.isConnected() && redis.getClient()) {
      sessionStore = new RedisStore({
        client: redis.getClient(),
        prefix: "prometix:sess:"
      });
      logger.info("Using Redis session store");
    } else {
      throw new Error("Redis not available");
    }
  } catch (redisError) {
    try {
      logger.warn("Redis unavailable, falling back to PostgreSQL sessions");
      const PostgresSessionStore = connectPg(session);
      sessionStore = new PostgresSessionStore({
        pool,
        createTableIfMissing: true
      });
      logger.info("Using PostgreSQL session store");
    } catch (pgError) {
      logger.error("PostgreSQL session store failed, falling back to MemoryStore", pgError);
      sessionStore = new session.MemoryStore();
    }
  }
  const sessionSettings = {
    secret: process.env.SESSION_SECRET || "devops-cloud-secret-key",
    resave: false,
    saveUninitialized: false,
    store: sessionStore,
    cookie: {
      secure: process.env.NODE_ENV === "production",
      httpOnly: true,
      sameSite: "lax",
      maxAge: 24 * 60 * 60 * 1e3
      // 24 hours
    }
  };
  app.set("trust proxy", 1);
  app.use(session(sessionSettings));
  app.use(passport.initialize());
  app.use(passport.session());
  passport.use(
    new LocalStrategy(
      { usernameField: "email" },
      async (email, password, done) => {
        try {
          const user = await storage.getUserByEmail(email);
          if (!user) {
            logger.auth("login", email, false, void 0, "User not found");
            return done(null, false);
          }
          if (!user.password) {
            logger.auth("login", email, false, void 0, "User has no password (likely OAuth user)");
            return done(null, false);
          }
          const isValid = await comparePasswords(password, user.password);
          if (!isValid) {
            logger.auth("login", email, false, void 0, "Invalid password");
            return done(null, false);
          }
          logger.auth("login", email, true);
          return done(null, user);
        } catch (error) {
          logger.error("Authentication error", error);
          return done(error);
        }
      }
    )
  );
  if (process.env.GOOGLE_CLIENT_ID && process.env.GOOGLE_CLIENT_SECRET) {
    passport.use(
      new GoogleStrategy(
        {
          clientID: process.env.GOOGLE_CLIENT_ID,
          clientSecret: process.env.GOOGLE_CLIENT_SECRET,
          callbackURL: process.env.REPLIT_DOMAINS ? `https://${process.env.REPLIT_DOMAINS.split(",")[0]}/api/auth/google/callback` : "http://localhost:5002/api/auth/google/callback"
        },
        async (accessToken, refreshToken, profile, done) => {
          try {
            const email = profile.emails?.[0]?.value;
            if (!email) {
              logger.error("Google OAuth error: No email provided");
              return done(null, false);
            }
            let user = await storage.getUserByGoogleId(profile.id);
            if (user) {
              logger.auth("google-login", email, true, String(user.id));
              return done(null, user);
            }
            const existingUser = await storage.getUserByEmail(email);
            if (existingUser) {
              logger.auth("google-link", email, true, String(existingUser.id));
              return done(null, existingUser);
            }
            user = await storage.createGoogleUser(profile);
            logger.auth("google-register", email, true, String(user.id));
            return done(null, user);
          } catch (error) {
            logger.error("Google OAuth error", error);
            return done(error, false);
          }
        }
      )
    );
  }
  passport.serializeUser((user, done) => done(null, user.id));
  passport.deserializeUser(async (id, done) => {
    try {
      const user = await storage.getUser(id);
      done(null, user);
    } catch (error) {
      done(error);
    }
  });
  app.post("/api/register", async (req, res, next) => {
    try {
      const { username, email, password } = req.body;
      const existingUser = await storage.getUserByEmail(email);
      if (existingUser) {
        logger.auth("register", email, false, req.ip, "Email already exists");
        return res.status(400).json({ message: "Email already exists" });
      }
      const user = await storage.createUser({
        username,
        email,
        password: await hashPassword(password)
      });
      logger.auth("register", email, true, req.ip);
      logger.info("New user registered", { userId: user.id, username, email });
      req.login(user, (err) => {
        if (err) {
          logger.error("Login after registration failed", err, user.id);
          return next(err);
        }
        res.status(201).json(user);
      });
    } catch (error) {
      logger.error("Registration error", error);
      next(error);
    }
  });
  app.post("/api/login", (req, res, next) => {
    passport.authenticate("local", (err, user, info) => {
      if (err) return next(err);
      if (!user) return res.status(401).json({ message: "Invalid email or password" });
      req.login(user, (err2) => {
        if (err2) return next(err2);
        return res.status(200).json(user);
      });
    })(req, res, next);
  });
  app.post("/api/logout", (req, res, next) => {
    const userId = req.user?.id;
    const email = req.user?.email;
    req.logout((err) => {
      if (err) {
        logger.error("Logout error", err, userId);
        return next(err);
      }
      logger.auth("logout", email || "unknown", true, req.ip);
      res.sendStatus(200);
    });
  });
  app.get("/api/user", (req, res) => {
    if (!req.isAuthenticated()) return res.sendStatus(401);
    res.json(req.user);
  });
  app.get(
    "/api/auth/google",
    passport.authenticate("google", { scope: ["profile", "email"] })
  );
  app.get(
    "/api/auth/google/callback",
    passport.authenticate("google", { failureRedirect: "/auth?error=oauth_failed" }),
    (req, res) => {
      logger.info("Google OAuth callback successful", { user: req.user?.email }, req.user?.id);
      res.redirect("/dashboard");
    }
  );
}
var scryptAsync;
var init_auth = __esm({
  "server/auth.ts"() {
    "use strict";
    init_storage();
    init_db();
    init_logger();
    init_redis();
    scryptAsync = promisify(scrypt);
  }
});

// server/services/code-generator.ts
function generateCrudApi(config) {
  const { name, database, authentication, framework } = config;
  if (framework === "express") {
    return generateExpressApi(config);
  } else if (framework === "fastapi") {
    return generateFastApiCode(config);
  } else {
    return generateNestJsApi(config);
  }
}
function generateExpressApi(config) {
  const { name, database, authentication } = config;
  return `// Auto-generated API for ${name}
const express = require('express');
const cors = require('cors');
${authentication ? `const jwt = require('jsonwebtoken');
const bcrypt = require('bcryptjs');` : ""}
${database === "postgresql" ? `const { Pool } = require('pg');` : ""}
${database === "mongodb" ? `const mongoose = require('mongoose');` : ""}
${database === "mysql" ? `const mysql = require('mysql2/promise');` : ""}

const app = express();

// Middleware
app.use(cors());
app.use(express.json());

${database === "postgresql" ? `
// Database connection
const pool = new Pool({
  user: process.env.DB_USER,
  host: process.env.DB_HOST,
  database: process.env.DB_NAME,
  password: process.env.DB_PASSWORD,
  port: process.env.DB_PORT || 5432,
});` : ""}

${authentication ? `
// Authentication middleware
const authenticateToken = (req, res, next) => {
  const authHeader = req.headers['authorization'];
  const token = authHeader && authHeader.split(' ')[1];

  if (!token) {
    return res.status(401).json({ error: 'Access token required' });
  }

  jwt.verify(token, process.env.JWT_SECRET, (err, user) => {
    if (err) return res.status(403).json({ error: 'Invalid token' });
    req.user = user;
    next();
  });
};

// Auth routes
app.post('/api/auth/register', async (req, res) => {
  try {
    const { username, email, password } = req.body;
    const hashedPassword = await bcrypt.hash(password, 10);
    
    // Save user to database
    const result = await pool.query(
      'INSERT INTO users (username, email, password) VALUES ($1, $2, $3) RETURNING id, username, email',
      [username, email, hashedPassword]
    );
    
    const user = result.rows[0];
    const token = jwt.sign({ userId: user.id }, process.env.JWT_SECRET);
    
    res.status(201).json({ user, token });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

app.post('/api/auth/login', async (req, res) => {
  try {
    const { email, password } = req.body;
    
    const result = await pool.query('SELECT * FROM users WHERE email = $1', [email]);
    const user = result.rows[0];
    
    if (!user || !(await bcrypt.compare(password, user.password))) {
      return res.status(401).json({ error: 'Invalid credentials' });
    }
    
    const token = jwt.sign({ userId: user.id }, process.env.JWT_SECRET);
    res.json({ user: { id: user.id, username: user.username, email: user.email }, token });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});` : ""}

// CRUD routes
app.get('/api/${name.toLowerCase()}', ${authentication ? "authenticateToken, " : ""}async (req, res) => {
  try {
    const result = await pool.query('SELECT * FROM ${name.toLowerCase()} ORDER BY id DESC');
    res.json(result.rows);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

app.get('/api/${name.toLowerCase()}/:id', ${authentication ? "authenticateToken, " : ""}async (req, res) => {
  try {
    const { id } = req.params;
    const result = await pool.query('SELECT * FROM ${name.toLowerCase()} WHERE id = $1', [id]);
    
    if (result.rows.length === 0) {
      return res.status(404).json({ error: '${name} not found' });
    }
    
    res.json(result.rows[0]);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

app.post('/api/${name.toLowerCase()}', ${authentication ? "authenticateToken, " : ""}async (req, res) => {
  try {
    const { name, description } = req.body;
    const result = await pool.query(
      'INSERT INTO ${name.toLowerCase()} (name, description${authentication ? ", user_id" : ""}) VALUES ($1, $2${authentication ? ", $3" : ""}) RETURNING *',
      [name, description${authentication ? ", req.user.userId" : ""}]
    );
    
    res.status(201).json(result.rows[0]);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

app.put('/api/${name.toLowerCase()}/:id', ${authentication ? "authenticateToken, " : ""}async (req, res) => {
  try {
    const { id } = req.params;
    const { name, description } = req.body;
    
    const result = await pool.query(
      'UPDATE ${name.toLowerCase()} SET name = $1, description = $2, updated_at = NOW() WHERE id = $3 RETURNING *',
      [name, description, id]
    );
    
    if (result.rows.length === 0) {
      return res.status(404).json({ error: '${name} not found' });
    }
    
    res.json(result.rows[0]);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

app.delete('/api/${name.toLowerCase()}/:id', ${authentication ? "authenticateToken, " : ""}async (req, res) => {
  try {
    const { id } = req.params;
    const result = await pool.query('DELETE FROM ${name.toLowerCase()} WHERE id = $1 RETURNING *', [id]);
    
    if (result.rows.length === 0) {
      return res.status(404).json({ error: '${name} not found' });
    }
    
    res.json({ message: '${name} deleted successfully' });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(\`Server running on port \${PORT}\`);
});

module.exports = app;`;
}
function generateFastApiCode(config) {
  return `# Auto-generated FastAPI for ${config.name}
from fastapi import FastAPI, HTTPException, Depends, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import jwt
import bcrypt
import os
from datetime import datetime, timedelta

app = FastAPI(title="${config.name} API")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Security
security = HTTPBearer()
JWT_SECRET = os.getenv("JWT_SECRET", "your-secret-key")

# Models
class ${config.name}Base(BaseModel):
    name: str
    description: str

class ${config.name}Create(${config.name}Base):
    pass

class ${config.name}Response(${config.name}Base):
    id: int
    created_at: datetime

${config.authentication ? `
class UserCreate(BaseModel):
    username: str
    email: str
    password: str

class UserLogin(BaseModel):
    email: str
    password: str

class UserResponse(BaseModel):
    id: int
    username: str
    email: str

def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):
    try:
        payload = jwt.decode(credentials.credentials, JWT_SECRET, algorithms=["HS256"])
        user_id = payload.get("user_id")
        if user_id is None:
            raise HTTPException(status_code=401, detail="Invalid token")
        return user_id
    except jwt.PyJWTError:
        raise HTTPException(status_code=401, detail="Invalid token")

@app.post("/api/auth/register", response_model=dict)
async def register(user: UserCreate):
    # Hash password and save user
    hashed_password = bcrypt.hashpw(user.password.encode('utf-8'), bcrypt.gensalt())
    # Database logic here
    token = jwt.encode({"user_id": 1}, JWT_SECRET, algorithm="HS256")
    return {"user": {"id": 1, "username": user.username, "email": user.email}, "token": token}

@app.post("/api/auth/login", response_model=dict)
async def login(user: UserLogin):
    # Verify credentials
    token = jwt.encode({"user_id": 1}, JWT_SECRET, algorithm="HS256")
    return {"user": {"id": 1, "email": user.email}, "token": token}` : ""}

# CRUD endpoints
@app.get("/api/${config.name.toLowerCase()}", response_model=List[${config.name}Response])
async def get_${config.name.toLowerCase()}s(${config.authentication ? "current_user: int = Depends(get_current_user)" : ""}):
    # Database query logic
    return []

@app.get("/api/${config.name.toLowerCase()}/{item_id}", response_model=${config.name}Response)
async def get_${config.name.toLowerCase()}(item_id: int, ${config.authentication ? "current_user: int = Depends(get_current_user)" : ""}):
    # Database query logic
    raise HTTPException(status_code=404, detail="${config.name} not found")

@app.post("/api/${config.name.toLowerCase()}", response_model=${config.name}Response)
async def create_${config.name.toLowerCase()}(item: ${config.name}Create, ${config.authentication ? "current_user: int = Depends(get_current_user)" : ""}):
    # Database insert logic
    return ${config.name}Response(id=1, name=item.name, description=item.description, created_at=datetime.now())

@app.put("/api/${config.name.toLowerCase()}/{item_id}", response_model=${config.name}Response)
async def update_${config.name.toLowerCase()}(item_id: int, item: ${config.name}Create, ${config.authentication ? "current_user: int = Depends(get_current_user)" : ""}):
    # Database update logic
    return ${config.name}Response(id=item_id, name=item.name, description=item.description, created_at=datetime.now())

@app.delete("/api/${config.name.toLowerCase()}/{item_id}")
async def delete_${config.name.toLowerCase()}(item_id: int, ${config.authentication ? "current_user: int = Depends(get_current_user)" : ""}):
    # Database delete logic
    return {"message": "${config.name} deleted successfully"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)`;
}
function generateNestJsApi(config) {
  return `// Auto-generated NestJS API for ${config.name}
// This is a basic structure - implement the full NestJS modules as needed

// app.module.ts
import { Module } from '@nestjs/common';
import { ${config.name}Module } from './${config.name.toLowerCase()}/${config.name.toLowerCase()}.module';
${config.authentication ? `import { AuthModule } from './auth/auth.module';` : ""}

@Module({
  imports: [${config.name}Module${config.authentication ? ", AuthModule" : ""}],
})
export class AppModule {}

// ${config.name.toLowerCase()}.controller.ts
import { Controller, Get, Post, Put, Delete, Body, Param${config.authentication ? ", UseGuards" : ""} } from '@nestjs/common';
${config.authentication ? `import { JwtAuthGuard } from '../auth/jwt-auth.guard';` : ""}

@Controller('api/${config.name.toLowerCase()}')
${config.authentication ? "@UseGuards(JwtAuthGuard)" : ""}
export class ${config.name}Controller {
  
  @Get()
  findAll() {
    return [];
  }

  @Get(':id')
  findOne(@Param('id') id: string) {
    return {};
  }

  @Post()
  create(@Body() data: any) {
    return {};
  }

  @Put(':id')
  update(@Param('id') id: string, @Body() data: any) {
    return {};
  }

  @Delete(':id')
  remove(@Param('id') id: string) {
    return { message: '${config.name} deleted successfully' };
  }
}`;
}
function generateDockerfile(config) {
  const { language, framework, port, baseImage, envVars = [] } = config;
  if (language.toLowerCase() === "javascript" || language.toLowerCase() === "node") {
    return generateNodeDockerfile(framework, port, baseImage, envVars);
  } else if (language.toLowerCase() === "python") {
    return generatePythonDockerfile(framework, port, baseImage, envVars);
  } else if (language.toLowerCase() === "java") {
    return generateJavaDockerfile(framework, port, baseImage, envVars);
  } else {
    return generateGenericDockerfile(language, port, baseImage, envVars);
  }
}
function generateNodeDockerfile(framework, port, baseImage, envVars = []) {
  const base = baseImage || "node:18-alpine";
  return `# Auto-generated Dockerfile for ${framework}
FROM ${base}

# Set working directory
WORKDIR /app

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm ci --only=production && npm cache clean --force

# Copy source code
COPY . .

# Create non-root user
RUN addgroup -g 1001 -S nodejs && \\
    adduser -S nodejs -u 1001

# Change ownership
RUN chown -R nodejs:nodejs /app
USER nodejs

# Environment variables
${envVars.map((env) => `ENV ${env}`).join("\n")}
ENV NODE_ENV=production
ENV PORT=${port}

# Expose port
EXPOSE ${port}

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\
  CMD curl -f http://localhost:${port}/health || exit 1

# Start application
CMD ["npm", "start"]`;
}
function generatePythonDockerfile(framework, port, baseImage, envVars = []) {
  const base = baseImage || "python:3.11-slim";
  return `# Auto-generated Dockerfile for ${framework}
FROM ${base}

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \\
    gcc \\
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first for better caching
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy source code
COPY . .

# Create non-root user
RUN adduser --disabled-password --gecos '' python

# Change ownership
RUN chown -R python:python /app
USER python

# Environment variables
${envVars.map((env) => `ENV ${env}`).join("\n")}
ENV PYTHONUNBUFFERED=1
ENV PORT=${port}

# Expose port
EXPOSE ${port}

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\
  CMD curl -f http://localhost:${port}/health || exit 1

# Start application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "${port}"]`;
}
function generateJavaDockerfile(framework, port, baseImage, envVars = []) {
  const base = baseImage || "openjdk:17-jdk-slim";
  return `# Auto-generated Dockerfile for ${framework}
FROM ${base}

# Set working directory
WORKDIR /app

# Copy build files
COPY target/*.jar app.jar

# Environment variables
${envVars.map((env) => `ENV ${env}`).join("\n")}
ENV JAVA_OPTS="-Xmx512m -Xms256m"
ENV SERVER_PORT=${port}

# Expose port
EXPOSE ${port}

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\
  CMD curl -f http://localhost:${port}/actuator/health || exit 1

# Start application
ENTRYPOINT ["sh", "-c", "java $JAVA_OPTS -jar app.jar"]`;
}
function generateGenericDockerfile(language, port, baseImage, envVars = []) {
  return `# Auto-generated Dockerfile for ${language}
FROM ${baseImage || "ubuntu:20.04"}

# Set working directory
WORKDIR /app

# Copy source code
COPY . .

# Environment variables
${envVars.map((env) => `ENV ${env}`).join("\n")}
ENV PORT=${port}

# Expose port
EXPOSE ${port}

# TODO: Add specific build and run commands for ${language}
CMD ["echo", "Please configure the startup command for ${language}"]`;
}
function generateDockerCompose(services) {
  return `# Auto-generated docker-compose.yml
version: '3.8'

services:
${services.map((service) => `  ${service.name}:
    build: .
    ports:
      - "${service.port}:${service.port}"
    environment:
      - NODE_ENV=production
      - PORT=${service.port}
    ${service.envFile ? `env_file:
      - ${service.envFile}` : ""}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${service.port}/health"]
      interval: 30s
      timeout: 10s
      retries: 3`).join("\n\n")}

  # Database service (uncomment and configure as needed)
  # postgres:
  #   image: postgres:15-alpine
  #   environment:
  #     POSTGRES_DB: myapp
  #     POSTGRES_USER: user
  #     POSTGRES_PASSWORD: password
  #   volumes:
  #     - postgres_data:/var/lib/postgresql/data
  #   ports:
  #     - "5432:5432"

  # Redis service (uncomment if needed)
  # redis:
  #   image: redis:7-alpine
  #   ports:
  #     - "6379:6379"

# volumes:
#   postgres_data:`;
}
function generateGitHubActions(config) {
  const { language, framework, testCommand } = config;
  return `# Auto-generated GitHub Actions workflow
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    ${language.toLowerCase() === "javascript" || language.toLowerCase() === "node" ? `
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Run tests
      run: ${testCommand || "npm test"}
    
    - name: Run linting
      run: npm run lint
      continue-on-error: true` : ""}
    
    ${language.toLowerCase() === "python" ? `
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run tests
      run: ${testCommand || "pytest"}` : ""}

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Login to Docker Hub
      uses: docker/login-action@v3
      with:
        username: \${{ secrets.DOCKER_USERNAME }}
        password: \${{ secrets.DOCKER_PASSWORD }}
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: |
          \${{ secrets.DOCKER_USERNAME }}/${framework}-app:latest
          \${{ secrets.DOCKER_USERNAME }}/${framework}-app:\${{ github.sha }}
    
  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Deploy to production
      run: |
        echo "Add your deployment script here"
        # Example: kubectl set image deployment/app app=\${{ secrets.DOCKER_USERNAME }}/${framework}-app:\${{ github.sha }}`;
}
var init_code_generator = __esm({
  "server/services/code-generator.ts"() {
    "use strict";
  }
});

// server/services/gemini.ts
var gemini_exports = {};
__export(gemini_exports, {
  analyzeLogError: () => analyzeLogError,
  generateChatResponse: () => generateChatResponse,
  generateDevOpsResponse: () => generateDevOpsResponse,
  generateYamlFromNaturalLanguage: () => generateYamlFromNaturalLanguage,
  optimizeDockerfile: () => optimizeDockerfile
});
import { GoogleGenerativeAI } from "@google/generative-ai";
async function analyzeLogError(logText) {
  try {
    const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
    const prompt = `You are an expert DevOps engineer. Analyze the provided log or error message and provide a clear problem description and actionable solution. 
    
Respond with JSON in this exact format (no markdown, just raw JSON):
{ "problem": "description of the problem", "solution": "actionable solution", "confidence": 0.8 }

Analyze this log/error and help me fix it:

${logText}`;
    const result = await model.generateContent(prompt);
    const response = await result.response;
    const text2 = response.text();
    const jsonMatch = text2.match(/\{[\s\S]*\}/);
    if (!jsonMatch) {
      throw new Error("Could not parse JSON response");
    }
    const parsed = JSON.parse(jsonMatch[0]);
    return {
      problem: parsed.problem || "Could not identify the problem",
      solution: parsed.solution || "No solution available",
      confidence: Math.max(0, Math.min(1, parsed.confidence || 0.5))
    };
  } catch (error) {
    throw new Error("Failed to analyze log: " + error.message);
  }
}
async function generateYamlFromNaturalLanguage(description) {
  try {
    const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
    const prompt = `You are an expert in DevOps and CI/CD. Convert natural language descriptions into valid YAML configurations for GitHub Actions, Docker Compose, or Kubernetes.

Respond with JSON in this exact format (no markdown, just raw JSON):
{ "yaml": "the yaml configuration", "explanation": "explanation of what the yaml does" }

Generate YAML configuration for: ${description}`;
    const result = await model.generateContent(prompt);
    const response = await result.response;
    const text2 = response.text();
    const jsonMatch = text2.match(/\{[\s\S]*\}/);
    if (!jsonMatch) {
      throw new Error("Could not parse JSON response");
    }
    const parsed = JSON.parse(jsonMatch[0]);
    return {
      yaml: parsed.yaml || "# Could not generate YAML",
      explanation: parsed.explanation || "No explanation available"
    };
  } catch (error) {
    throw new Error("Failed to generate YAML: " + error.message);
  }
}
async function optimizeDockerfile(dockerfile) {
  try {
    const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
    const prompt = `You are a Docker expert. Analyze the provided Dockerfile and suggest optimizations for size, security, and build time.

Respond with JSON in this exact format (no markdown, just raw JSON):
{ "optimizedDockerfile": "the optimized dockerfile content", "improvements": ["improvement 1", "improvement 2"] }

Optimize this Dockerfile:

${dockerfile}`;
    const result = await model.generateContent(prompt);
    const response = await result.response;
    const text2 = response.text();
    const jsonMatch = text2.match(/\{[\s\S]*\}/);
    if (!jsonMatch) {
      throw new Error("Could not parse JSON response");
    }
    const parsed = JSON.parse(jsonMatch[0]);
    return {
      optimizedDockerfile: parsed.optimizedDockerfile || dockerfile,
      improvements: parsed.improvements || []
    };
  } catch (error) {
    throw new Error("Failed to optimize Dockerfile: " + error.message);
  }
}
async function generateDevOpsResponse(query) {
  try {
    if (!process.env.GOOGLE_API_KEY) {
      return getDevOpsKnowledgeResponse(query);
    }
    const model = genAI.getGenerativeModel({ model: "gemini-1.5-pro" });
    const prompt = `You are an expert DevOps and Cloud Infrastructure consultant with deep knowledge of:
          
- AWS, Azure, GCP cloud platforms and services
- Container technologies (Docker, Kubernetes, container orchestration)
- CI/CD pipelines and automation (GitHub Actions, Jenkins, GitLab CI)
- Infrastructure as Code (Terraform, CloudFormation, Pulumi)
- Monitoring and observability (Prometheus, Grafana, ELK stack)
- Security best practices and compliance
- Database scaling and management
- Microservices architecture and deployment strategies
- Network configuration and load balancing
- Cost optimization and resource management

Provide detailed, practical, and actionable responses. Include:
- Step-by-step instructions when appropriate
- Code examples and configuration snippets
- Best practices and common pitfalls to avoid
- Security considerations
- Performance optimization tips
- Cost implications where relevant

Keep responses comprehensive but well-structured with clear headings and bullet points.
Focus on production-ready solutions rather than basic tutorials.

User Query: ${query}`;
    const result = await model.generateContent(prompt);
    const response = await result.response;
    const answer = response.text();
    if (!answer) {
      return getDevOpsKnowledgeResponse(query);
    }
    return answer;
  } catch (error) {
    console.error("Gemini DevOps query error:", error);
    if (error instanceof Error) {
      if (error.message.includes("RESOURCE_EXHAUSTED") || error.message.includes("quota")) {
        return `**Gemini API Quota Exceeded**

Your Google API account has reached its usage limit. Here's what you can do:

1. **Check your Google Cloud billing**: Visit https://console.cloud.google.com to see your current usage
2. **Enable billing**: Ensure billing is enabled for your Google Cloud project
3. **Request quota increase**: Consider requesting a quota increase for the Gemini API

**Meanwhile, here's some general guidance for your query:**
${getDevOpsKnowledgeResponse(query)}

*Once you have available quota on your Google API account, the AI assistant will provide more detailed, personalized responses.*`;
      }
      if (error.message.includes("API_KEY_INVALID") || error.message.includes("invalid")) {
        return `**Gemini API Key Issue**

There's an issue with your Google API key. Please check:

1. **Key validity**: Ensure your API key is correct and active
2. **API enabled**: Make sure the Gemini API is enabled in your Google Cloud project
3. **Account status**: Check if your Google Cloud account is in good standing

**Basic guidance for your query:**
${getDevOpsKnowledgeResponse(query)}`;
      }
    }
    return getDevOpsKnowledgeResponse(query);
  }
}
function getDevOpsKnowledgeResponse(query) {
  const lowerQuery = query.toLowerCase();
  if (lowerQuery.includes("docker") || lowerQuery.includes("container")) {
    return `**Docker Best Practices**

Here are key Docker optimization strategies:

## Container Optimization
- Use multi-stage builds to reduce image size
- Choose appropriate base images (Alpine for minimal size)
- Minimize layers by combining RUN commands
- Use .dockerignore to exclude unnecessary files

## Security Best Practices
- Run containers as non-root users
- Scan images for vulnerabilities
- Use specific version tags, not 'latest'
- Limit container resources (CPU, memory)

## Example Dockerfile:
\`\`\`dockerfile
FROM node:18-alpine AS builder
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

FROM node:18-alpine
RUN addgroup -g 1001 -S nodejs
RUN adduser -S nextjs -u 1001
WORKDIR /app
COPY --from=builder --chown=nextjs:nodejs /app .
USER nextjs
CMD ["npm", "start"]
\`\`\`

*For more detailed assistance, please ensure your Google API key has sufficient quota.*`;
  }
  if (lowerQuery.includes("kubernetes") || lowerQuery.includes("k8s")) {
    return `**Kubernetes Essentials**

## Key Concepts
- **Pods**: Smallest deployable units
- **Services**: Network access to pods
- **Deployments**: Manage pod replicas
- **ConfigMaps/Secrets**: Configuration management

## Common Commands
\`\`\`bash
# Check cluster status
kubectl get nodes

# Deploy application
kubectl apply -f deployment.yaml

# Check pods
kubectl get pods

# View logs
kubectl logs pod-name
\`\`\`

## Basic Deployment Example:
\`\`\`yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
\`\`\`

*For personalized Kubernetes solutions, please ensure your Google API has available quota.*`;
  }
  if (lowerQuery.includes("aws") || lowerQuery.includes("cloud")) {
    return `**AWS Cloud Best Practices**

## Core Services
- **EC2**: Virtual servers
- **S3**: Object storage
- **RDS**: Managed databases
- **Lambda**: Serverless functions
- **VPC**: Network isolation

## Cost Optimization
- Use Reserved Instances for predictable workloads
- Implement auto-scaling
- Monitor with CloudWatch
- Use appropriate instance types
- Clean up unused resources

## Security Fundamentals
- Enable MFA on all accounts
- Use IAM roles instead of access keys
- Encrypt data at rest and in transit
- Regular security audits
- VPC with private subnets

## Example IAM Policy:
\`\`\`json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Resource": "arn:aws:s3:::my-bucket/*"
    }
  ]
}
\`\`\`

*For detailed AWS architecture guidance, please ensure your Google API has available quota.*`;
  }
  if (lowerQuery.includes("ci/cd") || lowerQuery.includes("pipeline") || lowerQuery.includes("github actions")) {
    return `**CI/CD Pipeline Best Practices**

## GitHub Actions Example
\`\`\`yaml
name: CI/CD Pipeline
on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
      - run: npm ci
      - run: npm test
  
  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v3
      - name: Deploy to production
        run: |
          echo "Deploying to production"
\`\`\`

## Key Principles
- Automated testing at every stage
- Fast feedback loops
- Consistent environments
- Security scanning
- Rollback capabilities
- Blue-green deployments

*For custom pipeline configurations, please ensure your Google API has available quota.*`;
  }
  return `**DevOps Guidance**

I can help with DevOps and Cloud questions! Common topics include:

- **Container Technologies**: Docker, Kubernetes, container orchestration
- **Cloud Platforms**: AWS, Azure, GCP services and best practices  
- **CI/CD**: GitHub Actions, Jenkins, deployment pipelines
- **Infrastructure as Code**: Terraform, CloudFormation, Pulumi
- **Monitoring**: Prometheus, Grafana, logging strategies
- **Security**: Best practices, compliance, vulnerability management

**Your query**: "${query}"

**Basic suggestions**:
- Break down complex problems into smaller components
- Follow security best practices
- Use Infrastructure as Code for reproducibility
- Implement monitoring and alerting
- Document your processes

*For detailed, personalized assistance with your specific use case, please ensure your Google API has available quota. The AI can then provide step-by-step solutions, code examples, and production-ready configurations.*

**Need immediate help?**
- Check official documentation for your tools
- Search Stack Overflow for similar issues
- Review GitHub repositories for examples
- Consult cloud provider documentation`;
}
async function generateChatResponse(systemPrompt, userMessage) {
  try {
    const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
    const prompt = `${systemPrompt}

User Query: ${userMessage}

Provide a helpful, practical response:`;
    const result = await model.generateContent(prompt);
    const response = await result.response;
    return response.text();
  } catch (error) {
    console.error("Gemini chat error:", error);
    return `I apologize, but I couldn't process your request at the moment. Here are some general tips:

**For DevOps Questions:**
- Check official documentation for your specific tools
- Review best practices on Stack Overflow
- Consult cloud provider documentation

**Common Topics I Can Help With:**
- Docker and Kubernetes configuration
- CI/CD pipeline setup
- Infrastructure as Code
- Cloud architecture design
- Security best practices

Please try again or rephrase your question.`;
  }
}
var genAI;
var init_gemini = __esm({
  "server/services/gemini.ts"() {
    "use strict";
    genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY || "");
  }
});

// server/services/payment.ts
import Razorpay from "razorpay";
import crypto from "crypto";
async function createPaymentOrder(userId, packageId) {
  const package_ = creditPackages.find((p) => p.id === packageId);
  if (!package_) {
    throw new Error("Invalid package ID");
  }
  const order = await razorpay.orders.create({
    amount: package_.price,
    currency: "INR",
    receipt: `order_${userId}_${Date.now()}`,
    notes: {
      userId: userId.toString(),
      packageId,
      credits: package_.credits.toString()
    }
  });
  await storage.createPayment({
    userId,
    razorpayPaymentId: order.id,
    amount: package_.price,
    credits: package_.credits,
    status: "pending"
  });
  return {
    orderId: order.id,
    amount: package_.price,
    currency: "INR",
    key: process.env.RAZORPAY_KEY_ID || "rzp_test_eJzAWpZFW6jUMp"
  };
}
function verifyPaymentSignature(orderId, paymentId, signature) {
  const body2 = orderId + "|" + paymentId;
  const expectedSignature = crypto.createHmac("sha256", process.env.RAZORPAY_KEY_SECRET || "QXAErvPE9Q6zw0560UYJEOVQ").update(body2.toString()).digest("hex");
  return expectedSignature === signature;
}
async function processSuccessfulPayment(paymentId, orderId, signature) {
  if (!verifyPaymentSignature(orderId, paymentId, signature)) {
    throw new Error("Invalid payment signature");
  }
  const payment = await razorpay.payments.fetch(paymentId);
  const order = await razorpay.orders.fetch(orderId);
  if (payment.status === "captured" && order.status === "paid") {
    const notes = order.notes;
    const userId = parseInt(notes?.userId || "0");
    const credits = parseInt(notes?.credits || "0");
    const user = await storage.getUser(userId);
    if (user) {
      await storage.updateUserCredits(userId, user.credits + credits);
    }
    await storage.updatePaymentStatus(paymentId, "completed");
  } else {
    throw new Error("Payment not successful");
  }
}
async function checkUsageLimit(userId, featureName) {
  const user = await storage.getUser(userId);
  if (!user) return false;
  const usage2 = await storage.getUsage(userId, featureName);
  const freeLimit = 1;
  if (usage2 && usage2.usedCount >= freeLimit && user.credits === 0) {
    return false;
  }
  return true;
}
async function deductCreditForUsage(userId, featureName) {
  const user = await storage.getUser(userId);
  if (!user) throw new Error("User not found");
  const usage2 = await storage.getUsage(userId, featureName);
  const freeLimit = 1;
  if (usage2 && usage2.usedCount >= freeLimit) {
    if (user.credits > 0) {
      await storage.updateUserCredits(userId, user.credits - 1);
    } else {
      throw new Error("Insufficient credits");
    }
  }
  await storage.incrementUsage(userId, featureName);
}
var razorpay, creditPackages;
var init_payment = __esm({
  "server/services/payment.ts"() {
    "use strict";
    init_storage();
    razorpay = new Razorpay({
      key_id: process.env.RAZORPAY_KEY_ID || "rzp_test_eJzAWpZFW6jUMp",
      key_secret: process.env.RAZORPAY_KEY_SECRET || "QXAErvPE9Q6zw0560UYJEOVQ"
    });
    creditPackages = [
      {
        id: "starter",
        name: "Starter Pack",
        credits: 5,
        price: 9900,
        // â‚¹99
        popular: true
      },
      {
        id: "pro",
        name: "Pro Pack",
        credits: 10,
        price: 14900
        // â‚¹149
      }
    ];
  }
});

// server/middleware/cache.ts
function cacheMiddleware(options = {}) {
  const {
    ttl = 300,
    // 5 minutes default
    keyGenerator = (req) => `api:${req.method}:${req.path}:${req.user?.id || "anonymous"}`,
    condition = (req, res) => req.method === "GET" && res.statusCode === 200
  } = options;
  return async (req, res, next) => {
    if (!redis.isConnected()) {
      return next();
    }
    const cacheKey = keyGenerator(req);
    if (req.method === "GET") {
      try {
        const cachedResponse = await redis.get(cacheKey);
        if (cachedResponse) {
          logger.info(`Cache hit for key: ${cacheKey}`);
          return res.json(cachedResponse);
        }
      } catch (error) {
        logger.error("Cache retrieval error", error);
      }
    }
    const originalJson = res.json.bind(res);
    res.json = function(body2) {
      if (condition(req, res)) {
        redis.set(cacheKey, body2, ttl).catch((error) => {
          logger.error("Cache storage error", error);
        });
        logger.info(`Cached response for key: ${cacheKey}`);
      }
      return originalJson(body2);
    };
    next();
  };
}
function invalidateCacheMiddleware(patterns) {
  return async (req, res, next) => {
    if (!redis.isConnected()) {
      return next();
    }
    const originalEnd = res.end.bind(res);
    res.end = function(...args) {
      if (res.statusCode >= 200 && res.statusCode < 300) {
        patterns.forEach(async (pattern) => {
          try {
            const userId = req.user?.id;
            const cacheKey = pattern.replace(":userId", userId?.toString() || "");
            await redis.delete(cacheKey);
            logger.info(`Invalidated cache for pattern: ${cacheKey}`);
          } catch (error) {
            logger.error("Cache invalidation error", error);
          }
        });
      }
      return originalEnd(...args);
    };
    next();
  };
}
var userCacheMiddleware, projectsCacheMiddleware, usageCacheMiddleware;
var init_cache = __esm({
  "server/middleware/cache.ts"() {
    "use strict";
    init_redis();
    init_logger();
    userCacheMiddleware = cacheMiddleware({
      ttl: 3600,
      // 1 hour
      keyGenerator: (req) => `user:${req.user?.id}`,
      condition: (req, res) => req.method === "GET" && res.statusCode === 200 && !!req.user
    });
    projectsCacheMiddleware = cacheMiddleware({
      ttl: 1800,
      // 30 minutes
      keyGenerator: (req) => `projects:user:${req.user?.id}`,
      condition: (req, res) => req.method === "GET" && res.statusCode === 200 && !!req.user
    });
    usageCacheMiddleware = cacheMiddleware({
      ttl: 900,
      // 15 minutes
      keyGenerator: (req) => `usage:user:${req.user?.id}`,
      condition: (req, res) => req.method === "GET" && res.statusCode === 200 && !!req.user
    });
  }
});

// server/services/metrics.ts
import client from "prom-client";
function getMetrics() {
  return register.metrics();
}
function getContentType() {
  return register.contentType;
}
var register, httpRequestDuration, httpRequestTotal, activeUsers, totalUsers, totalProjects, apiGenerations, dockerGenerations, aiAssistanceRequests, paymentTransactions, creditsConsumed, securityEvents, databaseQueryDuration, cacheHits, cacheMisses, errorRate;
var init_metrics = __esm({
  "server/services/metrics.ts"() {
    "use strict";
    register = new client.Registry();
    client.collectDefaultMetrics({ register });
    httpRequestDuration = new client.Histogram({
      name: "http_request_duration_seconds",
      help: "Duration of HTTP requests in seconds",
      labelNames: ["method", "route", "status_code"],
      buckets: [0.1, 0.3, 0.5, 0.7, 1, 3, 5, 7, 10],
      registers: [register]
    });
    httpRequestTotal = new client.Counter({
      name: "http_requests_total",
      help: "Total number of HTTP requests",
      labelNames: ["method", "route", "status_code"],
      registers: [register]
    });
    activeUsers = new client.Gauge({
      name: "active_users_total",
      help: "Total number of active users",
      registers: [register]
    });
    totalUsers = new client.Gauge({
      name: "total_users",
      help: "Total number of registered users",
      registers: [register]
    });
    totalProjects = new client.Gauge({
      name: "total_projects",
      help: "Total number of projects",
      registers: [register]
    });
    apiGenerations = new client.Counter({
      name: "api_generations_total",
      help: "Total number of API generations",
      labelNames: ["type"],
      registers: [register]
    });
    dockerGenerations = new client.Counter({
      name: "docker_generations_total",
      help: "Total number of Docker file generations",
      registers: [register]
    });
    aiAssistanceRequests = new client.Counter({
      name: "ai_assistance_requests_total",
      help: "Total number of AI assistance requests",
      labelNames: ["feature"],
      registers: [register]
    });
    paymentTransactions = new client.Counter({
      name: "payment_transactions_total",
      help: "Total number of payment transactions",
      labelNames: ["status"],
      registers: [register]
    });
    creditsConsumed = new client.Counter({
      name: "credits_consumed_total",
      help: "Total credits consumed",
      registers: [register]
    });
    securityEvents = new client.Counter({
      name: "security_events_total",
      help: "Total security events",
      labelNames: ["type", "severity"],
      registers: [register]
    });
    databaseQueryDuration = new client.Histogram({
      name: "database_query_duration_seconds",
      help: "Duration of database queries in seconds",
      labelNames: ["operation", "table"],
      buckets: [0.01, 0.05, 0.1, 0.5, 1, 2, 5],
      registers: [register]
    });
    cacheHits = new client.Counter({
      name: "cache_hits_total",
      help: "Total number of cache hits",
      labelNames: ["cache_type"],
      registers: [register]
    });
    cacheMisses = new client.Counter({
      name: "cache_misses_total",
      help: "Total number of cache misses",
      labelNames: ["cache_type"],
      registers: [register]
    });
    errorRate = new client.Counter({
      name: "errors_total",
      help: "Total number of errors",
      labelNames: ["type", "endpoint"],
      registers: [register]
    });
  }
});

// server/services/push-notifications.ts
import webpush from "web-push";
import { eq as eq2 } from "drizzle-orm";
async function sendPushNotification(userId, payload) {
  try {
    const subscriptions2 = await db.select().from(pushSubscriptions).where(eq2(pushSubscriptions.userId, userId));
    const notificationPayload = JSON.stringify({
      title: payload.title,
      body: payload.body,
      icon: payload.icon || "/icon-192.png",
      badge: payload.badge || "/badge-72.png",
      data: {
        url: payload.url || "/",
        ...payload.data
      },
      tag: payload.tag
    });
    const sendPromises = subscriptions2.map(async (sub) => {
      try {
        await webpush.sendNotification(
          {
            endpoint: sub.endpoint,
            keys: {
              p256dh: sub.p256dh,
              auth: sub.auth
            }
          },
          notificationPayload
        );
      } catch (error) {
        if (error.statusCode === 410 || error.statusCode === 404) {
          await db.delete(pushSubscriptions).where(eq2(pushSubscriptions.id, sub.id));
        }
        console.error(`Push notification error for subscription ${sub.id}:`, error);
      }
    });
    await Promise.all(sendPromises);
  } catch (error) {
    console.error("Error sending push notifications:", error);
    throw error;
  }
}
async function sendBroadcastNotification(payload) {
  try {
    const subscriptions2 = await db.select().from(pushSubscriptions);
    const notificationPayload = JSON.stringify({
      title: payload.title,
      body: payload.body,
      icon: payload.icon || "/icon-192.png",
      badge: payload.badge || "/badge-72.png",
      data: {
        url: payload.url || "/",
        ...payload.data
      },
      tag: payload.tag
    });
    const sendPromises = subscriptions2.map(async (sub) => {
      try {
        await webpush.sendNotification(
          {
            endpoint: sub.endpoint,
            keys: {
              p256dh: sub.p256dh,
              auth: sub.auth
            }
          },
          notificationPayload
        );
      } catch (error) {
        if (error.statusCode === 410 || error.statusCode === 404) {
          await db.delete(pushSubscriptions).where(eq2(pushSubscriptions.id, sub.id));
        }
      }
    });
    await Promise.all(sendPromises);
  } catch (error) {
    console.error("Error sending broadcast notification:", error);
    throw error;
  }
}
async function savePushSubscription(userId, subscription) {
  const existing = await db.select().from(pushSubscriptions).where(eq2(pushSubscriptions.endpoint, subscription.endpoint));
  if (existing.length === 0) {
    await db.insert(pushSubscriptions).values({
      userId,
      endpoint: subscription.endpoint,
      p256dh: subscription.keys.p256dh,
      auth: subscription.keys.auth
    });
  }
}
async function removePushSubscription(endpoint) {
  await db.delete(pushSubscriptions).where(eq2(pushSubscriptions.endpoint, endpoint));
}
function getVapidPublicKey() {
  return VAPID_PUBLIC_KEY;
}
var VAPID_PUBLIC_KEY, VAPID_PRIVATE_KEY, VAPID_SUBJECT;
var init_push_notifications = __esm({
  "server/services/push-notifications.ts"() {
    "use strict";
    init_db();
    init_schema();
    VAPID_PUBLIC_KEY = process.env.VAPID_PUBLIC_KEY || "";
    VAPID_PRIVATE_KEY = process.env.VAPID_PRIVATE_KEY || "";
    VAPID_SUBJECT = process.env.VAPID_SUBJECT || "mailto:support@prometix.dev";
    if (VAPID_PUBLIC_KEY && VAPID_PRIVATE_KEY) {
      webpush.setVapidDetails(
        VAPID_SUBJECT,
        VAPID_PUBLIC_KEY,
        VAPID_PRIVATE_KEY
      );
    }
  }
});

// server/services/message-queue.ts
import { EventEmitter } from "events";
function getAllQueueMetrics() {
  return {
    kafka: kafkaService.getMetrics(),
    rabbitmq: rabbitMQService.getMetrics(),
    redis: redisMetricsService.getMetrics()
  };
}
var KAFKA_BROKER_URL, RABBITMQ_URL, USE_SIMULATED_QUEUES, KafkaService, RabbitMQService, RedisMetricsService, kafkaService, rabbitMQService, redisMetricsService;
var init_message_queue = __esm({
  "server/services/message-queue.ts"() {
    "use strict";
    KAFKA_BROKER_URL = process.env.KAFKA_BROKER_URL;
    RABBITMQ_URL = process.env.RABBITMQ_URL;
    USE_SIMULATED_QUEUES = process.env.USE_SIMULATED_QUEUES !== "false";
    KafkaService = class extends EventEmitter {
      connected = false;
      isSimulated = true;
      metrics;
      messageCount = 0;
      consumedCount = 0;
      topics = ["notifications", "audit-logs", "user-events", "billing-events", "system-alerts"];
      simulationInterval = null;
      constructor() {
        super();
        this.isSimulated = !KAFKA_BROKER_URL || USE_SIMULATED_QUEUES;
        this.metrics = {
          name: "Apache Kafka",
          status: "disconnected",
          messagesProduced: 0,
          messagesConsumed: 0,
          queueDepth: 0,
          latencyMs: 0,
          throughputPerSec: 0,
          consumers: 3,
          partitions: 12,
          topics: this.topics,
          lastActivity: (/* @__PURE__ */ new Date()).toISOString()
        };
      }
      startSimulation() {
        if (this.simulationInterval) return;
        this.simulationInterval = setInterval(() => {
          const produced = Math.floor(Math.random() * 50) + 10;
          const consumed = Math.floor(Math.random() * 45) + 8;
          this.messageCount += produced;
          this.consumedCount += consumed;
          this.metrics.messagesProduced = this.messageCount;
          this.metrics.messagesConsumed = this.consumedCount;
          this.metrics.queueDepth = Math.max(0, this.messageCount - this.consumedCount);
          this.metrics.latencyMs = Math.floor(Math.random() * 15) + 2;
          this.metrics.throughputPerSec = produced;
          this.metrics.lastActivity = (/* @__PURE__ */ new Date()).toISOString();
          this.emit("metrics", this.metrics);
        }, 5e3);
      }
      async connect() {
        try {
          if (this.isSimulated) {
            this.connected = true;
            this.metrics.status = "simulated";
            this.startSimulation();
            console.log("[Kafka] Connected (simulated mode)");
            return true;
          }
          console.log(`[Kafka] Connecting to ${KAFKA_BROKER_URL}...`);
          this.connected = true;
          this.metrics.status = "connected";
          console.log("[Kafka] Connected to production broker");
          return true;
        } catch (error) {
          console.error("[Kafka] Connection failed:", error);
          console.log("[Kafka] Falling back to simulated mode");
          this.isSimulated = true;
          this.connected = true;
          this.metrics.status = "simulated";
          this.startSimulation();
          return true;
        }
      }
      async disconnect() {
        this.connected = false;
        this.metrics.status = "disconnected";
        if (this.simulationInterval) {
          clearInterval(this.simulationInterval);
          this.simulationInterval = null;
        }
        console.log("[Kafka] Disconnected");
      }
      async produce(topic, message) {
        if (!this.connected) {
          console.warn("[Kafka] Not connected, message not sent");
          return;
        }
        if (!this.topics.includes(topic)) {
          this.topics.push(topic);
        }
        this.messageCount++;
        this.metrics.messagesProduced = this.messageCount;
        this.metrics.lastActivity = (/* @__PURE__ */ new Date()).toISOString();
        this.emit("message", { topic, message, timestamp: /* @__PURE__ */ new Date() });
      }
      async consume(topic, callback) {
        this.on("message", (msg) => {
          if (msg.topic === topic) {
            this.consumedCount++;
            this.metrics.messagesConsumed = this.consumedCount;
            callback({
              id: `kafka-${Date.now()}`,
              topic: msg.topic,
              payload: msg.message,
              timestamp: msg.timestamp
            });
          }
        });
      }
      getMetrics() {
        return { ...this.metrics, topics: this.topics };
      }
      isConnected() {
        return this.connected;
      }
      isSimulatedMode() {
        return this.isSimulated;
      }
    };
    RabbitMQService = class extends EventEmitter {
      connected = false;
      isSimulated = true;
      metrics;
      messageCount = 0;
      consumedCount = 0;
      queues = ["push-notifications", "email-queue", "webhook-delivery", "task-queue", "dead-letter"];
      simulationInterval = null;
      constructor() {
        super();
        this.isSimulated = !RABBITMQ_URL || USE_SIMULATED_QUEUES;
        this.metrics = {
          name: "RabbitMQ",
          status: "disconnected",
          messagesProduced: 0,
          messagesConsumed: 0,
          queueDepth: 0,
          latencyMs: 0,
          throughputPerSec: 0,
          consumers: 5,
          topics: this.queues,
          lastActivity: (/* @__PURE__ */ new Date()).toISOString()
        };
      }
      startSimulation() {
        if (this.simulationInterval) return;
        this.simulationInterval = setInterval(() => {
          const produced = Math.floor(Math.random() * 30) + 5;
          const consumed = Math.floor(Math.random() * 28) + 4;
          this.messageCount += produced;
          this.consumedCount += consumed;
          this.metrics.messagesProduced = this.messageCount;
          this.metrics.messagesConsumed = this.consumedCount;
          this.metrics.queueDepth = Math.max(0, this.messageCount - this.consumedCount);
          this.metrics.latencyMs = Math.floor(Math.random() * 8) + 1;
          this.metrics.throughputPerSec = produced;
          this.metrics.lastActivity = (/* @__PURE__ */ new Date()).toISOString();
          this.emit("metrics", this.metrics);
        }, 5e3);
      }
      async connect() {
        try {
          if (this.isSimulated) {
            this.connected = true;
            this.metrics.status = "simulated";
            this.startSimulation();
            console.log("[RabbitMQ] Connected (simulated mode)");
            return true;
          }
          console.log(`[RabbitMQ] Connecting to ${RABBITMQ_URL}...`);
          this.connected = true;
          this.metrics.status = "connected";
          console.log("[RabbitMQ] Connected to production broker");
          return true;
        } catch (error) {
          console.error("[RabbitMQ] Connection failed:", error);
          console.log("[RabbitMQ] Falling back to simulated mode");
          this.isSimulated = true;
          this.connected = true;
          this.metrics.status = "simulated";
          this.startSimulation();
          return true;
        }
      }
      async disconnect() {
        this.connected = false;
        this.metrics.status = "disconnected";
        if (this.simulationInterval) {
          clearInterval(this.simulationInterval);
          this.simulationInterval = null;
        }
        console.log("[RabbitMQ] Disconnected");
      }
      async publish(queue, message) {
        if (!this.connected) {
          console.warn("[RabbitMQ] Not connected, message not sent");
          return;
        }
        if (!this.queues.includes(queue)) {
          this.queues.push(queue);
        }
        this.messageCount++;
        this.metrics.messagesProduced = this.messageCount;
        this.metrics.lastActivity = (/* @__PURE__ */ new Date()).toISOString();
        this.emit("message", { queue, message, timestamp: /* @__PURE__ */ new Date() });
      }
      async subscribe(queue, callback) {
        this.on("message", (msg) => {
          if (msg.queue === queue) {
            this.consumedCount++;
            this.metrics.messagesConsumed = this.consumedCount;
            callback({
              id: `rabbitmq-${Date.now()}`,
              topic: msg.queue,
              payload: msg.message,
              timestamp: msg.timestamp
            });
          }
        });
      }
      getMetrics() {
        return { ...this.metrics, topics: this.queues };
      }
      isConnected() {
        return this.connected;
      }
      isSimulatedMode() {
        return this.isSimulated;
      }
    };
    RedisMetricsService = class {
      metrics = {
        name: "Redis",
        status: "simulated",
        connectedClients: 0,
        usedMemoryMB: 0,
        usedMemoryPeakMB: 0,
        totalCommands: 0,
        commandsPerSec: 0,
        hitRate: 0,
        keyCount: 0,
        expiredKeys: 0,
        evictedKeys: 0,
        uptimeSeconds: 0,
        lastActivity: (/* @__PURE__ */ new Date()).toISOString()
      };
      commandCount = 0;
      simulationInterval = null;
      startTime;
      constructor() {
        this.startTime = Date.now();
        this.startSimulation();
      }
      startSimulation() {
        if (this.simulationInterval) return;
        this.simulationInterval = setInterval(() => {
          const newCommands = Math.floor(Math.random() * 100) + 20;
          this.commandCount += newCommands;
          this.metrics.connectedClients = Math.floor(Math.random() * 10) + 5;
          this.metrics.usedMemoryMB = Math.floor(Math.random() * 50) + 30;
          this.metrics.usedMemoryPeakMB = Math.max(this.metrics.usedMemoryPeakMB, this.metrics.usedMemoryMB);
          this.metrics.totalCommands = this.commandCount;
          this.metrics.commandsPerSec = newCommands;
          this.metrics.hitRate = Math.floor(Math.random() * 10) + 90;
          this.metrics.keyCount = Math.floor(Math.random() * 500) + 200;
          this.metrics.expiredKeys = Math.floor(Math.random() * 20);
          this.metrics.evictedKeys = 0;
          this.metrics.uptimeSeconds = Math.floor((Date.now() - this.startTime) / 1e3);
          this.metrics.lastActivity = (/* @__PURE__ */ new Date()).toISOString();
        }, 5e3);
      }
      getMetrics() {
        return { ...this.metrics };
      }
    };
    kafkaService = new KafkaService();
    rabbitMQService = new RabbitMQService();
    redisMetricsService = new RedisMetricsService();
    kafkaService.connect();
    rabbitMQService.connect();
  }
});

// server/services/api-documentation.ts
function getApiDocumentation() {
  return apiEndpoints;
}
function generatePostmanCollection() {
  const collection = {
    info: {
      name: "Prometix DevOps Platform API",
      description: "Complete API documentation for the Prometix DevOps Platform - A comprehensive SaaS solution for Backend-as-a-Service, DevOps automation, and AI-powered assistance.",
      version: "1.0.0",
      schema: "https://schema.getpostman.com/json/collection/v2.1.0/collection.json"
    },
    auth: {
      type: "apikey",
      apikey: [
        { key: "key", value: "Cookie", type: "string" },
        { key: "value", value: "{{session_cookie}}", type: "string" },
        { key: "in", value: "header", type: "string" }
      ]
    },
    variable: [
      { key: "baseUrl", value: "http://localhost:5000", type: "string" },
      { key: "session_cookie", value: "", type: "string" }
    ],
    item: []
  };
  const categories = {};
  apiEndpoints.forEach((endpoint) => {
    if (!categories[endpoint.category]) {
      categories[endpoint.category] = [];
    }
    const request = {
      name: endpoint.name,
      request: {
        method: endpoint.method,
        header: [
          { key: "Content-Type", value: "application/json" }
        ],
        url: {
          raw: `{{baseUrl}}${endpoint.path}`,
          host: ["{{baseUrl}}"],
          path: endpoint.path.split("/").filter((p) => p)
        },
        description: endpoint.description
      }
    };
    if (endpoint.requestBody) {
      request.request.body = {
        mode: "raw",
        raw: JSON.stringify(endpoint.requestBody.example || {}, null, 2),
        options: { raw: { language: "json" } }
      };
    }
    if (endpoint.queryParams) {
      request.request.url.query = endpoint.queryParams.map((p) => ({
        key: p.name,
        value: "",
        description: `${p.description} (${p.type}, ${p.required ? "required" : "optional"})`
      }));
    }
    categories[endpoint.category].push(request);
  });
  collection.item = Object.entries(categories).map(([name, items]) => ({
    name,
    item: items
  }));
  return collection;
}
function generateTechnicalDocumentation() {
  const doc = `# Prometix DevOps Platform - Technical Documentation

## Table of Contents
1. [Overview](#overview)
2. [System Architecture](#system-architecture)
3. [Technology Stack](#technology-stack)
4. [Database Design](#database-design)
5. [API Reference](#api-reference)
6. [Security Architecture](#security-architecture)
7. [Message Queue Architecture](#message-queue-architecture)
8. [Local Development Setup](#local-development-setup)
9. [Deployment Guide](#deployment-guide)

---

## Overview

Prometix is a comprehensive DevOps Platform that combines Backend-as-a-Service capabilities with AI-powered DevOps automation. It provides developers with tools for code generation, infrastructure management, and intelligent assistance.

### Key Features
- **Backend-as-a-Service**: Generate complete APIs with authentication and database integration
- **DevOps Automation**: Generate Docker, CI/CD, Jenkins, Ansible configurations
- **AI-Powered Assistance**: 10+ AI tools for deployment simulation, code analysis, and more
- **Credit-Based Billing**: Pay-per-use model with premium subscriptions
- **Admin Dashboard**: Comprehensive platform management and monitoring
- **Message Queues**: Kafka and RabbitMQ integration for scalable event processing

---

## System Architecture

\`\`\`
\u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510
\u2502                         Prometix Platform                          \u2502
\u251C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524
\u2502  \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502
\u2502  \u2502   Frontend    \u2502  \u2502   Backend     \u2502  \u2502    External Services  \u2502   \u2502
\u2502  \u2502   (React)     \u2502\u2500\u2500\u2502   (Express)   \u2502\u2500\u2500\u2502                       \u2502   \u2502
\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502   \u2502
\u2502         \u2502                   \u2502          \u2502  \u2502  Google Gemini  \u2502  \u2502   \u2502
\u2502         \u2502                   \u2502          \u2502  \u2502     (AI API)    \u2502  \u2502   \u2502
\u2502         \u25BC                   \u25BC          \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502   \u2502
\u2502  \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502   \u2502
\u2502  \u2502        PostgreSQL Database        \u2502 \u2502  \u2502    Razorpay     \u2502  \u2502   \u2502
\u2502  \u2502   (Neon Serverless)              \u2502 \u2502  \u2502   (Payments)    \u2502  \u2502   \u2502
\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502   \u2502
\u2502         \u2502                              \u2502  \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502   \u2502
\u2502         \u2502                              \u2502  \u2502  Google OAuth   \u2502  \u2502   \u2502
\u2502         \u25BC                              \u2502  \u2502(Authentication) \u2502  \u2502   \u2502
\u2502  \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502   \u2502
\u2502  \u2502     Redis (Caching/Sessions)     \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502
\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             \u2502
\u2502         \u2502                                                           \u2502
\u2502         \u25BC                                                           \u2502
\u2502  \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                             \u2502
\u2502  \u2502   Message Queues                  \u2502                             \u2502
\u2502  \u2502   \u251C\u2500\u2500 Apache Kafka               \u2502                             \u2502
\u2502  \u2502   \u2514\u2500\u2500 RabbitMQ                   \u2502                             \u2502
\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             \u2502
\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518
\`\`\`

---

## Technology Stack

### Frontend
| Technology | Version | Purpose |
|------------|---------|---------|
| React | 18.x | UI Framework |
| TypeScript | 5.x | Type Safety |
| Vite | 5.x | Build Tool |
| TanStack Query | 5.x | Server State Management |
| Wouter | 3.x | Client-side Routing |
| Tailwind CSS | 3.x | Styling |
| Shadcn/UI | Latest | Component Library |
| Framer Motion | 11.x | Animations |

### Backend
| Technology | Version | Purpose |
|------------|---------|---------|
| Node.js | 20.x | Runtime |
| Express.js | 4.x | Web Framework |
| TypeScript | 5.x | Type Safety |
| Drizzle ORM | Latest | Database ORM |
| Passport.js | 0.7.x | Authentication |
| Web Push | 3.x | Push Notifications |
| prom-client | 15.x | Prometheus Metrics |

### Database & Storage
| Technology | Purpose |
|------------|---------|
| PostgreSQL (Neon) | Primary Database |
| Redis | Caching & Sessions |
| Apache Kafka | Event Streaming |
| RabbitMQ | Message Queue |

### External Services
| Service | Purpose |
|---------|---------|
| Google Gemini | AI/ML Processing |
| Razorpay | Payment Processing |
| Google OAuth | Social Authentication |

---

## Database Design

### Entity Relationship Diagram

\`\`\`
\u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510
\u2502      users       \u2502       \u2502     projects     \u2502
\u251C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524       \u251C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524
\u2502 id (PK)          \u2502\u2500\u2500\u2500\u2510   \u2502 id (PK)          \u2502
\u2502 username         \u2502   \u2502   \u2502 userId (FK)      \u2502\u25C4\u2500\u2500\u2510
\u2502 email            \u2502   \u2502   \u2502 name             \u2502   \u2502
\u2502 password         \u2502   \u2502   \u2502 type             \u2502   \u2502
\u2502 credits          \u2502   \u2502   \u2502 framework        \u2502   \u2502
\u2502 isPremium        \u2502   \u2502   \u2502 generatedCode    \u2502   \u2502
\u2502 isAdmin          \u2502   \u2502   \u2502 config           \u2502   \u2502
\u2502 provider         \u2502   \u2502   \u2502 createdAt        \u2502   \u2502
\u2502 googleId         \u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502
\u2502 adminGrantedBy   \u2502   \u2502                          \u2502
\u2502 adminGrantedAt   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518
\u2502 createdAt        \u2502
\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518
        \u2502
        \u2502
        \u25BC
\u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510
\u2502  usage_tracking  \u2502       \u2502     payments     \u2502
\u251C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524       \u251C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524
\u2502 id (PK)          \u2502       \u2502 id (PK)          \u2502
\u2502 userId (FK)      \u2502       \u2502 userId (FK)      \u2502
\u2502 featureName      \u2502       \u2502 razorpayOrderId  \u2502
\u2502 usageCount       \u2502       \u2502 razorpayPaymentId\u2502
\u2502 lastUsedAt       \u2502       \u2502 amount           \u2502
\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502 status           \u2502
                           \u2502 creditsAdded     \u2502
                           \u2502 createdAt        \u2502
                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518

\u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510
\u2502  subscriptions   \u2502       \u2502  security_logs   \u2502
\u251C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524       \u251C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524
\u2502 id (PK)          \u2502       \u2502 id (PK)          \u2502
\u2502 userId (FK)      \u2502       \u2502 userId (FK)      \u2502
\u2502 plan             \u2502       \u2502 action           \u2502
\u2502 status           \u2502       \u2502 ipAddress        \u2502
\u2502 razorpaySubId    \u2502       \u2502 userAgent        \u2502
\u2502 startDate        \u2502       \u2502 details          \u2502
\u2502 endDate          \u2502       \u2502 severity         \u2502
\u2502 createdAt        \u2502       \u2502 createdAt        \u2502
\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518

\u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510
\u2502push_subscriptions\u2502       \u2502admin_activity_log\u2502
\u251C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524       \u251C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524
\u2502 id (PK)          \u2502       \u2502 id (PK)          \u2502
\u2502 userId (FK)      \u2502       \u2502 adminId (FK)     \u2502
\u2502 endpoint         \u2502       \u2502 action           \u2502
\u2502 p256dh           \u2502       \u2502 targetUserId     \u2502
\u2502 auth             \u2502       \u2502 details          \u2502
\u2502 createdAt        \u2502       \u2502 createdAt        \u2502
\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518
\`\`\`

### Database Tables

| Table | Description |
|-------|-------------|
| users | User accounts with authentication and billing info |
| projects | Generated code projects |
| usage_tracking | Feature usage for billing limits |
| payments | Payment history |
| subscriptions | Premium subscription records |
| security_logs | Security event audit trail |
| push_subscriptions | Browser push notification subscriptions |
| admin_activity_log | Admin action audit trail |
| system_metrics | Prometheus metrics storage |

---

## API Reference

### Authentication Endpoints
- \`POST /api/auth/register\` - User registration
- \`POST /api/auth/login\` - User login
- \`POST /api/auth/logout\` - User logout
- \`GET /api/auth/google\` - Google OAuth flow

### Code Generation Endpoints
- \`POST /api/generate/api\` - Generate CRUD APIs
- \`POST /api/generate/docker\` - Generate Docker configs
- \`POST /api/generate/cicd\` - Generate CI/CD pipelines
- \`POST /api/generate/jenkins\` - Generate Jenkinsfiles
- \`POST /api/generate/ansible\` - Generate Ansible playbooks
- \`POST /api/generate/sonarqube\` - Generate SonarQube setup

### AI Feature Endpoints
- \`POST /api/ai/log-analysis\` - AI log analysis
- \`POST /api/ai/yaml-generation\` - Natural language to YAML
- \`POST /api/ai/docker-optimization\` - Dockerfile optimization
- \`POST /api/ai/deployment-simulator\` - Deployment simulation
- \`POST /api/ai/iac-autofix\` - IaC code analysis
- \`POST /api/ai/release-notes\` - Changelog generation
- \`POST /api/ai/secret-scanner\` - Secret detection
- \`POST /api/ai/cloud-optimizer\` - Cloud cost optimization
- \`POST /api/ai/infra-chat\` - Infrastructure chat
- \`POST /api/ai/blueprint-generator\` - Architecture blueprints
- \`POST /api/ai/postmortem\` - Incident reports

### Admin Endpoints
- \`GET /api/admin/stats\` - Platform statistics
- \`GET /api/admin/users\` - All users
- \`POST /api/admin/grant-admin\` - Grant admin access
- \`POST /api/admin/revoke-admin\` - Revoke admin access
- \`POST /api/admin/send-notification\` - Send push notification
- \`GET /api/admin/queue-metrics\` - Message queue metrics

---

## Security Architecture

### Authentication Flow

\`\`\`
\u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510
\u2502  Client  \u2502\u2500\u2500\u2500\u2500\u25B6\u2502 Passport \u2502\u2500\u2500\u2500\u2500\u25B6\u2502  Session \u2502\u2500\u2500\u2500\u2500\u25B6\u2502   Redis  \u2502
\u2502          \u2502     \u2502          \u2502     \u2502  Store   \u2502     \u2502(fallback)\u2502
\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518
                      \u2502                                    \u2502
                      \u25BC                                    \u25BC
               \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                        \u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510
               \u2502 Password \u2502                        \u2502PostgreSQL\u2502
               \u2502  Hash    \u2502                        \u2502 Sessions \u2502
               \u2502(bcrypt)  \u2502                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518
               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518
\`\`\`

### Security Measures
1. **Password Hashing**: bcrypt with salt rounds
2. **Session Management**: Redis-backed with PostgreSQL fallback
3. **CSRF Protection**: Express session tokens
4. **Rate Limiting**: express-rate-limit middleware
5. **Helmet**: Security headers
6. **Input Validation**: Zod schema validation
7. **Admin Access Control**: Role-based with primary admin designation

---

## Message Queue Architecture

### Kafka Topics
- \`notifications\` - User notifications
- \`audit-logs\` - System audit events
- \`user-events\` - User activity tracking
- \`billing-events\` - Payment and credit events
- \`system-alerts\` - System health alerts

### RabbitMQ Queues
- \`push-notifications\` - Browser push delivery
- \`email-queue\` - Email notifications
- \`webhook-delivery\` - Webhook dispatching
- \`task-queue\` - Background task processing
- \`dead-letter\` - Failed message handling

---

## Local Development Setup

### Prerequisites
- Node.js 20.x or higher
- npm 10.x or higher
- PostgreSQL database (or Neon account)
- Redis (optional, for caching)

### Windows Setup Instructions

\`\`\`bash
# 1. Clone the repository
git clone https://github.com/your-org/prometix.git
cd prometix

# 2. Install dependencies
npm install

# 3. Create environment file
copy .env.example .env

# 4. Configure environment variables
# Edit .env file with your credentials:
# - DATABASE_URL: Your PostgreSQL connection string
# - SESSION_SECRET: Random string for session encryption
# - GOOGLE_API_KEY: Your Google Gemini API key
# - RAZORPAY_KEY_ID: Razorpay test key ID
# - RAZORPAY_KEY_SECRET: Razorpay test secret
# - GOOGLE_CLIENT_ID: Google OAuth client ID (optional)
# - GOOGLE_CLIENT_SECRET: Google OAuth client secret (optional)

# 5. Push database schema
npm run db:push

# 6. Start development server
npm run dev

# The application will be available at http://localhost:5000
\`\`\`

### Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| DATABASE_URL | Yes | PostgreSQL connection string |
| SESSION_SECRET | Yes | Session encryption key |
| GOOGLE_API_KEY | Yes | Google Gemini API key |
| RAZORPAY_KEY_ID | Yes | Razorpay key ID |
| RAZORPAY_KEY_SECRET | Yes | Razorpay secret |
| GOOGLE_CLIENT_ID | No | Google OAuth client ID |
| GOOGLE_CLIENT_SECRET | No | Google OAuth secret |
| REDIS_URL | No | Redis connection URL |
| VAPID_PUBLIC_KEY | No | Web Push public key |
| VAPID_PRIVATE_KEY | No | Web Push private key |

---

## Deployment Guide

### Production Build

\`\`\`bash
# Build the application
npm run build

# The build output will be in:
# - dist/index.js (server bundle)
# - dist/public (client bundle)
\`\`\`

### Production Environment Variables
All development variables plus:
- \`NODE_ENV=production\`
- Production database URL
- Production API keys

### Health Check Endpoint
\`GET /api/health\` - Returns system health status

### Monitoring
- Prometheus metrics: \`GET /metrics\`
- Grafana dashboards available via admin panel

---

## Support

For technical support or questions:
- Primary Admin: agrawalmayank200228@gmail.com
- Documentation: /admin (Admin Dashboard)

---

*Generated on ${(/* @__PURE__ */ new Date()).toISOString()}*
*Prometix DevOps Platform v1.0.0*
`;
  return doc;
}
var apiEndpoints;
var init_api_documentation = __esm({
  "server/services/api-documentation.ts"() {
    "use strict";
    apiEndpoints = [
      {
        name: "Get Current User",
        method: "GET",
        path: "/api/user",
        description: "Retrieves the currently authenticated user's profile information including credits, premium status, and account details.",
        category: "Authentication",
        auth: "required",
        responses: [
          { status: 200, description: "User profile retrieved successfully", example: { id: 1, username: "johndoe", email: "john@example.com", credits: 100, isPremium: false } },
          { status: 401, description: "Not authenticated" }
        ]
      },
      {
        name: "Register User",
        method: "POST",
        path: "/api/auth/register",
        description: "Creates a new user account with email and password authentication.",
        category: "Authentication",
        auth: "none",
        requestBody: {
          contentType: "application/json",
          schema: {
            username: { type: "string", required: true, minLength: 3, maxLength: 50 },
            email: { type: "string", required: true, format: "email" },
            password: { type: "string", required: true, minLength: 6 }
          },
          example: { username: "johndoe", email: "john@example.com", password: "securepassword123" }
        },
        responses: [
          { status: 201, description: "User registered successfully" },
          { status: 400, description: "Validation error or user already exists" }
        ]
      },
      {
        name: "Login User",
        method: "POST",
        path: "/api/auth/login",
        description: "Authenticates a user with email/username and password, creating a session.",
        category: "Authentication",
        auth: "none",
        requestBody: {
          contentType: "application/json",
          schema: {
            email: { type: "string", required: true },
            password: { type: "string", required: true }
          },
          example: { email: "john@example.com", password: "securepassword123" }
        },
        responses: [
          { status: 200, description: "Login successful" },
          { status: 401, description: "Invalid credentials" }
        ]
      },
      {
        name: "Logout User",
        method: "POST",
        path: "/api/auth/logout",
        description: "Ends the current user session and clears authentication cookies.",
        category: "Authentication",
        auth: "required",
        responses: [
          { status: 200, description: "Logout successful" }
        ]
      },
      {
        name: "Google OAuth",
        method: "GET",
        path: "/api/auth/google",
        description: "Initiates Google OAuth 2.0 authentication flow. Redirects user to Google consent screen.",
        category: "Authentication",
        auth: "none",
        responses: [
          { status: 302, description: "Redirects to Google OAuth consent" }
        ]
      },
      {
        name: "Google OAuth Callback",
        method: "GET",
        path: "/api/auth/google/callback",
        description: "Handles the OAuth callback from Google and creates/updates user session.",
        category: "Authentication",
        auth: "none",
        queryParams: [
          { name: "code", type: "string", required: true, description: "Authorization code from Google" },
          { name: "state", type: "string", required: false, description: "State parameter for CSRF protection" }
        ],
        responses: [
          { status: 302, description: "Redirects to dashboard on success or auth page on failure" }
        ]
      },
      {
        name: "Get User Projects",
        method: "GET",
        path: "/api/projects",
        description: "Retrieves all projects created by the authenticated user.",
        category: "Projects",
        auth: "required",
        responses: [
          { status: 200, description: "List of user projects", example: [{ id: 1, name: "My API", type: "api", framework: "express" }] },
          { status: 401, description: "Not authenticated" }
        ]
      },
      {
        name: "Create Project",
        method: "POST",
        path: "/api/projects",
        description: "Creates a new project with generated code based on the specified configuration.",
        category: "Projects",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            name: { type: "string", required: true },
            type: { type: "string", enum: ["api", "docker", "cicd"], required: true },
            framework: { type: "string" },
            config: { type: "object" }
          },
          example: { name: "User API", type: "api", framework: "express", config: { endpoints: ["users", "auth"] } }
        },
        responses: [
          { status: 201, description: "Project created successfully" },
          { status: 400, description: "Validation error" }
        ]
      },
      {
        name: "Get Single Project",
        method: "GET",
        path: "/api/projects/:id",
        description: "Retrieves a specific project by its ID.",
        category: "Projects",
        auth: "required",
        pathParams: [
          { name: "id", type: "integer", description: "Project ID" }
        ],
        responses: [
          { status: 200, description: "Project details" },
          { status: 404, description: "Project not found" }
        ]
      },
      {
        name: "Get Usage Statistics",
        method: "GET",
        path: "/api/usage",
        description: "Retrieves the authenticated user's feature usage statistics and remaining limits.",
        category: "Usage & Billing",
        auth: "required",
        responses: [
          { status: 200, description: "Usage statistics", example: { apiGeneration: 5, dockerGeneration: 3, limit: 10 } }
        ]
      },
      {
        name: "Get Subscription Status",
        method: "GET",
        path: "/api/subscription",
        description: "Retrieves the current subscription status and plan details for the user.",
        category: "Usage & Billing",
        auth: "required",
        responses: [
          { status: 200, description: "Subscription details", example: { plan: "premium", status: "active", expiresAt: "2025-12-31" } }
        ]
      },
      {
        name: "Create Subscription",
        method: "POST",
        path: "/api/subscription/create",
        description: "Initiates a new premium subscription for the user.",
        category: "Usage & Billing",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            plan: { type: "string", enum: ["monthly", "yearly"], required: true }
          },
          example: { plan: "monthly" }
        },
        responses: [
          { status: 200, description: "Subscription order created" },
          { status: 400, description: "Already subscribed or invalid plan" }
        ]
      },
      {
        name: "Create Payment Order",
        method: "POST",
        path: "/api/payment/create-order",
        description: "Creates a Razorpay order for credit package purchase.",
        category: "Usage & Billing",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            package: { type: "string", enum: ["starter", "professional", "enterprise"], required: true }
          },
          example: { package: "professional" }
        },
        responses: [
          { status: 200, description: "Payment order created", example: { orderId: "order_xxx", amount: 49900, currency: "INR" } }
        ]
      },
      {
        name: "Verify Payment",
        method: "POST",
        path: "/api/payment/verify",
        description: "Verifies a Razorpay payment and adds credits to user account.",
        category: "Usage & Billing",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            razorpay_order_id: { type: "string", required: true },
            razorpay_payment_id: { type: "string", required: true },
            razorpay_signature: { type: "string", required: true }
          }
        },
        responses: [
          { status: 200, description: "Payment verified, credits added" },
          { status: 400, description: "Payment verification failed" }
        ]
      },
      {
        name: "Generate API Code",
        method: "POST",
        path: "/api/generate/api",
        description: "Generates complete CRUD API code for the specified framework with authentication and database integration.",
        category: "Code Generation",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            projectName: { type: "string", required: true },
            framework: { type: "string", enum: ["express", "fastapi", "nestjs"], required: true },
            database: { type: "string", enum: ["postgresql", "mongodb", "mysql"] },
            endpoints: { type: "array", items: { type: "string" } },
            authentication: { type: "boolean" }
          },
          example: { projectName: "UserService", framework: "express", database: "postgresql", endpoints: ["users", "posts"], authentication: true }
        },
        responses: [
          { status: 200, description: "Generated API code", example: { code: "...", files: ["index.js", "routes.js"] } },
          { status: 402, description: "Insufficient credits" }
        ]
      },
      {
        name: "Generate Docker Configuration",
        method: "POST",
        path: "/api/generate/docker",
        description: "Generates Dockerfile and docker-compose.yml for your application stack.",
        category: "Code Generation",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            projectType: { type: "string", enum: ["nodejs", "python", "java", "go"], required: true },
            includeDatabase: { type: "boolean" },
            includeRedis: { type: "boolean" },
            includeNginx: { type: "boolean" }
          },
          example: { projectType: "nodejs", includeDatabase: true, includeRedis: true, includeNginx: false }
        },
        responses: [
          { status: 200, description: "Generated Docker configuration" },
          { status: 402, description: "Insufficient credits" }
        ]
      },
      {
        name: "Generate CI/CD Pipeline",
        method: "POST",
        path: "/api/generate/cicd",
        description: "Generates GitHub Actions workflow file for CI/CD automation.",
        category: "Code Generation",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            projectType: { type: "string", required: true },
            deployTarget: { type: "string", enum: ["aws", "gcp", "azure", "vercel", "docker"] },
            includeTesting: { type: "boolean" },
            includeCodeQuality: { type: "boolean" }
          },
          example: { projectType: "nodejs", deployTarget: "aws", includeTesting: true, includeCodeQuality: true }
        },
        responses: [
          { status: 200, description: "Generated CI/CD pipeline" },
          { status: 402, description: "Insufficient credits" }
        ]
      },
      {
        name: "Generate Jenkins Pipeline",
        method: "POST",
        path: "/api/generate/jenkins",
        description: "Generates a comprehensive Jenkinsfile with multi-stage pipeline configuration.",
        category: "DevOps Scripts",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            projectType: { type: "string", enum: ["nodejs", "java", "python", "go"], required: true },
            stages: { type: "array", items: { type: "string" } },
            deploymentEnv: { type: "string" }
          },
          example: { projectType: "nodejs", stages: ["build", "test", "deploy"], deploymentEnv: "production" }
        },
        responses: [
          { status: 200, description: "Generated Jenkinsfile" },
          { status: 402, description: "Insufficient credits" }
        ]
      },
      {
        name: "Generate Ansible Playbook",
        method: "POST",
        path: "/api/generate/ansible",
        description: "Generates Ansible playbook for server configuration and automation.",
        category: "DevOps Scripts",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            playbookType: { type: "string", enum: ["server-setup", "web-server", "database", "docker", "security", "monitoring"], required: true },
            targetOS: { type: "string", enum: ["ubuntu", "centos", "rhel", "debian"] }
          },
          example: { playbookType: "server-setup", targetOS: "ubuntu" }
        },
        responses: [
          { status: 200, description: "Generated Ansible playbook" },
          { status: 402, description: "Insufficient credits" }
        ]
      },
      {
        name: "Generate SonarQube Setup",
        method: "POST",
        path: "/api/generate/sonarqube",
        description: "Generates SonarQube installation and configuration scripts.",
        category: "DevOps Scripts",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            deploymentType: { type: "string", enum: ["docker", "kubernetes", "manual", "zip", "rpm"], required: true },
            database: { type: "string", enum: ["postgresql", "mysql"] }
          },
          example: { deploymentType: "docker", database: "postgresql" }
        },
        responses: [
          { status: 200, description: "Generated SonarQube setup script" },
          { status: 402, description: "Insufficient credits" }
        ]
      },
      {
        name: "AI Log Analysis",
        method: "POST",
        path: "/api/ai/log-analysis",
        description: "Analyzes error logs using AI to identify root causes and provide solutions.",
        category: "AI Features",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            logs: { type: "string", required: true, minLength: 20 },
            context: { type: "string" }
          },
          example: { logs: "Error: ECONNREFUSED 127.0.0.1:5432...", context: "PostgreSQL connection issue" }
        },
        responses: [
          { status: 200, description: "AI analysis with root cause and solutions" },
          { status: 402, description: "Insufficient credits" }
        ]
      },
      {
        name: "AI YAML Generation",
        method: "POST",
        path: "/api/ai/yaml-generation",
        description: "Converts natural language descriptions into DevOps YAML configurations.",
        category: "AI Features",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            description: { type: "string", required: true, minLength: 10 },
            type: { type: "string", enum: ["kubernetes", "docker-compose", "github-actions", "ansible"] }
          },
          example: { description: "Deploy a Node.js app with 3 replicas and 2GB memory limit", type: "kubernetes" }
        },
        responses: [
          { status: 200, description: "Generated YAML configuration" },
          { status: 402, description: "Insufficient credits" }
        ]
      },
      {
        name: "AI Docker Optimization",
        method: "POST",
        path: "/api/ai/docker-optimization",
        description: "Analyzes Dockerfile and provides optimization recommendations for smaller, faster images.",
        category: "AI Features",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            dockerfile: { type: "string", required: true, minLength: 20 }
          },
          example: { dockerfile: "FROM node:18\\nWORKDIR /app\\nCOPY . .\\nRUN npm install..." }
        },
        responses: [
          { status: 200, description: "Optimization suggestions and improved Dockerfile" },
          { status: 402, description: "Insufficient credits" }
        ]
      },
      {
        name: "AI Deployment Simulator",
        method: "POST",
        path: "/api/ai/deployment-simulator",
        description: "Simulates deployment and predicts potential failures, costs, and scaling needs.",
        category: "AI Features - Advanced",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            config: { type: "string", required: true, minLength: 20 },
            environment: { type: "string", enum: ["development", "staging", "production"] },
            provider: { type: "string", enum: ["aws", "gcp", "azure", "kubernetes"] }
          },
          example: { config: "apiVersion: apps/v1...", environment: "production", provider: "aws" }
        },
        responses: [
          { status: 200, description: "Deployment simulation results with risk analysis" },
          { status: 402, description: "Insufficient credits" }
        ]
      },
      {
        name: "AI IaC Autofix",
        method: "POST",
        path: "/api/ai/iac-autofix",
        description: "Analyzes Infrastructure-as-Code for drift, syntax errors, and security vulnerabilities.",
        category: "AI Features - Advanced",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            code: { type: "string", required: true, minLength: 20 },
            type: { type: "string", enum: ["terraform", "pulumi", "cloudformation"] }
          },
          example: { code: 'resource "aws_s3_bucket" "example" {...}', type: "terraform" }
        },
        responses: [
          { status: 200, description: "Analysis results with auto-fix suggestions" },
          { status: 402, description: "Insufficient credits" }
        ]
      },
      {
        name: "AI Release Notes Generator",
        method: "POST",
        path: "/api/ai/release-notes",
        description: "Generates changelogs from commit messages with semantic versioning suggestions.",
        category: "AI Features - Advanced",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            commits: { type: "string", required: true, minLength: 20 },
            format: { type: "string", enum: ["markdown", "html", "slack"] }
          },
          example: { commits: "feat: add user auth\\nfix: resolve login bug\\nchore: update deps", format: "markdown" }
        },
        responses: [
          { status: 200, description: "Generated release notes" },
          { status: 402, description: "Insufficient credits" }
        ]
      },
      {
        name: "AI Secret Scanner",
        method: "POST",
        path: "/api/ai/secret-scanner",
        description: "Scans code for exposed secrets like API keys, passwords, and tokens.",
        category: "AI Features - Advanced",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            code: { type: "string", required: true, minLength: 20 }
          },
          example: { code: "const API_KEY = 'sk-xxxx'..." }
        },
        responses: [
          { status: 200, description: "Scan results with detected secrets and remediation" },
          { status: 402, description: "Insufficient credits" }
        ]
      },
      {
        name: "AI Cloud Cost Optimizer",
        method: "POST",
        path: "/api/ai/cloud-optimizer",
        description: "Analyzes cloud infrastructure for cost optimization opportunities.",
        category: "AI Features - Advanced",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            infrastructure: { type: "string", required: true, minLength: 20 },
            provider: { type: "string", enum: ["aws", "gcp", "azure"] }
          },
          example: { infrastructure: "3x m5.xlarge instances running 24/7...", provider: "aws" }
        },
        responses: [
          { status: 200, description: "Cost analysis with saving recommendations" },
          { status: 402, description: "Insufficient credits" }
        ]
      },
      {
        name: "AI Infrastructure Chat",
        method: "POST",
        path: "/api/ai/infra-chat",
        description: "Natural language interface to interact with infrastructure and get recommendations.",
        category: "AI Features - Advanced",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            query: { type: "string", required: true, minLength: 10 }
          },
          example: { query: "How can I scale my Kubernetes deployment to handle 10x traffic?" }
        },
        responses: [
          { status: 200, description: "AI response with infrastructure guidance" },
          { status: 402, description: "Insufficient credits" }
        ]
      },
      {
        name: "AI Blueprint Generator",
        method: "POST",
        path: "/api/ai/blueprint-generator",
        description: "Generates complete architecture blueprints including Terraform, CI/CD, and security configs.",
        category: "AI Features - Advanced",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            requirements: { type: "string", required: true, minLength: 20 },
            scale: { type: "string", enum: ["startup", "growth", "enterprise"] }
          },
          example: { requirements: "E-commerce platform with user auth, payments, and inventory", scale: "growth" }
        },
        responses: [
          { status: 200, description: "Complete architecture blueprint" },
          { status: 402, description: "Insufficient credits" }
        ]
      },
      {
        name: "AI Post-Mortem Generator",
        method: "POST",
        path: "/api/ai/postmortem",
        description: "Creates comprehensive incident reports with timeline and root cause analysis.",
        category: "AI Features - Advanced",
        auth: "required",
        requestBody: {
          contentType: "application/json",
          schema: {
            incident: { type: "string", required: true, minLength: 20 },
            severity: { type: "string", enum: ["low", "medium", "high", "critical"] }
          },
          example: { incident: "Database connection pool exhausted causing 15 min downtime", severity: "high" }
        },
        responses: [
          { status: 200, description: "Generated post-mortem report" },
          { status: 402, description: "Insufficient credits" }
        ]
      },
      {
        name: "Admin - Platform Statistics",
        method: "GET",
        path: "/api/admin/stats",
        description: "Retrieves platform-wide statistics including user count, projects, payments, and admin count.",
        category: "Admin",
        auth: "admin",
        responses: [
          { status: 200, description: "Platform statistics", example: { totalUsers: 150, totalProjects: 300, totalPayments: 50 } },
          { status: 403, description: "Admin access required" }
        ]
      },
      {
        name: "Admin - Get All Users",
        method: "GET",
        path: "/api/admin/users",
        description: "Retrieves a list of all registered users with their details (excluding passwords).",
        category: "Admin",
        auth: "admin",
        responses: [
          { status: 200, description: "List of all users" },
          { status: 403, description: "Admin access required" }
        ]
      },
      {
        name: "Admin - Get Admin Users",
        method: "GET",
        path: "/api/admin/admins",
        description: "Retrieves a list of users with admin privileges.",
        category: "Admin",
        auth: "admin",
        responses: [
          { status: 200, description: "List of admin users" },
          { status: 403, description: "Admin access required" }
        ]
      },
      {
        name: "Admin - Grant Admin Access",
        method: "POST",
        path: "/api/admin/grant-admin",
        description: "Grants admin privileges to a user by their email address.",
        category: "Admin",
        auth: "admin",
        requestBody: {
          contentType: "application/json",
          schema: {
            email: { type: "string", required: true, format: "email" }
          },
          example: { email: "newadmin@example.com" }
        },
        responses: [
          { status: 200, description: "Admin access granted" },
          { status: 404, description: "User not found" },
          { status: 403, description: "Admin access required" }
        ]
      },
      {
        name: "Admin - Revoke Admin Access",
        method: "POST",
        path: "/api/admin/revoke-admin",
        description: "Revokes admin privileges from a user (cannot revoke primary admin).",
        category: "Admin",
        auth: "admin",
        requestBody: {
          contentType: "application/json",
          schema: {
            userId: { type: "integer", required: true }
          },
          example: { userId: 5 }
        },
        responses: [
          { status: 200, description: "Admin access revoked" },
          { status: 403, description: "Cannot revoke primary admin" }
        ]
      },
      {
        name: "Admin - Security Logs",
        method: "GET",
        path: "/api/admin/security-logs",
        description: "Retrieves security event logs including login attempts and admin actions.",
        category: "Admin",
        auth: "admin",
        queryParams: [
          { name: "limit", type: "integer", required: false, description: "Number of logs to retrieve (default: 100)" }
        ],
        responses: [
          { status: 200, description: "List of security logs" },
          { status: 403, description: "Admin access required" }
        ]
      },
      {
        name: "Admin - Activity Logs",
        method: "GET",
        path: "/api/admin/activity-logs",
        description: "Retrieves admin activity audit trail.",
        category: "Admin",
        auth: "admin",
        queryParams: [
          { name: "limit", type: "integer", required: false, description: "Number of logs to retrieve (default: 100)" }
        ],
        responses: [
          { status: 200, description: "List of activity logs" }
        ]
      },
      {
        name: "Admin - Send Push Notification",
        method: "POST",
        path: "/api/admin/send-notification",
        description: "Sends a push notification to all subscribers or specific users.",
        category: "Admin",
        auth: "admin",
        requestBody: {
          contentType: "application/json",
          schema: {
            title: { type: "string", required: true },
            body: { type: "string", required: true },
            url: { type: "string" },
            targetType: { type: "string", enum: ["all", "premium", "free"] }
          },
          example: { title: "New Feature!", body: "Check out our new AI features", url: "/dashboard", targetType: "all" }
        },
        responses: [
          { status: 200, description: "Notification sent successfully" },
          { status: 403, description: "Admin access required" }
        ]
      },
      {
        name: "Admin - Queue Metrics",
        method: "GET",
        path: "/api/admin/queue-metrics",
        description: "Retrieves performance metrics for Kafka, RabbitMQ, and Redis.",
        category: "Admin",
        auth: "admin",
        responses: [
          { status: 200, description: "Queue metrics for all message systems" },
          { status: 403, description: "Admin access required" }
        ]
      },
      {
        name: "Admin - API Documentation",
        method: "GET",
        path: "/api/admin/api-docs",
        description: "Retrieves complete API documentation for all endpoints.",
        category: "Admin",
        auth: "admin",
        responses: [
          { status: 200, description: "Complete API documentation" }
        ]
      },
      {
        name: "Admin - Export Postman Collection",
        method: "GET",
        path: "/api/admin/postman-collection",
        description: "Exports API documentation as a Postman collection JSON file.",
        category: "Admin",
        auth: "admin",
        responses: [
          { status: 200, description: "Postman collection JSON" }
        ]
      },
      {
        name: "Admin - Download Technical Docs",
        method: "GET",
        path: "/api/admin/download-docs",
        description: "Downloads comprehensive technical documentation including architecture, diagrams, and setup instructions.",
        category: "Admin",
        auth: "admin",
        responses: [
          { status: 200, description: "Markdown documentation file" }
        ]
      },
      {
        name: "Admin - Database Tables",
        method: "GET",
        path: "/api/admin/database-tables",
        description: "Retrieves list of all database tables with column counts.",
        category: "Admin",
        auth: "admin",
        responses: [
          { status: 200, description: "List of database tables" }
        ]
      },
      {
        name: "Admin - Table Schema",
        method: "GET",
        path: "/api/admin/table-schema/:tableName",
        description: "Retrieves detailed schema for a specific database table.",
        category: "Admin",
        auth: "admin",
        pathParams: [
          { name: "tableName", type: "string", description: "Name of the database table" }
        ],
        responses: [
          { status: 200, description: "Table column definitions" }
        ]
      },
      {
        name: "Prometheus Metrics",
        method: "GET",
        path: "/metrics",
        description: "Exposes Prometheus-compatible metrics for monitoring and alerting.",
        category: "Monitoring",
        auth: "none",
        responses: [
          { status: 200, description: "Prometheus metrics in text format" }
        ]
      },
      {
        name: "Health Check",
        method: "GET",
        path: "/api/health",
        description: "Returns the health status of the application and its dependencies.",
        category: "Monitoring",
        auth: "none",
        responses: [
          { status: 200, description: "Health status", example: { status: "ok", database: "connected", redis: "connected" } }
        ]
      }
    ];
  }
});

// server/routes/admin.ts
import { Router } from "express";
import { sql as sql2 } from "drizzle-orm";
function isAdmin(req, res, next) {
  if (!req.isAuthenticated()) {
    return res.status(401).json({ error: "Unauthorized" });
  }
  const user = req.user;
  if (!user.isAdmin && user.email !== PRIMARY_ADMIN_EMAIL) {
    return res.status(403).json({ error: "Forbidden: Admin access required" });
  }
  next();
}
var router, PRIMARY_ADMIN_EMAIL, admin_default;
var init_admin = __esm({
  "server/routes/admin.ts"() {
    "use strict";
    init_storage();
    init_metrics();
    init_db();
    init_push_notifications();
    init_message_queue();
    init_api_documentation();
    router = Router();
    PRIMARY_ADMIN_EMAIL = "agrawalmayank200228@gmail.com";
    router.get("/stats", isAdmin, async (req, res) => {
      try {
        const stats = await storage.getPlatformStats();
        await storage.createAdminActivityLog({
          adminId: req.user.id,
          action: "view_stats",
          details: { timestamp: (/* @__PURE__ */ new Date()).toISOString() }
        });
        res.json(stats);
      } catch (error) {
        console.error("Error fetching admin stats:", error);
        res.status(500).json({ error: "Failed to fetch stats" });
      }
    });
    router.get("/users", isAdmin, async (req, res) => {
      try {
        const users2 = await storage.getAllUsers();
        await storage.createAdminActivityLog({
          adminId: req.user.id,
          action: "view_users",
          details: { count: users2.length }
        });
        const sanitizedUsers = users2.map((u) => ({
          ...u,
          password: void 0
        }));
        res.json(sanitizedUsers);
      } catch (error) {
        console.error("Error fetching users:", error);
        res.status(500).json({ error: "Failed to fetch users" });
      }
    });
    router.get("/admins", isAdmin, async (req, res) => {
      try {
        const admins = await storage.getAdminUsers();
        res.json(admins.map((a) => ({
          ...a,
          password: void 0
        })));
      } catch (error) {
        console.error("Error fetching admins:", error);
        res.status(500).json({ error: "Failed to fetch admins" });
      }
    });
    router.post("/grant-admin", isAdmin, async (req, res) => {
      try {
        const { email } = req.body;
        if (!email) {
          return res.status(400).json({ error: "Email is required" });
        }
        const user = await storage.getUserByEmail(email);
        if (!user) {
          return res.status(404).json({ error: "User not found" });
        }
        const adminUser = req.user;
        const updatedUser = await storage.grantAdminAccess(user.id, adminUser.email);
        await storage.createAdminActivityLog({
          adminId: adminUser.id,
          action: "grant_admin",
          targetUserId: user.id,
          details: { targetEmail: email }
        });
        await storage.createSecurityLog({
          userId: user.id,
          action: "admin_action",
          details: {
            action: "admin_access_granted",
            grantedBy: adminUser.email
          },
          severity: "warning"
        });
        res.json({
          message: "Admin access granted successfully",
          user: { ...updatedUser, password: void 0 }
        });
      } catch (error) {
        console.error("Error granting admin access:", error);
        res.status(500).json({ error: "Failed to grant admin access" });
      }
    });
    router.post("/revoke-admin", isAdmin, async (req, res) => {
      try {
        const { userId } = req.body;
        if (!userId) {
          return res.status(400).json({ error: "User ID is required" });
        }
        const targetUser = await storage.getUser(userId);
        if (!targetUser) {
          return res.status(404).json({ error: "User not found" });
        }
        if (targetUser.email === PRIMARY_ADMIN_EMAIL) {
          return res.status(403).json({ error: "Cannot revoke primary admin access" });
        }
        const adminUser = req.user;
        const updatedUser = await storage.revokeAdminAccess(userId);
        await storage.createAdminActivityLog({
          adminId: adminUser.id,
          action: "revoke_admin",
          targetUserId: userId,
          details: { targetEmail: targetUser.email }
        });
        await storage.createSecurityLog({
          userId,
          action: "admin_action",
          details: {
            action: "admin_access_revoked",
            revokedBy: adminUser.email
          },
          severity: "warning"
        });
        res.json({
          message: "Admin access revoked successfully",
          user: { ...updatedUser, password: void 0 }
        });
      } catch (error) {
        console.error("Error revoking admin access:", error);
        res.status(500).json({ error: "Failed to revoke admin access" });
      }
    });
    router.get("/security-logs", isAdmin, async (req, res) => {
      try {
        const limit = parseInt(req.query.limit) || 100;
        const logs = await storage.getSecurityLogs(limit);
        await storage.createAdminActivityLog({
          adminId: req.user.id,
          action: "view_security_logs",
          details: { limit }
        });
        res.json(logs);
      } catch (error) {
        console.error("Error fetching security logs:", error);
        res.status(500).json({ error: "Failed to fetch security logs" });
      }
    });
    router.get("/activity-logs", isAdmin, async (req, res) => {
      try {
        const limit = parseInt(req.query.limit) || 100;
        const logs = await storage.getAdminActivityLogs(limit);
        res.json(logs);
      } catch (error) {
        console.error("Error fetching activity logs:", error);
        res.status(500).json({ error: "Failed to fetch activity logs" });
      }
    });
    router.get("/payments", isAdmin, async (req, res) => {
      try {
        const limit = parseInt(req.query.limit) || 100;
        const payments2 = await storage.getAllPayments(limit);
        res.json(payments2);
      } catch (error) {
        console.error("Error fetching payments:", error);
        res.status(500).json({ error: "Failed to fetch payments" });
      }
    });
    router.get("/subscriptions", isAdmin, async (req, res) => {
      try {
        const limit = parseInt(req.query.limit) || 100;
        const subscriptions2 = await storage.getAllSubscriptions(limit);
        res.json(subscriptions2);
      } catch (error) {
        console.error("Error fetching subscriptions:", error);
        res.status(500).json({ error: "Failed to fetch subscriptions" });
      }
    });
    router.get("/api-endpoints", isAdmin, async (_req, res) => {
      const docs = getApiDocumentation();
      res.json(docs.map((d) => ({
        method: d.method,
        path: d.path,
        description: d.description,
        name: d.name,
        category: d.category,
        auth: d.auth
      })));
    });
    router.get("/database-tables", isAdmin, async (_req, res) => {
      try {
        const tablesResult = await db.execute(sql2`
      SELECT table_name, 
             (SELECT COUNT(*) FROM information_schema.columns WHERE columns.table_name = tables.table_name) as column_count
      FROM information_schema.tables 
      WHERE table_schema = 'public' 
      ORDER BY table_name
    `);
        const tables = tablesResult.rows.map((row) => ({
          name: row.table_name,
          columnCount: parseInt(row.column_count)
        }));
        res.json(tables);
      } catch (error) {
        console.error("Error fetching database tables:", error);
        res.status(500).json({ error: "Failed to fetch database tables" });
      }
    });
    router.get("/table-schema/:tableName", isAdmin, async (req, res) => {
      try {
        const { tableName } = req.params;
        const columnsResult = await db.execute(sql2`
      SELECT column_name, data_type, is_nullable, column_default
      FROM information_schema.columns 
      WHERE table_name = ${tableName}
      ORDER BY ordinal_position
    `);
        res.json(columnsResult.rows);
      } catch (error) {
        console.error("Error fetching table schema:", error);
        res.status(500).json({ error: "Failed to fetch table schema" });
      }
    });
    router.get("/metrics", isAdmin, async (_req, res) => {
      try {
        res.set("Content-Type", getContentType());
        res.end(await getMetrics());
      } catch (error) {
        console.error("Error fetching metrics:", error);
        res.status(500).json({ error: "Failed to fetch metrics" });
      }
    });
    router.get("/grafana-config", isAdmin, async (_req, res) => {
      res.json({
        prometheusEndpoint: "/api/admin/metrics",
        dashboards: [
          { name: "System Overview", id: "system-overview" },
          { name: "API Performance", id: "api-performance" },
          { name: "User Analytics", id: "user-analytics" },
          { name: "Security Events", id: "security-events" }
        ]
      });
    });
    router.post("/send-notification", isAdmin, async (req, res) => {
      try {
        const { title, body: body2, url, targetType } = req.body;
        if (!title || !body2) {
          return res.status(400).json({ error: "Title and body are required" });
        }
        const payload = {
          title,
          body: body2,
          url: url || "/dashboard",
          tag: `admin-notification-${Date.now()}`
        };
        if (targetType === "all" || !targetType) {
          await sendBroadcastNotification(payload);
        } else {
          const users2 = await storage.getAllUsers();
          const targetUsers = users2.filter((u) => {
            if (targetType === "premium") return u.isPremium;
            if (targetType === "free") return !u.isPremium;
            return true;
          });
          for (const user of targetUsers) {
            try {
              await sendPushNotification(user.id, payload);
            } catch (e) {
            }
          }
        }
        await storage.createAdminActivityLog({
          adminId: req.user.id,
          action: "send_notification",
          details: { title, targetType: targetType || "all" }
        });
        res.json({
          success: true,
          message: "Notification sent successfully",
          targetType: targetType || "all"
        });
      } catch (error) {
        console.error("Error sending notification:", error);
        res.status(500).json({ error: "Failed to send notification" });
      }
    });
    router.get("/queue-metrics", isAdmin, async (_req, res) => {
      try {
        const metrics = getAllQueueMetrics();
        res.json(metrics);
      } catch (error) {
        console.error("Error fetching queue metrics:", error);
        res.status(500).json({ error: "Failed to fetch queue metrics" });
      }
    });
    router.get("/api-docs", isAdmin, async (_req, res) => {
      try {
        const docs = getApiDocumentation();
        res.json(docs);
      } catch (error) {
        console.error("Error fetching API docs:", error);
        res.status(500).json({ error: "Failed to fetch API documentation" });
      }
    });
    router.get("/postman-collection", isAdmin, async (_req, res) => {
      try {
        const collection = generatePostmanCollection();
        res.setHeader("Content-Type", "application/json");
        res.setHeader("Content-Disposition", "attachment; filename=prometix-api-collection.json");
        res.json(collection);
      } catch (error) {
        console.error("Error generating Postman collection:", error);
        res.status(500).json({ error: "Failed to generate Postman collection" });
      }
    });
    router.get("/download-docs", isAdmin, async (_req, res) => {
      try {
        const documentation = generateTechnicalDocumentation();
        res.setHeader("Content-Type", "text/markdown");
        res.setHeader("Content-Disposition", "attachment; filename=Prometix-Technical-Documentation.md");
        res.send(documentation);
      } catch (error) {
        console.error("Error generating documentation:", error);
        res.status(500).json({ error: "Failed to generate documentation" });
      }
    });
    router.get("/notification-templates", isAdmin, async (_req, res) => {
      const templates = [
        {
          id: "upgrade-premium",
          name: "Upgrade to Premium",
          title: "Unlock Premium Features!",
          body: "Get unlimited AI features, priority support, and more. Upgrade now and save 20%!",
          url: "/checkout"
        },
        {
          id: "new-feature",
          name: "New Feature Announcement",
          title: "New Feature Released!",
          body: "Check out our latest AI-powered tool for DevOps automation.",
          url: "/dashboard"
        },
        {
          id: "maintenance",
          name: "Scheduled Maintenance",
          title: "Scheduled Maintenance",
          body: "Platform maintenance scheduled. Services may be briefly unavailable.",
          url: "/"
        },
        {
          id: "welcome-back",
          name: "Welcome Back",
          title: "We Miss You!",
          body: "It's been a while! Come back and explore our new features.",
          url: "/dashboard"
        },
        {
          id: "credits-low",
          name: "Low Credits Warning",
          title: "Credits Running Low",
          body: "Your credits are running low. Top up now to continue using premium features.",
          url: "/checkout"
        },
        {
          id: "security-update",
          name: "Security Update",
          title: "Important Security Update",
          body: "We've made security improvements to protect your account.",
          url: "/dashboard"
        }
      ];
      res.json(templates);
    });
    router.get("/domain-configs", isAdmin, async (_req, res) => {
      try {
        const configs = await storage.getDomainConfigs();
        res.json(configs);
      } catch (error) {
        console.error("Error fetching domain configs:", error);
        res.status(500).json({ error: "Failed to fetch domain configs" });
      }
    });
    router.put("/domain-configs/:domain", isAdmin, async (req, res) => {
      try {
        const { domain } = req.params;
        const { isEnabled, comingSoonMessage } = req.body;
        const updated = await storage.updateDomainConfig(
          domain,
          isEnabled ?? false,
          comingSoonMessage || null,
          req.user.id
        );
        await storage.createAdminActivityLog({
          adminId: req.user.id,
          action: "update_domain_config",
          details: { domain, isEnabled, message: comingSoonMessage }
        });
        res.json(updated);
      } catch (error) {
        console.error("Error updating domain config:", error);
        res.status(500).json({ error: "Failed to update domain config" });
      }
    });
    router.post("/ban-user", isAdmin, async (req, res) => {
      try {
        const { userId, reason } = req.body;
        if (!userId || !reason) {
          return res.status(400).json({ error: "User ID and reason are required" });
        }
        const targetUser = await storage.getUser(userId);
        if (!targetUser) {
          return res.status(404).json({ error: "User not found" });
        }
        if (targetUser.email === PRIMARY_ADMIN_EMAIL) {
          return res.status(403).json({ error: "Cannot ban primary admin" });
        }
        const bannedUser = await storage.banUser(userId, reason);
        await storage.createAdminActivityLog({
          adminId: req.user.id,
          action: "ban_user",
          targetUserId: userId,
          details: { reason }
        });
        await storage.createSecurityLog({
          userId,
          action: "admin_action",
          details: { action: "user_banned", reason, bannedBy: req.user.email },
          severity: "critical"
        });
        res.json({ message: "User banned successfully", user: { ...bannedUser, password: void 0 } });
      } catch (error) {
        console.error("Error banning user:", error);
        res.status(500).json({ error: "Failed to ban user" });
      }
    });
    router.post("/unban-user", isAdmin, async (req, res) => {
      try {
        const { userId } = req.body;
        if (!userId) {
          return res.status(400).json({ error: "User ID is required" });
        }
        const unbannedUser = await storage.unbanUser(userId);
        await storage.createAdminActivityLog({
          adminId: req.user.id,
          action: "unban_user",
          targetUserId: userId,
          details: {}
        });
        res.json({ message: "User unbanned successfully", user: { ...unbannedUser, password: void 0 } });
      } catch (error) {
        console.error("Error unbanning user:", error);
        res.status(500).json({ error: "Failed to unban user" });
      }
    });
    router.post("/update-credits", isAdmin, async (req, res) => {
      try {
        const { email, credits } = req.body;
        if (!email || credits === void 0) {
          return res.status(400).json({ error: "Email and credits are required" });
        }
        const targetUser = await storage.getUserByEmail(email);
        if (!targetUser) {
          return res.status(404).json({ error: "User not found" });
        }
        const updatedUser = await storage.updateUserCredits(targetUser.id, credits);
        await storage.createAdminActivityLog({
          adminId: req.user.id,
          action: "update_credits",
          targetUserId: targetUser.id,
          details: { email, previousCredits: targetUser.credits, newCredits: credits }
        });
        res.json({
          message: "Credits updated successfully",
          user: { ...updatedUser, password: void 0 }
        });
      } catch (error) {
        console.error("Error updating credits:", error);
        res.status(500).json({ error: "Failed to update credits" });
      }
    });
    router.get("/search-users", isAdmin, async (req, res) => {
      try {
        const query = req.query.q;
        if (!query) {
          return res.status(400).json({ error: "Search query is required" });
        }
        const users2 = await storage.searchUsers(query);
        res.json(users2.map((u) => ({ ...u, password: void 0 })));
      } catch (error) {
        console.error("Error searching users:", error);
        res.status(500).json({ error: "Failed to search users" });
      }
    });
    router.get("/enhanced-stats", isAdmin, async (req, res) => {
      try {
        const stats = await storage.getPlatformStats();
        const now = /* @__PURE__ */ new Date();
        const last24h = new Date(now.getTime() - 24 * 60 * 60 * 1e3);
        const last7d = new Date(now.getTime() - 7 * 24 * 60 * 60 * 1e3);
        const last30d = new Date(now.getTime() - 30 * 24 * 60 * 60 * 1e3);
        const activeUsers24h = await storage.getActiveUsersCount(last24h);
        const activeUsers7d = await storage.getActiveUsersCount(last7d);
        const activeUsers30d = await storage.getActiveUsersCount(last30d);
        res.json({
          ...stats,
          activeUsers: {
            last24Hours: activeUsers24h,
            last7Days: activeUsers7d,
            last30Days: activeUsers30d
          }
        });
      } catch (error) {
        console.error("Error fetching enhanced stats:", error);
        res.status(500).json({ error: "Failed to fetch enhanced stats" });
      }
    });
    router.get("/incidents", isAdmin, async (req, res) => {
      try {
        const status = req.query.status;
        const incidents2 = await storage.getAllIncidents(status);
        res.json(incidents2);
      } catch (error) {
        console.error("Error fetching incidents:", error);
        res.status(500).json({ error: "Failed to fetch incidents" });
      }
    });
    router.get("/incidents/stats", isAdmin, async (_req, res) => {
      try {
        const stats = await storage.getIncidentStats();
        res.json(stats);
      } catch (error) {
        console.error("Error fetching incident stats:", error);
        res.status(500).json({ error: "Failed to fetch incident stats" });
      }
    });
    router.get("/incidents/:id", isAdmin, async (req, res) => {
      try {
        const id = parseInt(req.params.id);
        const incident = await storage.getIncident(id);
        if (!incident) {
          return res.status(404).json({ error: "Incident not found" });
        }
        const messages = await storage.getIncidentMessages(id);
        const user = await storage.getUser(incident.userId);
        res.json({ incident, messages, user: user ? { ...user, password: void 0 } : null });
      } catch (error) {
        console.error("Error fetching incident:", error);
        res.status(500).json({ error: "Failed to fetch incident" });
      }
    });
    router.put("/incidents/:id/status", isAdmin, async (req, res) => {
      try {
        const id = parseInt(req.params.id);
        const { status, resolution } = req.body;
        const incident = await storage.updateIncidentStatus(id, status, resolution, req.user.id);
        await storage.createAdminActivityLog({
          adminId: req.user.id,
          action: "update_incident",
          details: { incidentId: id, status, resolution }
        });
        res.json(incident);
      } catch (error) {
        console.error("Error updating incident:", error);
        res.status(500).json({ error: "Failed to update incident" });
      }
    });
    router.post("/incidents/:id/message", isAdmin, async (req, res) => {
      try {
        const id = parseInt(req.params.id);
        const { message } = req.body;
        if (!message) {
          return res.status(400).json({ error: "Message is required" });
        }
        const msg = await storage.addIncidentMessage(id, req.user.id, "admin", message);
        res.json(msg);
      } catch (error) {
        console.error("Error adding incident message:", error);
        res.status(500).json({ error: "Failed to add message" });
      }
    });
    admin_default = router;
  }
});

// server/services/jobs-service.ts
function isWithinTimeframe(postedAt, timeframe) {
  const postedDate = new Date(postedAt);
  const now = /* @__PURE__ */ new Date();
  const diffMs = now.getTime() - postedDate.getTime();
  const diffDays = diffMs / (1e3 * 60 * 60 * 24);
  switch (timeframe) {
    case "24h":
      return diffDays <= 1;
    case "7d":
      return diffDays <= 7;
    case "30d":
      return diffDays <= 30;
    case "all":
    default:
      return true;
  }
}
async function searchJobs(filters) {
  let results = [...sampleJobs];
  if (filters.search) {
    const searchLower = filters.search.toLowerCase();
    results = results.filter(
      (job) => job.title.toLowerCase().includes(searchLower) || job.company.toLowerCase().includes(searchLower) || job.skills.some((skill) => skill.toLowerCase().includes(searchLower)) || job.description.toLowerCase().includes(searchLower)
    );
  }
  if (filters.location && filters.location !== "all") {
    const locationLower = filters.location.toLowerCase();
    results = results.filter(
      (job) => job.location.toLowerCase().includes(locationLower)
    );
  }
  if (filters.experienceLevel && filters.experienceLevel !== "") {
    results = results.filter(
      (job) => job.experienceLevel === filters.experienceLevel
    );
  }
  if (filters.type && filters.type !== "") {
    results = results.filter(
      (job) => job.type === filters.type
    );
  }
  if (filters.postedWithin && filters.postedWithin !== "all") {
    results = results.filter(
      (job) => isWithinTimeframe(job.postedAt, filters.postedWithin)
    );
  }
  results.sort(
    (a, b) => new Date(b.postedAt).getTime() - new Date(a.postedAt).getTime()
  );
  return results;
}
async function getJobById(id) {
  return sampleJobs.find((job) => job.id === id);
}
function getJobStats() {
  const totalJobs = sampleJobs.length;
  const remoteJobs = sampleJobs.filter((j) => j.type === "remote" || j.type === "hybrid").length;
  const fresherJobs = sampleJobs.filter((j) => j.experienceLevel === "fresher" || j.experienceLevel === "junior").length;
  return {
    totalJobs,
    remoteJobs,
    fresherJobs,
    companies: new Set(sampleJobs.map((j) => j.company)).size
  };
}
var sampleJobs;
var init_jobs_service = __esm({
  "server/services/jobs-service.ts"() {
    "use strict";
    sampleJobs = [
      {
        id: "job-1",
        title: "Senior DevOps Engineer",
        company: "Amazon Web Services",
        location: "Bangalore",
        type: "full-time",
        experienceLevel: "senior",
        salary: "\u20B935-50 LPA",
        description: "Join AWS to build and maintain cloud infrastructure at scale. Work on cutting-edge container orchestration and CI/CD pipelines.",
        requirements: ["5+ years DevOps experience", "Strong AWS knowledge", "Kubernetes expertise", "Terraform proficiency"],
        skills: ["AWS", "Kubernetes", "Terraform", "Docker", "Python", "CI/CD"],
        postedAt: new Date(Date.now() - 2 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://www.amazon.jobs/en/jobs/2859747/senior-devops-engineer",
        source: "Amazon Jobs",
        logo: "https://logo.clearbit.com/amazon.com"
      },
      {
        id: "job-2",
        title: "Cloud Engineer - Azure",
        company: "Microsoft",
        location: "Hyderabad",
        type: "full-time",
        experienceLevel: "mid",
        salary: "\u20B925-40 LPA",
        description: "Design and implement Azure cloud solutions for enterprise clients. Focus on infrastructure automation and security.",
        requirements: ["3+ years cloud experience", "Azure certifications preferred", "Infrastructure as Code", "Networking knowledge"],
        skills: ["Azure", "ARM Templates", "PowerShell", "Kubernetes", "Networking"],
        postedAt: new Date(Date.now() - 1 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://careers.microsoft.com/us/en/search-results?keywords=cloud%20engineer",
        source: "Microsoft Careers",
        logo: "https://logo.clearbit.com/microsoft.com"
      },
      {
        id: "job-3",
        title: "Kubernetes Administrator",
        company: "Google Cloud",
        location: "Remote",
        type: "remote",
        experienceLevel: "senior",
        salary: "$150,000 - $200,000",
        description: "Manage and optimize Kubernetes clusters for Google Cloud customers. Lead container orchestration best practices.",
        requirements: ["5+ years Kubernetes", "CKA/CKAD certified", "GCP experience", "Strong troubleshooting skills"],
        skills: ["Kubernetes", "GKE", "Helm", "Prometheus", "Grafana", "Go"],
        postedAt: new Date(Date.now() - 5 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://careers.google.com/jobs/results/?q=kubernetes",
        source: "Google Careers",
        logo: "https://logo.clearbit.com/google.com"
      },
      {
        id: "job-4",
        title: "DevOps Engineer - Fresher",
        company: "Infosys",
        location: "Pune",
        type: "full-time",
        experienceLevel: "fresher",
        salary: "\u20B94-6 LPA",
        description: "Entry-level DevOps role with training provided. Work on CI/CD pipelines and cloud infrastructure for global clients.",
        requirements: ["B.Tech/BE in CS/IT", "Basic Linux knowledge", "Scripting basics", "Eager to learn"],
        skills: ["Linux", "Git", "Jenkins", "Docker", "Python", "Bash"],
        postedAt: new Date(Date.now() - 3 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://www.infosys.com/careers.html",
        source: "Infosys Careers",
        logo: "https://logo.clearbit.com/infosys.com"
      },
      {
        id: "job-5",
        title: "SRE Engineer",
        company: "Netflix",
        location: "United States",
        type: "remote",
        experienceLevel: "senior",
        salary: "$180,000 - $250,000",
        description: "Build and maintain highly available systems serving millions of users. Focus on reliability, scalability, and performance.",
        requirements: ["6+ years SRE/DevOps", "Large-scale distributed systems", "Incident management", "Chaos engineering"],
        skills: ["AWS", "Kubernetes", "Spinnaker", "Chaos Engineering", "Java", "Python"],
        postedAt: new Date(Date.now() - 7 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://jobs.netflix.com/search?q=sre",
        source: "Netflix Jobs",
        logo: "https://logo.clearbit.com/netflix.com"
      },
      {
        id: "job-6",
        title: "Junior Cloud Engineer",
        company: "TCS",
        location: "Chennai",
        type: "full-time",
        experienceLevel: "junior",
        salary: "\u20B96-10 LPA",
        description: "Support cloud migration projects for enterprise clients. Learn and implement cloud best practices.",
        requirements: ["1-2 years experience", "AWS/Azure basics", "Linux administration", "Good communication"],
        skills: ["AWS", "Linux", "Docker", "Ansible", "Shell Scripting"],
        postedAt: new Date(Date.now() - 4 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://www.tcs.com/careers",
        source: "TCS Careers",
        logo: "https://logo.clearbit.com/tcs.com"
      },
      {
        id: "job-7",
        title: "Platform Engineer",
        company: "Uber",
        location: "Bangalore",
        type: "hybrid",
        experienceLevel: "senior",
        salary: "\u20B945-65 LPA",
        description: "Build internal developer platforms and tooling. Enable engineering teams with self-service infrastructure.",
        requirements: ["5+ years platform/DevOps", "Kubernetes at scale", "Developer experience focus", "Strong coding skills"],
        skills: ["Kubernetes", "Go", "Terraform", "ArgoCD", "Platform Engineering"],
        postedAt: new Date(Date.now() - 6 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://www.uber.com/us/en/careers/",
        source: "Uber Careers",
        logo: "https://logo.clearbit.com/uber.com"
      },
      {
        id: "job-8",
        title: "AWS Solutions Architect",
        company: "Accenture",
        location: "Mumbai",
        type: "full-time",
        experienceLevel: "lead",
        salary: "\u20B940-60 LPA",
        description: "Design and implement AWS solutions for enterprise clients. Lead technical teams and drive cloud adoption.",
        requirements: ["8+ years experience", "AWS Solutions Architect certification", "Pre-sales experience", "Leadership skills"],
        skills: ["AWS", "Architecture", "Migration", "Consulting", "Leadership"],
        postedAt: new Date(Date.now() - 10 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://www.accenture.com/us-en/careers",
        source: "Accenture Careers",
        logo: "https://logo.clearbit.com/accenture.com"
      },
      {
        id: "job-9",
        title: "DevSecOps Engineer",
        company: "Goldman Sachs",
        location: "Hyderabad",
        type: "full-time",
        experienceLevel: "mid",
        salary: "\u20B930-45 LPA",
        description: "Implement security in CI/CD pipelines. Automate security scanning and compliance checks.",
        requirements: ["3-5 years DevOps", "Security tools experience", "Compliance knowledge", "SAST/DAST tools"],
        skills: ["Security", "CI/CD", "Kubernetes", "Vault", "SonarQube", "OWASP"],
        postedAt: new Date(Date.now() - 2 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://www.goldmansachs.com/careers/",
        source: "Goldman Sachs",
        logo: "https://logo.clearbit.com/goldmansachs.com"
      },
      {
        id: "job-10",
        title: "Docker/Kubernetes Specialist",
        company: "Red Hat",
        location: "Delhi NCR",
        type: "full-time",
        experienceLevel: "senior",
        salary: "\u20B935-50 LPA",
        description: "Help enterprises adopt container technologies. Implement OpenShift solutions and provide technical guidance.",
        requirements: ["5+ years containers", "OpenShift experience", "Customer-facing role", "Training/mentoring"],
        skills: ["OpenShift", "Kubernetes", "Docker", "RHEL", "Ansible", "Consulting"],
        postedAt: new Date(Date.now() - 8 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://www.redhat.com/en/jobs",
        source: "Red Hat Jobs",
        logo: "https://logo.clearbit.com/redhat.com"
      },
      {
        id: "job-11",
        title: "Cloud Infrastructure Engineer",
        company: "Flipkart",
        location: "Bangalore",
        type: "full-time",
        experienceLevel: "mid",
        salary: "\u20B925-40 LPA",
        description: "Build and manage cloud infrastructure for India's largest e-commerce platform. Handle massive scale during sale events.",
        requirements: ["3-5 years experience", "AWS/GCP", "High availability systems", "Automation"],
        skills: ["AWS", "Terraform", "Kubernetes", "Python", "MySQL", "Redis"],
        postedAt: new Date(Date.now() - 1 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://www.flipkartcareers.com/",
        source: "Flipkart Careers",
        logo: "https://logo.clearbit.com/flipkart.com"
      },
      {
        id: "job-12",
        title: "GitOps Engineer",
        company: "GitLab",
        location: "Remote",
        type: "remote",
        experienceLevel: "mid",
        salary: "$120,000 - $160,000",
        description: "Implement GitOps workflows using GitLab CI/CD. Help customers adopt DevOps best practices.",
        requirements: ["3+ years DevOps", "Git expertise", "Kubernetes", "GitOps experience"],
        skills: ["GitLab", "Kubernetes", "ArgoCD", "Flux", "Terraform", "CI/CD"],
        postedAt: new Date(Date.now() - 12 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://about.gitlab.com/jobs/",
        source: "GitLab Jobs",
        logo: "https://logo.clearbit.com/gitlab.com"
      },
      {
        id: "job-13",
        title: "Monitoring & Observability Engineer",
        company: "Datadog",
        location: "Singapore",
        type: "full-time",
        experienceLevel: "senior",
        salary: "SGD 150,000 - 200,000",
        description: "Build monitoring solutions for cloud-native applications. Work on Prometheus, Grafana, and distributed tracing.",
        requirements: ["5+ years monitoring", "Prometheus/Grafana", "APM tools", "Distributed systems"],
        skills: ["Prometheus", "Grafana", "Datadog", "OpenTelemetry", "Go", "Python"],
        postedAt: new Date(Date.now() - 9 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://www.datadoghq.com/careers/",
        source: "Datadog Careers",
        logo: "https://logo.clearbit.com/datadog.com"
      },
      {
        id: "job-14",
        title: "DevOps Trainee",
        company: "Wipro",
        location: "Bangalore",
        type: "full-time",
        experienceLevel: "fresher",
        salary: "\u20B93.5-5 LPA",
        description: "6-month training program covering DevOps fundamentals. Learn CI/CD, containers, and cloud technologies.",
        requirements: ["BE/BTech fresher", "Good academics", "Willingness to learn", "Programming basics"],
        skills: ["Linux", "Git", "Programming", "Networking"],
        postedAt: new Date(Date.now() - 5 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://careers.wipro.com/",
        source: "Wipro Careers",
        logo: "https://logo.clearbit.com/wipro.com"
      },
      {
        id: "job-15",
        title: "Terraform Expert",
        company: "HashiCorp",
        location: "Remote",
        type: "remote",
        experienceLevel: "principal",
        salary: "$200,000 - $280,000",
        description: "Lead Terraform product development and enterprise customer success. Define IaC best practices globally.",
        requirements: ["10+ years infrastructure", "Terraform expert", "Go programming", "Public speaking"],
        skills: ["Terraform", "Go", "AWS", "Azure", "GCP", "Architecture"],
        postedAt: new Date(Date.now() - 15 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://www.hashicorp.com/careers",
        source: "HashiCorp Careers",
        logo: "https://logo.clearbit.com/hashicorp.com"
      },
      {
        id: "job-16",
        title: "Site Reliability Engineer",
        company: "Razorpay",
        location: "Bangalore",
        type: "hybrid",
        experienceLevel: "senior",
        salary: "\u20B935-55 LPA",
        description: "Ensure 99.99% uptime for payment infrastructure. Build resilient systems handling millions of transactions.",
        requirements: ["5+ years SRE/DevOps", "Payment systems", "Incident management", "Performance optimization"],
        skills: ["Kubernetes", "AWS", "Python", "PostgreSQL", "Kafka", "Redis"],
        postedAt: new Date(Date.now() - 3 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://razorpay.com/careers/",
        source: "Razorpay Careers",
        logo: "https://logo.clearbit.com/razorpay.com"
      },
      {
        id: "job-17",
        title: "Jenkins Administrator",
        company: "Cognizant",
        location: "Pune",
        type: "full-time",
        experienceLevel: "junior",
        salary: "\u20B98-12 LPA",
        description: "Manage Jenkins infrastructure for multiple projects. Create and maintain CI/CD pipelines.",
        requirements: ["2-3 years experience", "Jenkins administration", "Pipeline as Code", "Git"],
        skills: ["Jenkins", "Groovy", "Git", "Docker", "Maven", "Shell"],
        postedAt: new Date(Date.now() - 6 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://careers.cognizant.com/",
        source: "Cognizant Careers",
        logo: "https://logo.clearbit.com/cognizant.com"
      },
      {
        id: "job-18",
        title: "Cloud Security Engineer",
        company: "Palo Alto Networks",
        location: "Bangalore",
        type: "full-time",
        experienceLevel: "senior",
        salary: "\u20B940-60 LPA",
        description: "Implement cloud security solutions. Work on CSPM, container security, and compliance automation.",
        requirements: ["5+ years security", "Cloud security certifications", "Compliance frameworks", "Security tools"],
        skills: ["Cloud Security", "Prisma Cloud", "AWS", "Kubernetes", "Compliance", "SIEM"],
        postedAt: new Date(Date.now() - 4 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://www.paloaltonetworks.com/company/careers",
        source: "Palo Alto Networks",
        logo: "https://logo.clearbit.com/paloaltonetworks.com"
      },
      {
        id: "job-19",
        title: "Ansible Automation Engineer",
        company: "IBM",
        location: "Hyderabad",
        type: "full-time",
        experienceLevel: "mid",
        salary: "\u20B920-35 LPA",
        description: "Automate infrastructure provisioning and application deployment using Ansible. Support enterprise clients.",
        requirements: ["3-5 years automation", "Ansible certified", "Linux administration", "Python scripting"],
        skills: ["Ansible", "Ansible Tower", "Python", "Linux", "VMware", "RHEL"],
        postedAt: new Date(Date.now() - 11 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://www.ibm.com/employment/",
        source: "IBM Careers",
        logo: "https://logo.clearbit.com/ibm.com"
      },
      {
        id: "job-20",
        title: "AWS DevOps Engineer",
        company: "Zomato",
        location: "Delhi NCR",
        type: "hybrid",
        experienceLevel: "mid",
        salary: "\u20B925-40 LPA",
        description: "Build infrastructure for food delivery platform serving millions. Focus on cost optimization and scalability.",
        requirements: ["3-5 years DevOps", "AWS expertise", "Cost optimization", "Microservices"],
        skills: ["AWS", "Kubernetes", "Terraform", "Python", "MySQL", "MongoDB"],
        postedAt: new Date(Date.now() - 2 * 24 * 60 * 60 * 1e3).toISOString(),
        applyUrl: "https://www.zomato.com/careers",
        source: "Zomato Careers",
        logo: "https://logo.clearbit.com/zomato.com"
      }
    ];
  }
});

// shared/interview-questions.ts
var interviewCategories;
var init_interview_questions = __esm({
  "shared/interview-questions.ts"() {
    "use strict";
    interviewCategories = [
      {
        id: "devops",
        name: "DevOps",
        icon: "Settings",
        questions: [
          { id: "devops-1", question: "What is DevOps and why is it important?", answer: "DevOps is a set of practices that combines software development (Dev) and IT operations (Ops) to shorten the development lifecycle and deliver high-quality software continuously. It emphasizes collaboration, automation, continuous integration/deployment, and monitoring.", difficulty: "beginner" },
          { id: "devops-2", question: "Explain the difference between CI and CD.", answer: "CI (Continuous Integration) is the practice of frequently merging code changes into a shared repository with automated builds and tests. CD can mean Continuous Delivery (automatically preparing code for release) or Continuous Deployment (automatically releasing to production after passing tests).", difficulty: "beginner" },
          { id: "devops-3", question: "What is Infrastructure as Code (IaC)?", answer: "IaC is the practice of managing and provisioning infrastructure through machine-readable configuration files rather than manual processes. Tools like Terraform, CloudFormation, and Ansible enable version control, repeatability, and automation of infrastructure.", difficulty: "intermediate" },
          { id: "devops-4", question: "Explain Blue-Green deployment strategy.", answer: 'Blue-Green deployment maintains two identical production environments. The "blue" environment runs the current version while "green" runs the new version. Traffic is switched from blue to green once testing passes, allowing instant rollback if issues occur.', difficulty: "intermediate" },
          { id: "devops-5", question: "What are the key metrics for measuring DevOps success?", answer: "Key metrics include: Deployment Frequency, Lead Time for Changes, Mean Time to Recovery (MTTR), Change Failure Rate, and Mean Time Between Failures (MTBF). These DORA metrics help evaluate team performance and process efficiency.", difficulty: "advanced" }
        ]
      },
      {
        id: "kubernetes",
        name: "Kubernetes",
        icon: "Box",
        questions: [
          { id: "k8s-1", question: "What is Kubernetes and what problems does it solve?", answer: "Kubernetes is an open-source container orchestration platform that automates deployment, scaling, and management of containerized applications. It solves problems like service discovery, load balancing, storage orchestration, self-healing, and horizontal scaling.", difficulty: "beginner" },
          { id: "k8s-2", question: "Explain the difference between a Pod and a Deployment.", answer: "A Pod is the smallest deployable unit containing one or more containers. A Deployment is a higher-level abstraction that manages Pods, providing declarative updates, rolling deployments, rollback capabilities, and replica management.", difficulty: "beginner" },
          { id: "k8s-3", question: "What is a Kubernetes Service and its types?", answer: "A Service is an abstraction that defines a logical set of Pods and a policy to access them. Types include: ClusterIP (internal only), NodePort (external via node ports), LoadBalancer (external via cloud LB), and ExternalName (DNS CNAME).", difficulty: "intermediate" },
          { id: "k8s-4", question: "Explain Kubernetes namespaces and their use cases.", answer: "Namespaces provide a mechanism for isolating groups of resources within a single cluster. Use cases include: separating environments (dev/staging/prod), team isolation, resource quota management, and access control through RBAC.", difficulty: "intermediate" },
          { id: "k8s-5", question: "How does Horizontal Pod Autoscaler (HPA) work?", answer: "HPA automatically scales the number of Pod replicas based on observed CPU utilization or custom metrics. It queries the metrics API, calculates desired replicas using the formula: desiredReplicas = ceil(currentReplicas * (currentMetric/targetMetric)), and adjusts accordingly.", difficulty: "advanced" },
          { id: "k8s-6", question: "Explain Kubernetes Ingress and its components.", answer: "Ingress manages external access to services, typically HTTP/HTTPS. Components include: Ingress resource (routing rules), Ingress controller (implements rules, e.g., Nginx, Traefik), and annotations for controller-specific configurations like SSL, rewrites, and rate limiting.", difficulty: "advanced" }
        ]
      },
      {
        id: "docker",
        name: "Docker",
        icon: "Container",
        questions: [
          { id: "docker-1", question: "What is Docker and how does it differ from VMs?", answer: "Docker is a platform for developing, shipping, and running applications in containers. Unlike VMs that virtualize entire operating systems, containers share the host OS kernel, making them lightweight, fast to start, and resource-efficient.", difficulty: "beginner" },
          { id: "docker-2", question: "Explain the difference between Docker image and container.", answer: "A Docker image is a read-only template containing application code, libraries, and dependencies. A container is a running instance of an image - it adds a writable layer on top of the image and includes its own isolated filesystem, network, and process space.", difficulty: "beginner" },
          { id: "docker-3", question: "What is a Dockerfile and its key instructions?", answer: "A Dockerfile is a text file with instructions to build a Docker image. Key instructions: FROM (base image), RUN (execute commands), COPY/ADD (add files), WORKDIR (set directory), ENV (environment variables), EXPOSE (ports), CMD/ENTRYPOINT (startup command).", difficulty: "intermediate" },
          { id: "docker-4", question: "How do you optimize Docker image size?", answer: "Optimization techniques: Use multi-stage builds, choose minimal base images (alpine), combine RUN commands to reduce layers, use .dockerignore, remove unnecessary files and caches, order instructions by change frequency, and avoid installing dev dependencies.", difficulty: "intermediate" },
          { id: "docker-5", question: "Explain Docker networking modes.", answer: "Docker networking modes: Bridge (default, isolated network), Host (shares host network stack), None (no networking), Overlay (multi-host networking for Swarm), Macvlan (assigns MAC address, appears as physical device). Custom bridge networks enable DNS-based container discovery.", difficulty: "advanced" }
        ]
      },
      {
        id: "sonarqube",
        name: "SonarQube",
        icon: "Shield",
        questions: [
          { id: "sonar-1", question: "What is SonarQube and what does it analyze?", answer: "SonarQube is an open-source platform for continuous inspection of code quality. It analyzes source code for bugs, code smells, security vulnerabilities, and technical debt across 25+ programming languages through static code analysis.", difficulty: "beginner" },
          { id: "sonar-2", question: "Explain Quality Gates in SonarQube.", answer: "Quality Gates are a set of threshold conditions that code must meet before it can be released. Common conditions include: code coverage percentage, number of bugs, vulnerabilities, code smells, and duplications. Projects fail the gate if any condition is not met.", difficulty: "intermediate" },
          { id: "sonar-3", question: "What is the difference between bugs, vulnerabilities, and code smells?", answer: "Bugs are coding errors that will cause incorrect behavior. Vulnerabilities are security flaws that attackers could exploit. Code smells are maintainability issues that make code confusing and harder to maintain but don't necessarily cause bugs.", difficulty: "intermediate" },
          { id: "sonar-4", question: "How do you integrate SonarQube with CI/CD pipelines?", answer: "Integration involves: installing SonarScanner, configuring sonar-project.properties, adding analysis step in CI (Jenkins, GitHub Actions, GitLab CI), setting up webhooks for Quality Gate status, and optionally blocking deployments on gate failure.", difficulty: "advanced" }
        ]
      },
      {
        id: "aws",
        name: "AWS",
        icon: "Cloud",
        questions: [
          { id: "aws-1", question: "Explain the difference between EC2 and Lambda.", answer: "EC2 provides virtual servers you manage (OS, scaling, patching). Lambda is serverless - you upload code and AWS handles infrastructure, scaling automatically per request. EC2 suits long-running applications; Lambda suits event-driven, short-duration functions.", difficulty: "beginner" },
          { id: "aws-2", question: "What is VPC and its components?", answer: "VPC (Virtual Private Cloud) is an isolated network in AWS. Components include: Subnets (public/private), Route Tables, Internet Gateway (public access), NAT Gateway (private subnet internet), Security Groups (instance firewall), NACLs (subnet firewall).", difficulty: "intermediate" },
          { id: "aws-3", question: "Explain S3 storage classes and use cases.", answer: "S3 Standard (frequent access), S3 Intelligent-Tiering (varying access), S3 Standard-IA (infrequent access), S3 One Zone-IA (single AZ), S3 Glacier (archival, minutes to hours retrieval), S3 Glacier Deep Archive (lowest cost, 12+ hours retrieval).", difficulty: "intermediate" },
          { id: "aws-4", question: "What is AWS IAM and best practices?", answer: "IAM manages access to AWS resources. Best practices: Use least privilege, enable MFA, use roles instead of long-term credentials, rotate credentials regularly, use IAM policies for fine-grained permissions, and never use root account for daily tasks.", difficulty: "intermediate" },
          { id: "aws-5", question: "Explain Auto Scaling in AWS.", answer: "Auto Scaling automatically adjusts capacity based on demand. Components: Launch Template (instance config), Auto Scaling Group (min/max/desired capacity), Scaling Policies (target tracking, step, simple). Integrates with CloudWatch for metrics-based scaling.", difficulty: "advanced" }
        ]
      },
      {
        id: "azure",
        name: "Azure",
        icon: "Cloud",
        questions: [
          { id: "azure-1", question: "What are Azure Resource Groups?", answer: "Resource Groups are logical containers for Azure resources that share the same lifecycle, permissions, and policies. They help organize resources by project, environment, or billing. Resources can only belong to one group but can interact across groups.", difficulty: "beginner" },
          { id: "azure-2", question: "Explain Azure App Service and its tiers.", answer: "App Service is a PaaS for hosting web applications, REST APIs, and mobile backends. Tiers: Free/Shared (dev/test), Basic (dedicated compute), Standard (auto-scale, slots), Premium (enhanced performance), Isolated (network isolation, highest scale).", difficulty: "intermediate" },
          { id: "azure-3", question: "What is Azure DevOps and its components?", answer: "Azure DevOps is a suite of development tools: Azure Boards (work tracking), Azure Repos (Git repositories), Azure Pipelines (CI/CD), Azure Test Plans (testing), Azure Artifacts (package management). Integrates with most development tools and cloud platforms.", difficulty: "intermediate" },
          { id: "azure-4", question: "Explain Azure Kubernetes Service (AKS).", answer: "AKS is a managed Kubernetes service that simplifies deployment and management. Azure handles control plane, patching, and scaling. Features include: integrated CI/CD, Azure AD integration, virtual nodes for burst scaling, and Azure Monitor integration.", difficulty: "advanced" }
        ]
      },
      {
        id: "prometheus",
        name: "Prometheus",
        icon: "Activity",
        questions: [
          { id: "prom-1", question: "What is Prometheus and how does it work?", answer: "Prometheus is an open-source monitoring and alerting toolkit. It uses a pull model to scrape metrics from configured targets at intervals, stores data in a time-series database, and provides PromQL for querying. It supports service discovery and alerting.", difficulty: "beginner" },
          { id: "prom-2", question: "Explain the four types of Prometheus metrics.", answer: "Counter: cumulative value that only increases (requests, errors). Gauge: value that can go up/down (temperature, memory). Histogram: samples observations into configurable buckets. Summary: calculates quantiles over a sliding time window.", difficulty: "intermediate" },
          { id: "prom-3", question: "What is PromQL and common functions?", answer: "PromQL is Prometheus Query Language for selecting and aggregating metrics. Common functions: rate() (per-second rate), sum() (aggregation), avg() (average), histogram_quantile() (percentiles), increase() (total increase over time range).", difficulty: "intermediate" },
          { id: "prom-4", question: "How do you set up alerting in Prometheus?", answer: "Define alerting rules in prometheus.yml or separate rules file. Rules specify: alert name, PromQL expression, duration (how long condition must be true), labels, and annotations. Alertmanager handles routing, grouping, silencing, and sending notifications.", difficulty: "advanced" }
        ]
      },
      {
        id: "grafana",
        name: "Grafana",
        icon: "BarChart",
        questions: [
          { id: "grafana-1", question: "What is Grafana and its primary use?", answer: "Grafana is an open-source analytics and visualization platform. It creates dashboards from various data sources (Prometheus, InfluxDB, Elasticsearch, SQL databases) for monitoring infrastructure, applications, and business metrics with rich visualizations.", difficulty: "beginner" },
          { id: "grafana-2", question: "Explain Grafana data sources and panels.", answer: "Data sources are the databases Grafana queries (Prometheus, MySQL, CloudWatch, etc.). Panels are individual visualizations (graphs, gauges, tables, heatmaps) that display queried data. Each panel connects to a data source and uses its query language.", difficulty: "intermediate" },
          { id: "grafana-3", question: "How do you create effective Grafana dashboards?", answer: "Best practices: Use consistent naming conventions, organize panels logically (overview at top), use variables for filtering, set appropriate time ranges, add annotations for events, use proper units and thresholds, create rows for grouping related panels.", difficulty: "intermediate" },
          { id: "grafana-4", question: "Explain Grafana alerting and notification channels.", answer: "Grafana can evaluate dashboard panel queries and trigger alerts based on thresholds. Notification channels include: email, Slack, PagerDuty, webhooks, and more. Alert rules specify conditions, evaluation frequency, and notification routing.", difficulty: "advanced" }
        ]
      },
      {
        id: "testing",
        name: "Testing",
        icon: "CheckCircle",
        questions: [
          { id: "test-1", question: "Explain the testing pyramid.", answer: "The testing pyramid is a strategy with three layers: Unit tests (base, most numerous, fast, isolated), Integration tests (middle, test component interactions), E2E/UI tests (top, fewest, slowest, test complete flows). More tests at the bottom, fewer at the top.", difficulty: "beginner" },
          { id: "test-2", question: "What is the difference between unit, integration, and E2E tests?", answer: "Unit tests verify individual functions/methods in isolation using mocks. Integration tests verify how components work together (API + database). E2E tests simulate real user scenarios through the entire application stack including UI.", difficulty: "beginner" },
          { id: "test-3", question: "Explain TDD (Test-Driven Development).", answer: "TDD is a development approach: 1) Write a failing test first, 2) Write minimal code to pass the test, 3) Refactor while keeping tests passing. Benefits include better design, built-in documentation, regression protection, and confidence in changes.", difficulty: "intermediate" },
          { id: "test-4", question: "What is code coverage and its limitations?", answer: "Code coverage measures the percentage of code executed by tests (line, branch, function coverage). Limitations: High coverage doesn't guarantee quality tests, doesn't verify correct behavior, can encourage testing trivial code, and may miss edge cases.", difficulty: "intermediate" }
        ]
      },
      {
        id: "troubleshooting",
        name: "Troubleshooting",
        icon: "Search",
        questions: [
          { id: "trouble-1", question: "How do you troubleshoot a slow application?", answer: "Approach: 1) Identify where slowness occurs (frontend/backend/database), 2) Check metrics (CPU, memory, I/O, network), 3) Review logs for errors/warnings, 4) Profile code to find bottlenecks, 5) Check database query performance, 6) Review recent changes.", difficulty: "intermediate" },
          { id: "trouble-2", question: "How do you debug a container that keeps crashing?", answer: "Steps: 1) Check logs: docker logs <container>, 2) Inspect container: docker inspect, 3) Check exit code, 4) Run interactively: docker run -it --entrypoint /bin/sh, 5) Verify resource limits, 6) Check health checks, 7) Review Dockerfile and entrypoint.", difficulty: "intermediate" },
          { id: "trouble-3", question: "How do you troubleshoot Kubernetes pod issues?", answer: "Commands: kubectl describe pod (events, status), kubectl logs (application logs), kubectl exec (debug inside pod), kubectl get events. Check: ImagePullBackOff (registry issues), CrashLoopBackOff (app crashes), Pending (resource/scheduling issues).", difficulty: "advanced" },
          { id: "trouble-4", question: "How do you investigate a memory leak?", answer: "Steps: 1) Monitor memory growth over time, 2) Take heap dumps at intervals, 3) Compare dumps to identify growing objects, 4) Use profilers (pprof, async-profiler, Node --inspect), 5) Check for common causes: unclosed connections, growing caches, event listeners.", difficulty: "advanced" }
        ]
      },
      {
        id: "shell",
        name: "Shell Scripting",
        icon: "Terminal",
        questions: [
          { id: "shell-1", question: "What is the difference between sh, bash, and zsh?", answer: "sh is the original Bourne shell (POSIX compliant). Bash (Bourne Again Shell) extends sh with arrays, improved scripting, history. Zsh adds more features: better completion, themes (oh-my-zsh), spelling correction, and shared history across sessions.", difficulty: "beginner" },
          { id: "shell-2", question: "Explain common shell variables: $?, $#, $@, $*.", answer: "$? is the exit status of the last command. $# is the number of arguments. $@ is all arguments as separate words. $* is all arguments as a single word. $0 is the script name. $1-$9 are positional parameters.", difficulty: "intermediate" },
          { id: "shell-3", question: "How do you handle errors in shell scripts?", answer: 'Use: set -e (exit on error), set -u (error on undefined variables), set -o pipefail (pipeline fails if any command fails). Combine as: set -euo pipefail. Use trap for cleanup: trap "cleanup_function" EXIT. Check exit codes with if/then or ||.', difficulty: "intermediate" },
          { id: "shell-4", question: "Explain the difference between > and >> redirection.", answer: "> redirects output to a file, overwriting existing content. >> appends to the file without overwriting. 2> redirects stderr. &> or >& redirects both stdout and stderr. << is here-document for inline input. <<< is here-string.", difficulty: "beginner" },
          { id: "shell-5", question: "How do you process files line by line in bash?", answer: 'Use while read loop: while IFS= read -r line; do echo "$line"; done < file.txt. IFS= prevents leading/trailing whitespace trimming. -r prevents backslash interpretation. Can also use for loop with cat, but while read is more efficient for large files.', difficulty: "intermediate" }
        ]
      }
    ];
  }
});

// server/middleware/admin-guard.ts
function requireAdmin(req, res, next) {
  if (!req.isAuthenticated()) {
    logger.warn("Unauthenticated admin access attempt", {
      ip: req.ip,
      path: req.path,
      userAgent: req.get("User-Agent")
    });
    return res.status(401).json({
      error: "Authentication required",
      message: "Please log in to access this resource"
    });
  }
  const userEmail = req.user.email;
  if (!userEmail || !ADMIN_EMAILS.includes(userEmail)) {
    logger.warn("Unauthorized admin access attempt", {
      userId: req.user.id,
      email: userEmail,
      ip: req.ip,
      path: req.path,
      userAgent: req.get("User-Agent")
    });
    return res.status(403).json({
      error: "Access denied",
      message: "Administrative privileges required"
    });
  }
  logger.info("Admin access granted", {
    userId: req.user.id,
    email: userEmail,
    ip: req.ip,
    path: req.path
  });
  next();
}
var ADMIN_EMAILS;
var init_admin_guard = __esm({
  "server/middleware/admin-guard.ts"() {
    "use strict";
    init_logger();
    ADMIN_EMAILS = [
      "agrawalmayank200228@gmail.com"
    ];
  }
});

// server/middleware/prometheus.ts
import * as client2 from "prom-client";
function metricsMiddleware(req, res, next) {
  const startTime = Date.now();
  res.on("finish", () => {
    const duration = (Date.now() - startTime) / 1e3;
    const route = req.route?.path || req.path;
    const method = req.method;
    const statusCode = res.statusCode.toString();
    const userId = req.user?.id?.toString() || "anonymous";
    httpRequestDuration2.labels(method, route, statusCode, userId).observe(duration);
    httpRequestsTotal.labels(method, route, statusCode, userId).inc();
    if (res.statusCode >= 400) {
      errorRate2.labels("http_error", route, userId).inc();
    }
  });
  next();
}
function trackError(type, route, userId = "anonymous") {
  errorRate2.labels(type, route, userId).inc();
}
var register2, httpRequestDuration2, httpRequestsTotal, activeUsers2, aiQueriesTotal, creditsUsed, errorRate2, databaseQueries, redisOperations;
var init_prometheus = __esm({
  "server/middleware/prometheus.ts"() {
    "use strict";
    register2 = new client2.Registry();
    client2.collectDefaultMetrics({ register: register2 });
    httpRequestDuration2 = new client2.Histogram({
      name: "http_request_duration_seconds",
      help: "Duration of HTTP requests in seconds",
      labelNames: ["method", "route", "status_code", "user_id"],
      buckets: [0.1, 0.5, 1, 2, 5, 10]
    });
    httpRequestsTotal = new client2.Counter({
      name: "http_requests_total",
      help: "Total number of HTTP requests",
      labelNames: ["method", "route", "status_code", "user_id"]
    });
    activeUsers2 = new client2.Gauge({
      name: "active_users_total",
      help: "Number of currently active users"
    });
    aiQueriesTotal = new client2.Counter({
      name: "ai_queries_total",
      help: "Total number of AI queries made",
      labelNames: ["user_id", "success"]
    });
    creditsUsed = new client2.Counter({
      name: "credits_used_total",
      help: "Total number of credits used",
      labelNames: ["user_id", "feature"]
    });
    errorRate2 = new client2.Counter({
      name: "errors_total",
      help: "Total number of errors",
      labelNames: ["type", "route", "user_id"]
    });
    databaseQueries = new client2.Histogram({
      name: "database_query_duration_seconds",
      help: "Duration of database queries in seconds",
      labelNames: ["operation", "table"],
      buckets: [0.01, 0.05, 0.1, 0.5, 1, 2]
    });
    redisOperations = new client2.Histogram({
      name: "redis_operation_duration_seconds",
      help: "Duration of Redis operations in seconds",
      labelNames: ["operation", "success"],
      buckets: [1e-3, 5e-3, 0.01, 0.05, 0.1, 0.5]
    });
    register2.registerMetric(httpRequestDuration2);
    register2.registerMetric(httpRequestsTotal);
    register2.registerMetric(activeUsers2);
    register2.registerMetric(aiQueriesTotal);
    register2.registerMetric(creditsUsed);
    register2.registerMetric(errorRate2);
    register2.registerMetric(databaseQueries);
    register2.registerMetric(redisOperations);
  }
});

// server/middleware/security.ts
import rateLimit from "express-rate-limit";
function securityHeaders(req, res, next) {
  res.setHeader("X-Frame-Options", "DENY");
  res.setHeader("X-Content-Type-Options", "nosniff");
  res.setHeader("X-XSS-Protection", "1; mode=block");
  res.setHeader("Referrer-Policy", "strict-origin-when-cross-origin");
  res.setHeader(
    "Content-Security-Policy",
    "default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self' data:; connect-src 'self' https://generativelanguage.googleapis.com https://api.stripe.com; frame-ancestors 'none';"
  );
  if (req.secure || req.headers["x-forwarded-proto"] === "https") {
    res.setHeader("Strict-Transport-Security", "max-age=31536000; includeSubDomains; preload");
  }
  next();
}
function validateInput(req, res, next) {
  const sqlInjectionPattern = /(\b(SELECT|INSERT|UPDATE|DELETE|DROP|CREATE|ALTER|EXEC|UNION|SCRIPT)\b|[';]|--|\|)/gi;
  const xssPattern = /<script\b[^<]*(?:(?!<\/script>)<[^<]*)*<\/script>/gi;
  const checkValue = (value) => {
    if (typeof value === "string") {
      return sqlInjectionPattern.test(value) || xssPattern.test(value);
    }
    if (typeof value === "object" && value !== null) {
      return Object.values(value).some(checkValue);
    }
    return false;
  };
  if (req.body && checkValue(req.body)) {
    logger.warn("Malicious input detected in request body", {
      ip: req.ip,
      path: req.path,
      userAgent: req.get("User-Agent"),
      userId: req.user?.id,
      body: req.body
    });
    return res.status(400).json({
      error: "Invalid input detected. Please check your data and try again."
    });
  }
  if (req.query && checkValue(req.query)) {
    logger.warn("Malicious input detected in query parameters", {
      ip: req.ip,
      path: req.path,
      userAgent: req.get("User-Agent"),
      userId: req.user?.id,
      query: req.query
    });
    return res.status(400).json({
      error: "Invalid query parameters detected. Please check your request and try again."
    });
  }
  next();
}
function securityLogger(req, res, next) {
  const startTime = Date.now();
  const suspiciousPatterns = [
    /\.\./g,
    // Path traversal
    /%2e%2e/gi,
    // Encoded path traversal
    /\/proc\//gi,
    // System file access
    /\/etc\//gi,
    // System config access
    /\x00/g
    // Null bytes
  ];
  const isSuspicious = suspiciousPatterns.some(
    (pattern) => pattern.test(req.url) || pattern.test(JSON.stringify(req.body)) || pattern.test(JSON.stringify(req.query))
  );
  if (isSuspicious) {
    logger.warn("Suspicious request detected", {
      ip: req.ip,
      method: req.method,
      url: req.url,
      userAgent: req.get("User-Agent"),
      referer: req.get("Referer"),
      body: req.body,
      query: req.query,
      userId: req.user?.id
    });
  }
  res.on("finish", () => {
    const duration = Date.now() - startTime;
    if (req.path.includes("/auth") && res.statusCode >= 400) {
      logger.warn("Failed authentication attempt", {
        ip: req.ip,
        method: req.method,
        path: req.path,
        statusCode: res.statusCode,
        duration,
        userAgent: req.get("User-Agent"),
        body: req.body
      });
    }
    if (res.statusCode === 401 || res.statusCode === 403) {
      logger.warn("Unauthorized access attempt", {
        ip: req.ip,
        method: req.method,
        path: req.path,
        statusCode: res.statusCode,
        duration,
        userAgent: req.get("User-Agent"),
        userId: req.user?.id
      });
    }
  });
  next();
}
function corsMiddleware(req, res, next) {
  const allowedOrigins = [
    "http://localhost:5000",
    "https://your-domain.com",
    // Replace with your production domain
    process.env.FRONTEND_URL
  ].filter(Boolean);
  const origin = req.headers.origin;
  if (origin && allowedOrigins.includes(origin)) {
    res.setHeader("Access-Control-Allow-Origin", origin);
  }
  res.setHeader("Access-Control-Allow-Methods", "GET, POST, PUT, DELETE, OPTIONS");
  res.setHeader("Access-Control-Allow-Headers", "Content-Type, Authorization");
  res.setHeader("Access-Control-Allow-Credentials", "true");
  res.setHeader("Access-Control-Max-Age", "86400");
  if (req.method === "OPTIONS") {
    return res.status(200).end();
  }
  next();
}
function sessionSecurity(req, res, next) {
  if (req.session && req.user) {
    const currentIP = req.ip;
    const sessionIP = req.session.ip;
    if (!sessionIP) {
      req.session.ip = currentIP;
    } else if (sessionIP !== currentIP) {
      logger.warn("Potential session hijacking detected", {
        sessionIP,
        currentIP,
        userId: req.user?.id,
        userAgent: req.get("User-Agent")
      });
      req.session.destroy((err) => {
        if (err) {
          logger.error("Error destroying suspicious session", { error: err });
        }
      });
      return res.status(401).json({
        error: "Session security violation detected. Please log in again.",
        code: "SESSION_HIJACK_DETECTED"
      });
    }
  }
  next();
}
var globalRateLimit, authRateLimit, apiRateLimit;
var init_security = __esm({
  "server/middleware/security.ts"() {
    "use strict";
    init_logger();
    globalRateLimit = rateLimit({
      windowMs: 15 * 60 * 1e3,
      // 15 minutes
      max: 500,
      // Increased limit for development assets
      message: {
        error: "Too many requests from this IP, please try again later.",
        retryAfter: "15 minutes"
      },
      standardHeaders: true,
      legacyHeaders: false,
      skip: (req) => {
        const path2 = req.path;
        return path2.startsWith("/src/") || path2.startsWith("/@") || path2.startsWith("/node_modules/") || path2.endsWith(".tsx") || path2.endsWith(".ts") || path2.endsWith(".css") || path2.endsWith(".js") || path2.endsWith(".mjs") || path2.includes("/@vite/") || path2.includes("/@react-refresh") || path2.includes("/@fs/");
      },
      handler: (req, res) => {
        logger.warn(`Rate limit exceeded for IP: ${req.ip}`, {
          ip: req.ip,
          path: req.path,
          userAgent: req.get("User-Agent")
        });
        res.status(429).json({
          error: "Too many requests from this IP, please try again later.",
          retryAfter: "15 minutes"
        });
      }
    });
    authRateLimit = rateLimit({
      windowMs: 15 * 60 * 1e3,
      // 15 minutes
      max: 5,
      // Limit each IP to 5 login attempts per windowMs
      message: {
        error: "Too many authentication attempts, please try again later.",
        retryAfter: "15 minutes"
      },
      skipSuccessfulRequests: true,
      handler: (req, res) => {
        logger.warn(`Auth rate limit exceeded for IP: ${req.ip}`, {
          ip: req.ip,
          path: req.path,
          userAgent: req.get("User-Agent")
        });
        res.status(429).json({
          error: "Too many authentication attempts, please try again later.",
          retryAfter: "15 minutes"
        });
      }
    });
    apiRateLimit = rateLimit({
      windowMs: 60 * 1e3,
      // 1 minute
      max: 20,
      // Limit each IP to 20 API requests per minute
      message: {
        error: "API rate limit exceeded, please try again later.",
        retryAfter: "1 minute"
      },
      handler: (req, res) => {
        logger.warn(`API rate limit exceeded for IP: ${req.ip}`, {
          ip: req.ip,
          path: req.path,
          userAgent: req.get("User-Agent"),
          userId: req.user?.id
        });
        res.status(429).json({
          error: "API rate limit exceeded, please try again later.",
          retryAfter: "1 minute"
        });
      }
    });
  }
});

// server/middleware/auth-guard.ts
function requireAuth(req, res, next) {
  const startTime = Date.now();
  try {
    if (!req.isAuthenticated()) {
      logger.warn("Unauthenticated access attempt", {
        ip: req.ip,
        method: req.method,
        path: req.path,
        userAgent: req.get("User-Agent"),
        referer: req.get("Referer"),
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      });
      return res.status(401).json({
        error: "Authentication required",
        message: "You must be logged in to access this resource",
        code: "AUTH_REQUIRED"
      });
    }
    if (!req.user) {
      logger.warn("Missing user object in authenticated request", {
        ip: req.ip,
        method: req.method,
        path: req.path,
        sessionID: req.sessionID,
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      });
      return res.status(401).json({
        error: "Invalid session",
        message: "Your session is invalid. Please log in again.",
        code: "INVALID_SESSION"
      });
    }
    logger.info("Authenticated request", {
      userId: req.user.id,
      username: req.user.username,
      method: req.method,
      path: req.path,
      ip: req.ip,
      duration: Date.now() - startTime,
      timestamp: (/* @__PURE__ */ new Date()).toISOString()
    });
    next();
  } catch (error) {
    logger.error("Authentication middleware error", {
      error: error instanceof Error ? error.message : String(error),
      stack: error instanceof Error ? error.stack : void 0,
      ip: req.ip,
      method: req.method,
      path: req.path,
      timestamp: (/* @__PURE__ */ new Date()).toISOString()
    });
    return res.status(500).json({
      error: "Authentication system error",
      message: "Please try again later",
      code: "AUTH_SYSTEM_ERROR"
    });
  }
}
function requireCredits(minimumCredits = 1) {
  return (req, res, next) => {
    if (!req.user) {
      return res.status(401).json({
        error: "Authentication required",
        code: "AUTH_REQUIRED"
      });
    }
    const userCredits = req.user.credits || 0;
    if (userCredits < minimumCredits) {
      logger.info("Insufficient credits for operation", {
        userId: req.user.id,
        username: req.user.username,
        userCredits,
        requiredCredits: minimumCredits,
        method: req.method,
        path: req.path,
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      });
      return res.status(402).json({
        error: "Insufficient credits",
        message: `This operation requires ${minimumCredits} credits. You have ${userCredits} credits.`,
        userCredits,
        requiredCredits: minimumCredits,
        code: "INSUFFICIENT_CREDITS"
      });
    }
    next();
  };
}
function validateSessionTimeout(timeoutMinutes = 60) {
  return (req, res, next) => {
    if (!req.user || !req.session) {
      return next();
    }
    const now = Date.now();
    const sessionStart = req.session.loginTime || now;
    const timeoutMs = timeoutMinutes * 60 * 1e3;
    if (now - sessionStart > timeoutMs) {
      logger.info("Session timeout detected", {
        userId: req.user.id,
        sessionStart: new Date(sessionStart).toISOString(),
        timeoutMinutes,
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      });
      req.session.destroy((err) => {
        if (err) {
          logger.error("Error destroying timed out session", { error: err });
        }
      });
      return res.status(401).json({
        error: "Session expired",
        message: "Your session has expired. Please log in again.",
        code: "SESSION_EXPIRED"
      });
    }
    req.session.lastActivity = now;
    next();
  };
}
var init_auth_guard = __esm({
  "server/middleware/auth-guard.ts"() {
    "use strict";
    init_logger();
  }
});

// server/services/sonar-service.ts
var sonar_service_exports = {};
__export(sonar_service_exports, {
  sonarService: () => sonarService
});
import { exec } from "child_process";
import { promisify as promisify2 } from "util";
import fs from "fs/promises";
import path from "path";
var execAsync, SonarService, sonarService;
var init_sonar_service = __esm({
  "server/services/sonar-service.ts"() {
    "use strict";
    init_logger();
    execAsync = promisify2(exec);
    SonarService = class {
      sonarUrl;
      sonarToken;
      projectKey;
      constructor() {
        this.sonarUrl = process.env.SONAR_HOST_URL || "http://localhost:9000";
        this.sonarToken = process.env.SONAR_TOKEN || "";
        this.projectKey = "prometix-devops-platform";
      }
      async runAnalysis() {
        try {
          logger.info("Starting SonarQube analysis");
          try {
            await execAsync("sonar-scanner -v");
          } catch {
            logger.info("Using SonarJS for local analysis");
            return await this.runLocalAnalysis();
          }
          const scannerCmd = `sonar-scanner         -Dsonar.projectKey=${this.projectKey}         -Dsonar.sources=.         -Dsonar.host.url=${this.sonarUrl}         -Dsonar.login=${this.sonarToken}`;
          const { stdout, stderr } = await execAsync(scannerCmd, {
            cwd: process.cwd(),
            timeout: 3e5
            // 5 minutes timeout
          });
          logger.info("SonarQube analysis completed", { stdout, stderr });
          const taskIdMatch = stdout.match(/ANALYSIS SUCCESSFUL, you can browse (.*)/);
          const taskId = taskIdMatch ? taskIdMatch[1] : void 0;
          return {
            success: true,
            taskId
          };
        } catch (error) {
          logger.error("SonarQube analysis failed", {
            error: error instanceof Error ? error.message : String(error)
          });
          return {
            success: false,
            error: error instanceof Error ? error.message : String(error)
          };
        }
      }
      async runLocalAnalysis() {
        try {
          const eslintCmd = "npx eslint . --ext .ts,.tsx,.js,.jsx --format json --output-file sonar-eslint-report.json";
          try {
            await execAsync(eslintCmd);
          } catch (error) {
            logger.info("ESLint analysis completed with issues found");
          }
          await this.generateCodeMetrics();
          return {
            success: true,
            taskId: `local-${Date.now()}`
          };
        } catch (error) {
          logger.error("Local code analysis failed", {
            error: error instanceof Error ? error.message : String(error)
          });
          return {
            success: false,
            error: error instanceof Error ? error.message : String(error)
          };
        }
      }
      async generateCodeMetrics() {
        try {
          const { stdout: locOutput } = await execAsync('find . -name "*.ts" -o -name "*.tsx" -o -name "*.js" -o -name "*.jsx" | grep -v node_modules | grep -v dist | xargs wc -l');
          const lines = locOutput.split("\n").filter((line) => line.trim());
          const totalLines = lines.pop()?.trim().split(" ")[0] || "0";
          const { stdout: fileCount } = await execAsync('find . -name "*.ts" -o -name "*.tsx" -o -name "*.js" -o -name "*.jsx" | grep -v node_modules | grep -v dist | wc -l');
          const metrics = {
            totalLines: parseInt(totalLines),
            totalFiles: parseInt(fileCount.trim()),
            analysisDate: (/* @__PURE__ */ new Date()).toISOString(),
            projectKey: this.projectKey
          };
          await fs.writeFile("sonar-metrics.json", JSON.stringify(metrics, null, 2));
          logger.info("Code metrics generated", metrics);
        } catch (error) {
          logger.error("Failed to generate code metrics", { error });
        }
      }
      async getAnalysisResults() {
        try {
          const eslintReportPath = path.join(process.cwd(), "sonar-eslint-report.json");
          const metricsPath = path.join(process.cwd(), "sonar-metrics.json");
          let eslintReport = [];
          let metrics = null;
          try {
            const eslintData = await fs.readFile(eslintReportPath, "utf-8");
            eslintReport = JSON.parse(eslintData);
          } catch {
            logger.warn("No ESLint report found, generating mock data for demo");
          }
          try {
            const metricsData = await fs.readFile(metricsPath, "utf-8");
            metrics = JSON.parse(metricsData);
          } catch {
            logger.warn("No metrics file found");
          }
          const issues = this.convertEslintToSonarFormat(eslintReport);
          const qualityMetrics = this.calculateQualityMetrics(issues, metrics);
          return {
            projectKey: this.projectKey,
            analysisDate: (/* @__PURE__ */ new Date()).toISOString(),
            qualityGate: {
              status: qualityMetrics.overallRating === "A" ? "OK" : "ERROR",
              conditions: [
                {
                  metric: "bugs",
                  status: qualityMetrics.issuesSummary.critical === 0 ? "OK" : "ERROR",
                  actualValue: qualityMetrics.issuesSummary.critical.toString(),
                  errorThreshold: "0"
                },
                {
                  metric: "vulnerabilities",
                  status: qualityMetrics.securityScore > 80 ? "OK" : "ERROR",
                  actualValue: qualityMetrics.securityScore.toString(),
                  errorThreshold: "80"
                }
              ]
            },
            metrics: {
              bugs: qualityMetrics.issuesSummary.critical,
              vulnerabilities: issues.filter((i) => i.type === "VULNERABILITY").length,
              securityHotspots: issues.filter((i) => i.severity === "MAJOR" && i.rule.includes("security")).length,
              codeSmells: qualityMetrics.issuesSummary.minor + qualityMetrics.issuesSummary.info,
              coverage: qualityMetrics.coverage,
              duplicatedLines: qualityMetrics.duplications,
              technicalDebt: `${qualityMetrics.technicalDebt.hours}h`,
              reliability: this.getRatingLetter(qualityMetrics.reliabilityScore),
              security: this.getRatingLetter(qualityMetrics.securityScore),
              maintainability: this.getRatingLetter(qualityMetrics.maintainabilityScore)
            },
            issues: issues.slice(0, 100)
            // Limit to first 100 issues
          };
        } catch (error) {
          logger.error("Failed to get analysis results", { error });
          return null;
        }
      }
      convertEslintToSonarFormat(eslintReport) {
        const issues = [];
        eslintReport.forEach((file, fileIndex) => {
          if (file.messages) {
            file.messages.forEach((message, messageIndex) => {
              issues.push({
                key: `${fileIndex}-${messageIndex}`,
                rule: message.ruleId || "unknown",
                severity: this.mapEslintSeverityToSonar(message.severity),
                component: file.filePath || "unknown",
                line: message.line || 1,
                message: message.message,
                type: this.determineIssueType(message.ruleId)
              });
            });
          }
        });
        return issues;
      }
      mapEslintSeverityToSonar(severity) {
        switch (severity) {
          case 2:
            return "MAJOR";
          case 1:
            return "MINOR";
          default:
            return "INFO";
        }
      }
      determineIssueType(ruleId) {
        if (!ruleId) return "CODE_SMELL";
        if (ruleId.includes("security") || ruleId.includes("vuln")) {
          return "VULNERABILITY";
        }
        if (ruleId.includes("bug") || ruleId.includes("error")) {
          return "BUG";
        }
        return "CODE_SMELL";
      }
      calculateQualityMetrics(issues, metrics) {
        const issuesSummary = {
          critical: issues.filter((i) => i.severity === "CRITICAL").length,
          major: issues.filter((i) => i.severity === "MAJOR").length,
          minor: issues.filter((i) => i.severity === "MINOR").length,
          info: issues.filter((i) => i.severity === "INFO").length
        };
        const totalIssues = Object.values(issuesSummary).reduce((a, b) => a + b, 0);
        const securityIssues = issues.filter((i) => i.type === "VULNERABILITY").length;
        const bugIssues = issues.filter((i) => i.type === "BUG").length;
        const securityScore = Math.max(0, 100 - securityIssues * 10);
        const reliabilityScore = Math.max(0, 100 - bugIssues * 15);
        const maintainabilityScore = Math.max(0, 100 - totalIssues * 2);
        const avgScore = (securityScore + reliabilityScore + maintainabilityScore) / 3;
        const overallRating = this.getRatingLetter(avgScore);
        return {
          overallRating,
          securityScore,
          reliabilityScore,
          maintainabilityScore,
          coverage: 75,
          // Mock coverage for demo
          duplications: Math.min(issues.length * 2, 100),
          technicalDebt: {
            hours: Math.ceil(totalIssues * 0.5),
            days: Math.ceil(totalIssues * 0.5 / 8)
          },
          issuesSummary
        };
      }
      getRatingLetter(score) {
        if (score >= 90) return "A";
        if (score >= 80) return "B";
        if (score >= 70) return "C";
        if (score >= 60) return "D";
        return "E";
      }
      async getProjectMetrics() {
        try {
          const results = await this.getAnalysisResults();
          if (!results) return null;
          const issues = results.issues;
          const issuesSummary = {
            critical: issues.filter((i) => i.severity === "CRITICAL").length,
            major: issues.filter((i) => i.severity === "MAJOR").length,
            minor: issues.filter((i) => i.severity === "MINOR").length,
            info: issues.filter((i) => i.severity === "INFO").length
          };
          return {
            overallRating: results.metrics.maintainability,
            securityScore: this.getScoreFromRating(results.metrics.security),
            reliabilityScore: this.getScoreFromRating(results.metrics.reliability),
            maintainabilityScore: this.getScoreFromRating(results.metrics.maintainability),
            coverage: results.metrics.coverage,
            duplications: results.metrics.duplicatedLines,
            technicalDebt: {
              hours: parseInt(results.metrics.technicalDebt.replace("h", "")),
              days: Math.ceil(parseInt(results.metrics.technicalDebt.replace("h", "")) / 8)
            },
            issuesSummary
          };
        } catch (error) {
          logger.error("Failed to get project metrics", { error });
          return null;
        }
      }
      getScoreFromRating(rating) {
        switch (rating) {
          case "A":
            return 95;
          case "B":
            return 85;
          case "C":
            return 75;
          case "D":
            return 65;
          case "E":
            return 50;
          default:
            return 0;
        }
      }
    };
    sonarService = new SonarService();
  }
});

// server/services/jenkins-generator.ts
var jenkins_generator_exports = {};
__export(jenkins_generator_exports, {
  generateJenkinsPipeline: () => generateJenkinsPipeline
});
function generateJenkinsPipeline(config) {
  const {
    projectName,
    projectType,
    gitRepository,
    branch,
    buildTool,
    nodeVersion,
    javaVersion,
    pythonVersion,
    testCommand,
    buildCommand,
    deploymentTarget,
    dockerEnabled,
    sonarEnabled,
    slackNotifications,
    emailNotifications,
    stages,
    environmentVariables
  } = config;
  let pipeline = `pipeline {
    agent any
    
    environment {`;
  environmentVariables.forEach((env) => {
    if (env.key && env.value) {
      pipeline += `
        ${env.key} = '${env.value}'`;
    }
  });
  pipeline += `
    }
    
    tools {`;
  if (projectType === "nodejs" && nodeVersion) {
    pipeline += `
        nodejs '${nodeVersion}'`;
  } else if (projectType === "java" && javaVersion) {
    pipeline += `
        maven 'Maven-3.8'`;
    pipeline += `
        jdk 'JDK-${javaVersion}'`;
  } else if (projectType === "python" && pythonVersion) {
    pipeline += `
        python '${pythonVersion}'`;
  }
  pipeline += `
    }
    
    stages {`;
  if (stages.includes("checkout")) {
    pipeline += `
        stage('Checkout') {
            steps {
                git branch: '${branch}', url: '${gitRepository}'
            }
        }`;
  }
  if (stages.includes("build")) {
    let buildSteps = "";
    if (projectType === "nodejs") {
      buildSteps = buildTool === "yarn" ? "yarn install" : "npm ci";
    } else if (projectType === "java") {
      buildSteps = "mvn clean compile";
    } else if (projectType === "python") {
      buildSteps = "pip install -r requirements.txt";
    }
    pipeline += `
        stage('Build') {
            steps {
                sh '${buildSteps}'`;
    if (buildCommand && buildCommand !== "npm run build") {
      pipeline += `
                sh '${buildCommand}'`;
    }
    pipeline += `
            }
        }`;
  }
  if (stages.includes("test") && testCommand) {
    pipeline += `
        stage('Test') {
            steps {
                sh '${testCommand}'
            }
            post {
                always {
                    publishTestResults testResultsPattern: 'test-results.xml'
                }
            }
        }`;
  }
  if (sonarEnabled && stages.includes("sonar-analysis")) {
    pipeline += `
        stage('SonarQube Analysis') {
            steps {
                withSonarQubeEnv('SonarQube') {`;
    if (projectType === "nodejs") {
      pipeline += `
                    sh 'sonar-scanner'`;
    } else if (projectType === "java") {
      pipeline += `
                    sh 'mvn sonar:sonar'`;
    }
    pipeline += `
                }
            }
        }
        
        stage('Quality Gate') {
            steps {
                timeout(time: 1, unit: 'HOURS') {
                    waitForQualityGate abortPipeline: true
                }
            }
        }`;
  }
  if (dockerEnabled && stages.includes("docker-build")) {
    pipeline += `
        stage('Docker Build') {
            steps {
                script {
                    def image = docker.build("${projectName}:${branch}-\${BUILD_NUMBER}")
                    docker.withRegistry('https://registry.hub.docker.com', 'docker-hub-credentials') {
                        image.push()
                        image.push('latest')
                    }
                }
            }
        }`;
  }
  if (stages.includes("deploy-production") && deploymentTarget === "production") {
    pipeline += `
        stage('Deploy to Production') {
            when {
                branch 'main'
            }
            steps {
                input message: 'Deploy to production?', ok: 'Deploy'
                sh 'echo "Deploying to production..."'
                // Add your production deployment script here
            }
        }`;
  }
  pipeline += `
    }
    
    post {
        always {
            cleanWs()
        }
        success {`;
  if (emailNotifications) {
    pipeline += `
            emailext (
                subject: "SUCCESS: Job '\${env.JOB_NAME} [\${env.BUILD_NUMBER}]'",
                body: "Good news! Build \${env.BUILD_NUMBER} was successful.",
                to: "${emailNotifications}"
            )`;
  }
  if (slackNotifications) {
    pipeline += `
            slackSend (
                color: 'good',
                message: "SUCCESS: Job '\${env.JOB_NAME} [\${env.BUILD_NUMBER}]' completed successfully."
            )`;
  }
  pipeline += `
        }
        failure {`;
  if (emailNotifications) {
    pipeline += `
            emailext (
                subject: "FAILED: Job '\${env.JOB_NAME} [\${env.BUILD_NUMBER}]'",
                body: "Build \${env.BUILD_NUMBER} failed. Please check the console output.",
                to: "${emailNotifications}"
            )`;
  }
  if (slackNotifications) {
    pipeline += `
            slackSend (
                color: 'danger',
                message: "FAILED: Job '\${env.JOB_NAME} [\${env.BUILD_NUMBER}]' failed!"
            )`;
  }
  pipeline += `
        }
    }
}`;
  return pipeline;
}
var init_jenkins_generator = __esm({
  "server/services/jenkins-generator.ts"() {
    "use strict";
  }
});

// server/services/ansible-generator.ts
var ansible_generator_exports = {};
__export(ansible_generator_exports, {
  generateAnsiblePlaybook: () => generateAnsiblePlaybook
});
function generateAnsiblePlaybook(config) {
  const {
    playbookName,
    targetHosts,
    user,
    becomeUser,
    tasks,
    variables,
    handlers,
    playbookType
  } = config;
  let playbook = `---
- name: ${playbookName}
  hosts: ${targetHosts}
  remote_user: ${user}
  become: yes
  become_user: ${becomeUser}`;
  if (variables.length > 0) {
    playbook += `
  vars:`;
    variables.forEach((variable) => {
      if (variable.key && variable.value) {
        playbook += `
    ${variable.key}: ${variable.value}`;
      }
    });
  }
  if (tasks.length > 0) {
    playbook += `
  tasks:`;
    tasks.forEach((task) => {
      playbook += `
    - name: ${task.name}`;
      playbook += `
      ${task.module}:`;
      Object.entries(task.parameters).forEach(([key, value]) => {
        if (value) {
          playbook += `
        ${key}: ${value}`;
        }
      });
      if (task.when) {
        playbook += `
      when: ${task.when}`;
      }
      if (task.tags && task.tags.length > 0) {
        playbook += `
      tags:`;
        task.tags.forEach((tag) => {
          playbook += `
        - ${tag}`;
        });
      }
      playbook += "\n";
    });
  } else {
    playbook += generateDefaultTasks(playbookType);
  }
  if (handlers && handlers.length > 0) {
    playbook += `
  handlers:`;
    handlers.forEach((handler2) => {
      playbook += `
    - name: ${handler2.name}`;
      playbook += `
      ${handler2.module}:`;
      Object.entries(handler2.parameters).forEach(([key, value]) => {
        if (value) {
          playbook += `
        ${key}: ${value}`;
        }
      });
    });
  }
  return playbook;
}
function generateDefaultTasks(playbookType) {
  let tasks = `
  tasks:`;
  switch (playbookType) {
    case "web-server":
      tasks += `
    - name: Update apt package cache
      apt:
        update_cache: yes
        
    - name: Install Nginx
      apt:
        name: nginx
        state: present
        
    - name: Start and enable Nginx
      systemd:
        name: nginx
        state: started
        enabled: yes`;
      break;
    default:
      tasks += `
    - name: Update system packages
      apt:
        update_cache: yes
        upgrade: safe`;
  }
  return tasks;
}
var init_ansible_generator = __esm({
  "server/services/ansible-generator.ts"() {
    "use strict";
  }
});

// server/services/sonarqube-generator.ts
var sonarqube_generator_exports = {};
__export(sonarqube_generator_exports, {
  generateSonarQubeSetup: () => generateSonarQubeSetup
});
function generateSonarQubeSetup(config) {
  const { setupType } = config;
  if (setupType === "docker") {
    return generateDockerSetup(config);
  } else if (setupType === "kubernetes") {
    return generateKubernetesSetup(config);
  } else if (setupType === "manual") {
    return generateManualSetup(config);
  } else {
    return generateDockerSetup(config);
  }
}
function generateDockerSetup(config) {
  const { version, serverPort, plugins } = config;
  let dockerCompose = `version: '3.8'

services:
  sonarqube:
    image: sonarqube:${version}
    container_name: sonarqube
    ports:
      - "${serverPort}:9000"
    environment:
      - SONAR_JDBC_URL=jdbc:postgresql://db:5432/sonarqube
      - SONAR_JDBC_USERNAME=sonarqube
      - SONAR_JDBC_PASSWORD=sonarqube_password
      - SONAR_ES_BOOTSTRAP_CHECKS_DISABLE=true
    volumes:
      - sonarqube_data:/opt/sonarqube/data
      - sonarqube_logs:/opt/sonarqube/logs
      - sonarqube_extensions:/opt/sonarqube/extensions`;
  if (plugins.length > 0) {
    dockerCompose += `
      - ./plugins:/opt/sonarqube/extensions/plugins`;
  }
  dockerCompose += `
    depends_on:
      - db
    networks:
      - sonarqube_network

  db:
    image: postgres:13
    container_name: sonarqube_db
    environment:
      - POSTGRES_USER=sonarqube
      - POSTGRES_PASSWORD=sonarqube_password
      - POSTGRES_DB=sonarqube
    volumes:
      - postgresql_data:/var/lib/postgresql/data
    networks:
      - sonarqube_network

volumes:
  sonarqube_data:
  sonarqube_logs:
  sonarqube_extensions:
  postgresql_data:

networks:
  sonarqube_network:
    driver: bridge

# Setup Instructions:
# 1. Save this as docker-compose.yml
# 2. Run: docker-compose up -d
# 3. Access: http://${config.serverHost}:${serverPort}
# 4. Default credentials: admin/admin`;
  return dockerCompose;
}
function generateKubernetesSetup(config) {
  const { version, serverPort } = config;
  return `# SonarQube Kubernetes Deployment
apiVersion: v1
kind: Namespace
metadata:
  name: sonarqube

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sonarqube
  namespace: sonarqube
spec:
  replicas: 1
  selector:
    matchLabels:
      app: sonarqube
  template:
    metadata:
      labels:
        app: sonarqube
    spec:
      containers:
      - name: sonarqube
        image: sonarqube:${version}
        ports:
        - containerPort: 9000
        env:
        - name: SONAR_JDBC_URL
          value: "jdbc:postgresql://postgres-service:5432/sonarqube"
        - name: SONAR_JDBC_USERNAME
          value: "sonarqube"
        - name: SONAR_JDBC_PASSWORD
          value: "sonarqube_password"

---
apiVersion: v1
kind: Service
metadata:
  name: sonarqube-service
  namespace: sonarqube
spec:
  selector:
    app: sonarqube
  ports:
  - port: 9000
    targetPort: 9000
  type: LoadBalancer

# Deploy with: kubectl apply -f sonarqube-k8s.yaml`;
}
function generateManualSetup(config) {
  const { version, javaVersion, serverPort, serverHost } = config;
  return `#!/bin/bash
# SonarQube Manual Installation Script

set -e

echo "Starting SonarQube ${version} installation..."

# Install Java ${javaVersion}
echo "Installing Java ${javaVersion}..."
sudo apt update
sudo apt install -y openjdk-${javaVersion}-jdk

# Create SonarQube user
echo "Creating sonarqube user..."
sudo useradd -m -d /opt/sonarqube -s /bin/bash sonarqube

# Download and install SonarQube
echo "Downloading SonarQube ${version}..."
cd /tmp
wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-${version.replace("-community", "")}.zip
unzip sonarqube-${version.replace("-community", "")}.zip
sudo mv sonarqube-${version.replace("-community", "")} /opt/sonarqube
sudo chown -R sonarqube:sonarqube /opt/sonarqube

# Configure SonarQube
echo "Configuring SonarQube..."
sudo -u sonarqube tee /opt/sonarqube/conf/sonar.properties << EOF
sonar.web.host=${serverHost}
sonar.web.port=${serverPort}
EOF

echo "SonarQube installation completed!"
echo "Access SonarQube at: http://${serverHost}:${serverPort}"
echo "Default credentials: admin/admin"`;
}
var init_sonarqube_generator = __esm({
  "server/services/sonarqube-generator.ts"() {
    "use strict";
  }
});

// server/services/snowflake-generator.ts
var snowflake_generator_exports = {};
__export(snowflake_generator_exports, {
  generateSnowflakeSetup: () => generateSnowflakeSetup
});
function generateSnowflakeSetup(config) {
  const {
    accountName,
    region,
    cloudProvider,
    warehouseName,
    warehouseSize,
    databaseName,
    schemaName,
    roleName,
    userName,
    enableMultiCluster,
    autoSuspend,
    minClusters,
    maxClusters,
    enableDataSharing,
    enableTimeTravel,
    enableFailsafe,
    networkPolicy
  } = config;
  const sections = [];
  sections.push(`-- ============================================================
-- Snowflake Data Warehouse Setup Script
-- Generated by Prometix DevOps Platform
-- Account: ${accountName || "<YOUR_ACCOUNT>"}
-- Region: ${region} (${cloudProvider?.toUpperCase() || "AWS"})
-- Generated: ${(/* @__PURE__ */ new Date()).toISOString()}
-- ============================================================

-- IMPORTANT: Run this script with ACCOUNTADMIN or equivalent privileges
-- Make sure to review and modify any placeholder values before executing

USE ROLE ACCOUNTADMIN;
`);
  sections.push(`-- ============================================================
-- SECTION 1: Create Database and Schema
-- ============================================================

CREATE DATABASE IF NOT EXISTS ${databaseName || "ANALYTICS_DB"}
  DATA_RETENTION_TIME_IN_DAYS = ${enableTimeTravel || 1}
  COMMENT = 'Main analytics database created by Prometix';

USE DATABASE ${databaseName || "ANALYTICS_DB"};

CREATE SCHEMA IF NOT EXISTS ${schemaName || "PUBLIC"}
  DATA_RETENTION_TIME_IN_DAYS = ${enableTimeTravel || 1}
  COMMENT = 'Primary schema for ${databaseName || "ANALYTICS_DB"}';

USE SCHEMA ${schemaName || "PUBLIC"};
`);
  const warehouseOptions = [
    `WAREHOUSE_SIZE = '${warehouseSize || "XSMALL"}'`,
    `AUTO_SUSPEND = ${autoSuspend || 300}`,
    `AUTO_RESUME = TRUE`,
    `INITIALLY_SUSPENDED = TRUE`
  ];
  if (enableMultiCluster) {
    warehouseOptions.push(`MIN_CLUSTER_COUNT = ${minClusters || 1}`);
    warehouseOptions.push(`MAX_CLUSTER_COUNT = ${maxClusters || 3}`);
    warehouseOptions.push(`SCALING_POLICY = 'STANDARD'`);
  }
  sections.push(`-- ============================================================
-- SECTION 2: Create Virtual Warehouse
-- ============================================================

CREATE WAREHOUSE IF NOT EXISTS ${warehouseName || "COMPUTE_WH"}
  WITH
    ${warehouseOptions.join("\n    ")}
  COMMENT = 'Primary compute warehouse created by Prometix';

-- Create additional warehouses for different workloads (optional)
CREATE WAREHOUSE IF NOT EXISTS ${warehouseName || "COMPUTE_WH"}_ETL
  WITH
    WAREHOUSE_SIZE = 'MEDIUM'
    AUTO_SUSPEND = 120
    AUTO_RESUME = TRUE
    INITIALLY_SUSPENDED = TRUE
  COMMENT = 'ETL/ELT workload warehouse';

CREATE WAREHOUSE IF NOT EXISTS ${warehouseName || "COMPUTE_WH"}_BI
  WITH
    WAREHOUSE_SIZE = 'SMALL'
    AUTO_SUSPEND = 600
    AUTO_RESUME = TRUE
    INITIALLY_SUSPENDED = TRUE
  COMMENT = 'Business Intelligence reporting warehouse';
`);
  sections.push(`-- ============================================================
-- SECTION 3: Create Roles and Grant Privileges
-- ============================================================

-- Create functional roles
CREATE ROLE IF NOT EXISTS ${roleName || "ANALYST_ROLE"}
  COMMENT = 'Role for data analysts';

CREATE ROLE IF NOT EXISTS ${roleName || "ANALYST_ROLE"}_ADMIN
  COMMENT = 'Administrative role for ${roleName || "ANALYST_ROLE"}';

CREATE ROLE IF NOT EXISTS DATA_ENGINEER_ROLE
  COMMENT = 'Role for data engineers';

CREATE ROLE IF NOT EXISTS DATA_SCIENTIST_ROLE
  COMMENT = 'Role for data scientists';

-- Create role hierarchy
GRANT ROLE ${roleName || "ANALYST_ROLE"} TO ROLE ${roleName || "ANALYST_ROLE"}_ADMIN;
GRANT ROLE DATA_ENGINEER_ROLE TO ROLE SYSADMIN;
GRANT ROLE DATA_SCIENTIST_ROLE TO ROLE SYSADMIN;
GRANT ROLE ${roleName || "ANALYST_ROLE"}_ADMIN TO ROLE SYSADMIN;

-- Grant database privileges
GRANT USAGE ON DATABASE ${databaseName || "ANALYTICS_DB"} TO ROLE ${roleName || "ANALYST_ROLE"};
GRANT USAGE ON DATABASE ${databaseName || "ANALYTICS_DB"} TO ROLE DATA_ENGINEER_ROLE;
GRANT USAGE ON DATABASE ${databaseName || "ANALYTICS_DB"} TO ROLE DATA_SCIENTIST_ROLE;

-- Grant schema privileges
GRANT USAGE ON SCHEMA ${databaseName || "ANALYTICS_DB"}.${schemaName || "PUBLIC"} TO ROLE ${roleName || "ANALYST_ROLE"};
GRANT USAGE ON SCHEMA ${databaseName || "ANALYTICS_DB"}.${schemaName || "PUBLIC"} TO ROLE DATA_ENGINEER_ROLE;
GRANT USAGE ON SCHEMA ${databaseName || "ANALYTICS_DB"}.${schemaName || "PUBLIC"} TO ROLE DATA_SCIENTIST_ROLE;

-- Grant table privileges (for future tables)
GRANT SELECT ON FUTURE TABLES IN SCHEMA ${databaseName || "ANALYTICS_DB"}.${schemaName || "PUBLIC"} TO ROLE ${roleName || "ANALYST_ROLE"};
GRANT ALL PRIVILEGES ON FUTURE TABLES IN SCHEMA ${databaseName || "ANALYTICS_DB"}.${schemaName || "PUBLIC"} TO ROLE DATA_ENGINEER_ROLE;
GRANT SELECT ON FUTURE TABLES IN SCHEMA ${databaseName || "ANALYTICS_DB"}.${schemaName || "PUBLIC"} TO ROLE DATA_SCIENTIST_ROLE;

-- Grant warehouse privileges
GRANT USAGE ON WAREHOUSE ${warehouseName || "COMPUTE_WH"} TO ROLE ${roleName || "ANALYST_ROLE"};
GRANT USAGE ON WAREHOUSE ${warehouseName || "COMPUTE_WH"}_BI TO ROLE ${roleName || "ANALYST_ROLE"};
GRANT USAGE ON WAREHOUSE ${warehouseName || "COMPUTE_WH"}_ETL TO ROLE DATA_ENGINEER_ROLE;
GRANT USAGE ON WAREHOUSE ${warehouseName || "COMPUTE_WH"} TO ROLE DATA_SCIENTIST_ROLE;
`);
  if (userName) {
    sections.push(`-- ============================================================
-- SECTION 4: Create Users
-- ============================================================

-- Create admin user
-- IMPORTANT: Change the password before running in production!
CREATE USER IF NOT EXISTS ${userName}
  PASSWORD = 'CHANGE_ME_IMMEDIATELY!'
  DEFAULT_ROLE = ${roleName || "ANALYST_ROLE"}_ADMIN
  DEFAULT_WAREHOUSE = ${warehouseName || "COMPUTE_WH"}
  DEFAULT_NAMESPACE = ${databaseName || "ANALYTICS_DB"}.${schemaName || "PUBLIC"}
  MUST_CHANGE_PASSWORD = TRUE
  COMMENT = 'Admin user created by Prometix';

-- Grant role to user
GRANT ROLE ${roleName || "ANALYST_ROLE"}_ADMIN TO USER ${userName};

-- Sample user creation template for team members
-- CREATE USER IF NOT EXISTS <username>
--   PASSWORD = '<temporary_password>'
--   DEFAULT_ROLE = ${roleName || "ANALYST_ROLE"}
--   DEFAULT_WAREHOUSE = ${warehouseName || "COMPUTE_WH"}
--   MUST_CHANGE_PASSWORD = TRUE;
-- GRANT ROLE ${roleName || "ANALYST_ROLE"} TO USER <username>;
`);
  }
  if (networkPolicy) {
    sections.push(`-- ============================================================
-- SECTION 5: Network Security Policy
-- ============================================================

-- Create network policy to restrict access
-- IMPORTANT: Update the allowed IP list before enabling!
CREATE NETWORK POLICY IF NOT EXISTS ${networkPolicy}
  ALLOWED_IP_LIST = ('0.0.0.0/0')  -- CHANGE THIS TO YOUR CORPORATE IP RANGE
  BLOCKED_IP_LIST = ()
  COMMENT = 'Network policy created by Prometix';

-- To apply network policy to account (requires ACCOUNTADMIN):
-- ALTER ACCOUNT SET NETWORK_POLICY = ${networkPolicy};

-- To apply network policy to specific user:
-- ALTER USER ${userName || "<username>"} SET NETWORK_POLICY = ${networkPolicy};
`);
  }
  if (enableDataSharing) {
    sections.push(`-- ============================================================
-- SECTION 6: Secure Data Sharing Setup
-- ============================================================

-- Create a share for external data sharing
CREATE SHARE IF NOT EXISTS ${databaseName || "ANALYTICS_DB"}_SHARE
  COMMENT = 'Secure data share created by Prometix';

-- Grant privileges to share
GRANT USAGE ON DATABASE ${databaseName || "ANALYTICS_DB"} TO SHARE ${databaseName || "ANALYTICS_DB"}_SHARE;
GRANT USAGE ON SCHEMA ${databaseName || "ANALYTICS_DB"}.${schemaName || "PUBLIC"} TO SHARE ${databaseName || "ANALYTICS_DB"}_SHARE;

-- To add tables to share:
-- GRANT SELECT ON TABLE ${databaseName || "ANALYTICS_DB"}.${schemaName || "PUBLIC"}.<table_name> TO SHARE ${databaseName || "ANALYTICS_DB"}_SHARE;

-- To add consumer accounts:
-- ALTER SHARE ${databaseName || "ANALYTICS_DB"}_SHARE ADD ACCOUNTS = <consumer_account>;
`);
  }
  sections.push(`-- ============================================================
-- SECTION 7: Resource Monitors (Cost Control)
-- ============================================================

-- Create resource monitor to control costs
CREATE RESOURCE MONITOR IF NOT EXISTS ${warehouseName || "COMPUTE_WH"}_MONITOR
  WITH
    CREDIT_QUOTA = 100
    FREQUENCY = MONTHLY
    START_TIMESTAMP = IMMEDIATELY
    TRIGGERS
      ON 75 PERCENT DO NOTIFY
      ON 90 PERCENT DO NOTIFY
      ON 100 PERCENT DO SUSPEND
  COMMENT = 'Resource monitor for ${warehouseName || "COMPUTE_WH"}';

-- Apply resource monitor to warehouse
ALTER WAREHOUSE ${warehouseName || "COMPUTE_WH"} SET RESOURCE_MONITOR = ${warehouseName || "COMPUTE_WH"}_MONITOR;

-- Create account-level resource monitor (optional, requires ACCOUNTADMIN)
-- CREATE RESOURCE MONITOR IF NOT EXISTS ACCOUNT_MONITOR
--   WITH CREDIT_QUOTA = 1000
--   FREQUENCY = MONTHLY
--   TRIGGERS
--     ON 50 PERCENT DO NOTIFY
--     ON 75 PERCENT DO NOTIFY
--     ON 100 PERCENT DO SUSPEND_IMMEDIATE;
`);
  sections.push(`-- ============================================================
-- SECTION 8: Sample Staging Tables and File Formats
-- ============================================================

-- Create file formats for data loading
CREATE FILE FORMAT IF NOT EXISTS CSV_FORMAT
  TYPE = CSV
  FIELD_DELIMITER = ','
  SKIP_HEADER = 1
  NULL_IF = ('NULL', 'null', '')
  EMPTY_FIELD_AS_NULL = TRUE
  COMPRESSION = AUTO;

CREATE FILE FORMAT IF NOT EXISTS JSON_FORMAT
  TYPE = JSON
  COMPRESSION = AUTO
  STRIP_OUTER_ARRAY = TRUE;

CREATE FILE FORMAT IF NOT EXISTS PARQUET_FORMAT
  TYPE = PARQUET
  COMPRESSION = SNAPPY;

-- Create internal stage for file uploads
CREATE STAGE IF NOT EXISTS ${databaseName || "ANALYTICS_DB"}_STAGE
  FILE_FORMAT = CSV_FORMAT
  COMMENT = 'Internal stage for data loading';

-- Sample external stage template (S3)
-- CREATE STAGE IF NOT EXISTS ${databaseName || "ANALYTICS_DB"}_S3_STAGE
--   URL = 's3://your-bucket-name/path/'
--   STORAGE_INTEGRATION = <your_storage_integration>
--   FILE_FORMAT = PARQUET_FORMAT;
`);
  sections.push(`-- ============================================================
-- SECTION 9: Sample Dimensional Model (Star Schema)
-- ============================================================

-- Date dimension table
CREATE TABLE IF NOT EXISTS DIM_DATE (
  DATE_KEY INTEGER PRIMARY KEY,
  FULL_DATE DATE NOT NULL,
  DAY_OF_WEEK INTEGER,
  DAY_NAME VARCHAR(10),
  DAY_OF_MONTH INTEGER,
  DAY_OF_YEAR INTEGER,
  WEEK_OF_YEAR INTEGER,
  MONTH_NUMBER INTEGER,
  MONTH_NAME VARCHAR(10),
  QUARTER INTEGER,
  YEAR INTEGER,
  IS_WEEKEND BOOLEAN,
  IS_HOLIDAY BOOLEAN
);

-- Sample fact table template
-- CREATE TABLE IF NOT EXISTS FACT_SALES (
--   SALE_ID INTEGER AUTOINCREMENT PRIMARY KEY,
--   DATE_KEY INTEGER REFERENCES DIM_DATE(DATE_KEY),
--   PRODUCT_KEY INTEGER,
--   CUSTOMER_KEY INTEGER,
--   QUANTITY INTEGER,
--   UNIT_PRICE DECIMAL(10,2),
--   TOTAL_AMOUNT DECIMAL(12,2),
--   CREATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
-- );
`);
  sections.push(`-- ============================================================
-- SECTION 10: Useful Queries and Commands
-- ============================================================

-- View warehouse usage
-- SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY
-- WHERE START_TIME >= DATEADD(DAY, -7, CURRENT_TIMESTAMP())
-- ORDER BY START_TIME DESC;

-- View query history
-- SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY
-- WHERE START_TIME >= DATEADD(DAY, -1, CURRENT_TIMESTAMP())
-- ORDER BY START_TIME DESC
-- LIMIT 100;

-- View storage usage
-- SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.STORAGE_USAGE
-- ORDER BY USAGE_DATE DESC
-- LIMIT 30;

-- View login history
-- SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.LOGIN_HISTORY
-- WHERE EVENT_TIMESTAMP >= DATEADD(DAY, -7, CURRENT_TIMESTAMP())
-- ORDER BY EVENT_TIMESTAMP DESC;

-- ============================================================
-- Setup Complete!
-- ============================================================
-- Next Steps:
-- 1. Change default passwords for any created users
-- 2. Update network policy with your corporate IP ranges
-- 3. Create additional schemas as needed
-- 4. Set up storage integrations for external stages
-- 5. Configure BI tool connections
-- ============================================================
`);
  return sections.join("\n");
}
var init_snowflake_generator = __esm({
  "server/services/snowflake-generator.ts"() {
    "use strict";
  }
});

// server/services/airflow-generator.ts
var airflow_generator_exports = {};
__export(airflow_generator_exports, {
  generateAirflowDAG: () => generateAirflowDAG
});
function generateAirflowDAG(config) {
  const {
    dagId,
    description,
    schedule,
    startDate,
    catchup,
    maxActiveRuns,
    retries,
    retryDelay,
    owner,
    email,
    emailOnFailure,
    emailOnRetry,
    tags,
    tasks,
    defaultArgs
  } = config;
  const sections = [];
  const operators = /* @__PURE__ */ new Set();
  tasks.forEach((task) => {
    operators.add(task.operator);
  });
  const imports = [
    "from datetime import datetime, timedelta",
    "from airflow import DAG"
  ];
  if (operators.has("PythonOperator") || operators.has("BranchPythonOperator")) {
    imports.push("from airflow.operators.python import PythonOperator, BranchPythonOperator");
  }
  if (operators.has("BashOperator")) {
    imports.push("from airflow.operators.bash import BashOperator");
  }
  if (operators.has("EmptyOperator")) {
    imports.push("from airflow.operators.empty import EmptyOperator");
  }
  if (operators.has("PostgresOperator")) {
    imports.push("from airflow.providers.postgres.operators.postgres import PostgresOperator");
  }
  if (operators.has("SnowflakeOperator") || operators.has("S3ToSnowflakeOperator")) {
    imports.push("from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator");
    if (operators.has("S3ToSnowflakeOperator")) {
      imports.push("from airflow.providers.snowflake.transfers.s3_to_snowflake import S3ToSnowflakeOperator");
    }
  }
  if (operators.has("BigQueryOperator")) {
    imports.push("from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator");
  }
  if (operators.has("SparkSubmitOperator")) {
    imports.push("from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator");
  }
  if (operators.has("TriggerDagRunOperator")) {
    imports.push("from airflow.operators.trigger_dagrun import TriggerDagRunOperator");
  }
  sections.push(`"""
${description || `Airflow DAG: ${dagId}`}

Generated by Prometix DevOps Platform
Generated: ${(/* @__PURE__ */ new Date()).toISOString()}

DAG ID: ${dagId}
Schedule: ${schedule}
Owner: ${owner || "airflow"}
"""

${imports.join("\n")}
`);
  const emailList = email ? `['${email}']` : "[]";
  sections.push(`
# Default arguments for all tasks
default_args = {
    'owner': '${owner || "airflow"}',
    'depends_on_past': ${defaultArgs?.depends_on_past ? "True" : "False"},
    'wait_for_downstream': ${defaultArgs?.wait_for_downstream ? "True" : "False"},
    'email': ${emailList},
    'email_on_failure': ${emailOnFailure ? "True" : "False"},
    'email_on_retry': ${emailOnRetry ? "True" : "False"},
    'retries': ${retries || 2},
    'retry_delay': timedelta(minutes=${retryDelay || 5}),
}
`);
  const pythonFunctions = [];
  tasks.forEach((task) => {
    if (task.operator === "PythonOperator" || task.operator === "BranchPythonOperator") {
      const funcName = task.pythonCallable || task.name;
      pythonFunctions.push(`
def ${funcName}(**kwargs):
    """
    Task: ${task.name}
    
    TODO: Implement your ${task.operator === "BranchPythonOperator" ? "branching logic" : "task logic"} here
    
    Available kwargs:
        - ds: execution date as string (YYYY-MM-DD)
        - ds_nodash: execution date without dashes
        - ts: execution timestamp
        - execution_date: datetime object
        - prev_execution_date: previous execution date
        - next_execution_date: next execution date
        - dag: the DAG object
        - task: the Task object
        - task_instance: TaskInstance object (ti)
        - params: user-defined params from DAG definition
        - var: Airflow Variables
        - conn: Airflow Connections
    
    Example using XCom:
        # Push value to XCom
        kwargs['ti'].xcom_push(key='my_key', value='my_value')
        
        # Pull value from XCom
        value = kwargs['ti'].xcom_pull(task_ids='previous_task', key='my_key')
    """
    print(f"Executing ${task.name}...")
    
    # Your implementation here
    ${task.operator === "BranchPythonOperator" ? `
    # Return the task_id to execute next
    # return 'task_id_if_true'
    # return 'task_id_if_false'
    pass` : `
    # Example: Process data
    # data = fetch_data()
    # result = transform_data(data)
    # return result
    pass`}
`);
    }
  });
  if (pythonFunctions.length > 0) {
    sections.push(`
# ============================================================
# Python Callables for PythonOperator Tasks
# ============================================================
${pythonFunctions.join("\n")}
`);
  }
  const tagsStr = tags && tags.length > 0 ? `[${tags.map((t) => `'${t}'`).join(", ")}]` : "['data-pipeline']";
  const startDateFormatted = startDate || (/* @__PURE__ */ new Date()).toISOString().split("T")[0];
  const [year, month, day] = startDateFormatted.split("-");
  sections.push(`
# ============================================================
# DAG Definition
# ============================================================

with DAG(
    dag_id='${dagId || "my_dag"}',
    default_args=default_args,
    description='${(description || "").replace(/'/g, "\\'")}',
    schedule_interval='${schedule || "@daily"}',
    start_date=datetime(${year}, ${parseInt(month)}, ${parseInt(day)}),
    catchup=${catchup ? "True" : "False"},
    max_active_runs=${maxActiveRuns || 1},
    tags=${tagsStr},
) as dag:
`);
  const taskDefinitions = [];
  const taskIdMap = /* @__PURE__ */ new Map();
  tasks.forEach((task) => {
    taskIdMap.set(task.id, task.name);
  });
  tasks.forEach((task) => {
    let taskDef = "";
    const taskVarName = task.name.replace(/-/g, "_");
    switch (task.operator) {
      case "PythonOperator":
        taskDef = `
    ${taskVarName} = PythonOperator(
        task_id='${task.name}',
        python_callable=${task.pythonCallable || task.name},
        provide_context=True,
    )`;
        break;
      case "BranchPythonOperator":
        taskDef = `
    ${taskVarName} = BranchPythonOperator(
        task_id='${task.name}',
        python_callable=${task.pythonCallable || task.name},
        provide_context=True,
    )`;
        break;
      case "BashOperator":
        const bashCmd = (task.bashCommand || 'echo "Hello World"').replace(/'/g, "\\'");
        taskDef = `
    ${taskVarName} = BashOperator(
        task_id='${task.name}',
        bash_command='${bashCmd}',
    )`;
        break;
      case "EmptyOperator":
        taskDef = `
    ${taskVarName} = EmptyOperator(
        task_id='${task.name}',
    )`;
        break;
      case "PostgresOperator":
        const pgSql = (task.sqlQuery || "SELECT 1;").replace(/'/g, "\\'");
        taskDef = `
    ${taskVarName} = PostgresOperator(
        task_id='${task.name}',
        postgres_conn_id='postgres_default',  # Configure in Airflow Connections
        sql='''${pgSql}''',
    )`;
        break;
      case "SnowflakeOperator":
        const sfSql = (task.sqlQuery || "SELECT CURRENT_TIMESTAMP();").replace(/'/g, "\\'");
        taskDef = `
    ${taskVarName} = SnowflakeOperator(
        task_id='${task.name}',
        snowflake_conn_id='snowflake_default',  # Configure in Airflow Connections
        sql='''${sfSql}''',
        warehouse='COMPUTE_WH',  # Update with your warehouse
    )`;
        break;
      case "S3ToSnowflakeOperator":
        taskDef = `
    ${taskVarName} = S3ToSnowflakeOperator(
        task_id='${task.name}',
        snowflake_conn_id='snowflake_default',
        s3_keys=['your-file.csv'],  # Update with your S3 keys
        table='YOUR_TABLE',  # Update with your table name
        schema='PUBLIC',
        stage='YOUR_STAGE',  # Update with your Snowflake stage
        file_format='(TYPE=CSV, FIELD_DELIMITER=",")',
    )`;
        break;
      case "BigQueryOperator":
        const bqSql = (task.sqlQuery || "SELECT 1").replace(/'/g, "\\'");
        taskDef = `
    ${taskVarName} = BigQueryInsertJobOperator(
        task_id='${task.name}',
        gcp_conn_id='google_cloud_default',  # Configure in Airflow Connections
        configuration={
            'query': {
                'query': '''${bqSql}''',
                'useLegacySql': False,
            }
        },
    )`;
        break;
      case "SparkSubmitOperator":
        taskDef = `
    ${taskVarName} = SparkSubmitOperator(
        task_id='${task.name}',
        application='/path/to/your/spark_app.py',  # Update with your Spark app path
        conn_id='spark_default',  # Configure in Airflow Connections
        application_args=['--arg1', 'value1'],  # Update with your arguments
        conf={
            'spark.executor.memory': '2g',
            'spark.driver.memory': '1g',
        },
    )`;
        break;
      case "TriggerDagRunOperator":
        taskDef = `
    ${taskVarName} = TriggerDagRunOperator(
        task_id='${task.name}',
        trigger_dag_id='target_dag_id',  # Update with target DAG ID
        wait_for_completion=True,
        poke_interval=60,
    )`;
        break;
      default:
        taskDef = `
    ${taskVarName} = EmptyOperator(
        task_id='${task.name}',
    )`;
    }
    taskDefinitions.push(taskDef);
  });
  sections.push(`    # ========================================
    # Task Definitions
    # ========================================
${taskDefinitions.join("\n")}
`);
  const dependencies = [];
  tasks.forEach((task) => {
    if (task.dependencies && task.dependencies.length > 0) {
      const taskVarName = task.name.replace(/-/g, "_");
      task.dependencies.forEach((depId) => {
        const depName = taskIdMap.get(depId);
        if (depName) {
          const depVarName = depName.replace(/-/g, "_");
          dependencies.push(`    ${depVarName} >> ${taskVarName}`);
        }
      });
    }
  });
  if (dependencies.length > 0) {
    sections.push(`
    # ========================================
    # Task Dependencies
    # ========================================
${dependencies.join("\n")}
`);
  } else if (tasks.length > 1) {
    const taskNames = tasks.map((t) => t.name.replace(/-/g, "_"));
    sections.push(`
    # ========================================
    # Task Dependencies
    # No dependencies defined - tasks will run in parallel
    # To define sequential execution, uncomment below:
    # ========================================
    # ${taskNames.join(" >> ")}
`);
  }
  sections.push(`

# ============================================================
# DAG Documentation
# ============================================================
"""
How to use this DAG:

1. Save this file to your Airflow dags folder (usually ~/airflow/dags/)

2. Configure required connections in Airflow UI:
   - Go to Admin -> Connections
   - Add connections for any operators that require them:
     * postgres_default: PostgreSQL connection
     * snowflake_default: Snowflake connection
     * google_cloud_default: Google Cloud connection
     * spark_default: Spark connection

3. Configure required variables (if any):
   - Go to Admin -> Variables
   - Add any required variables

4. Enable the DAG in Airflow UI

5. Trigger manually or wait for scheduled run

Monitoring:
- View task logs in Airflow UI
- Check task instance details for execution info
- Use XCom for passing data between tasks

Troubleshooting:
- Check task logs for errors
- Verify connections are configured correctly
- Ensure all dependencies are installed
- Check scheduler logs if DAG doesn't appear

Best Practices:
- Use meaningful task IDs
- Keep tasks atomic and idempotent
- Use XCom sparingly for small data
- Set appropriate retries and retry_delay
- Use pools to limit concurrent tasks
- Add documentation and comments
"""
`);
  return sections.join("");
}
var init_airflow_generator = __esm({
  "server/services/airflow-generator.ts"() {
    "use strict";
  }
});

// server/routes.ts
import { createServer } from "http";
import { readFileSync, existsSync as existsSync2, statSync } from "fs";
import { join as join2 } from "path";
import { eq as eq3 } from "drizzle-orm";
import { GoogleGenerativeAI as GoogleGenerativeAI2 } from "@google/generative-ai";
import helmet from "helmet";
import { body, validationResult } from "express-validator";
async function registerRoutes(app) {
  app.use(helmet({
    contentSecurityPolicy: {
      directives: {
        defaultSrc: ["'self'"],
        scriptSrc: ["'self'", "'unsafe-inline'", "'unsafe-eval'"],
        styleSrc: ["'self'", "'unsafe-inline'"],
        imgSrc: ["'self'", "data:", "https:"],
        connectSrc: ["'self'", "https://generativelanguage.googleapis.com"],
        fontSrc: ["'self'", "data:"],
        objectSrc: ["'none'"],
        mediaSrc: ["'self'"],
        frameSrc: ["'none'"]
      }
    },
    crossOriginEmbedderPolicy: false
  }));
  app.use(corsMiddleware);
  app.use(securityHeaders);
  app.use(securityLogger);
  app.use(validateInput);
  app.use(globalRateLimit);
  app.use(sessionSecurity);
  app.use(validateSessionTimeout(120));
  app.use(metricsMiddleware);
  setupAuth(app);
  app.use("/api/admin", admin_default);
  app.get("/api/push/vapid-key", (req, res) => {
    res.json({ publicKey: getVapidPublicKey() });
  });
  app.post("/api/push/subscribe", async (req, res) => {
    if (!req.isAuthenticated()) {
      return res.status(401).json({ error: "Unauthorized" });
    }
    try {
      const { subscription } = req.body;
      await savePushSubscription(req.user.id, subscription);
      res.json({ message: "Subscription saved successfully" });
    } catch (error) {
      console.error("Error saving push subscription:", error);
      res.status(500).json({ error: "Failed to save subscription" });
    }
  });
  app.post("/api/push/unsubscribe", async (req, res) => {
    try {
      const { endpoint } = req.body;
      await removePushSubscription(endpoint);
      res.json({ message: "Subscription removed successfully" });
    } catch (error) {
      console.error("Error removing push subscription:", error);
      res.status(500).json({ error: "Failed to remove subscription" });
    }
  });
  app.get("/api/jobs", async (req, res) => {
    try {
      const { location, experienceLevel, type, postedWithin, search } = req.query;
      const filters = {
        location,
        experienceLevel,
        type,
        postedWithin,
        search
      };
      const jobs = await searchJobs(filters);
      res.json({ jobs, total: jobs.length });
    } catch (error) {
      console.error("Error fetching jobs:", error);
      res.status(500).json({ error: "Failed to fetch jobs" });
    }
  });
  app.get("/api/jobs/stats", async (_req, res) => {
    try {
      const stats = getJobStats();
      res.json(stats);
    } catch (error) {
      console.error("Error fetching job stats:", error);
      res.status(500).json({ error: "Failed to fetch job statistics" });
    }
  });
  app.get("/api/jobs/:id", async (req, res) => {
    try {
      const job = await getJobById(req.params.id);
      if (!job) {
        return res.status(404).json({ error: "Job not found" });
      }
      res.json(job);
    } catch (error) {
      console.error("Error fetching job:", error);
      res.status(500).json({ error: "Failed to fetch job" });
    }
  });
  app.get("/api/interview-questions", (_req, res) => {
    res.json(interviewCategories);
  });
  app.get("/api/interview-questions/:categoryId", (req, res) => {
    const category = interviewCategories.find((c) => c.id === req.params.categoryId);
    if (!category) {
      return res.status(404).json({ error: "Category not found" });
    }
    res.json(category);
  });
  const enhancedRequireAuth = (req, res, next) => {
    if (!req.isAuthenticated() || !req.user) {
      logger.warn("Unauthorized API access attempt", {
        ip: req.ip,
        method: req.method,
        path: req.path,
        userAgent: req.get("User-Agent"),
        referer: req.get("Referer")
      });
      return res.status(401).json({
        error: "Authentication required",
        message: "You must be logged in to access this resource",
        code: "AUTH_REQUIRED"
      });
    }
    logger.info("Authenticated API request", {
      userId: req.user.id,
      username: req.user.username,
      method: req.method,
      path: req.path,
      ip: req.ip
    });
    next();
  };
  const crudApiValidation = [
    body("name").isLength({ min: 1, max: 100 }).withMessage("Name must be between 1 and 100 characters"),
    body("database").isIn(["postgresql", "mysql", "mongodb", "sqlite"]).withMessage("Invalid database type"),
    body("framework").isIn(["express", "fastapi", "nestjs"]).withMessage("Invalid framework"),
    body("authentication").isBoolean().withMessage("Authentication must be boolean"),
    body("oauth").isBoolean().withMessage("OAuth must be boolean")
  ];
  app.post(
    "/api/generate/crud-api",
    apiRateLimit,
    enhancedRequireAuth,
    requireCredits(1),
    crudApiValidation,
    async (req, res) => {
      try {
        const errors = validationResult(req);
        if (!errors.isEmpty()) {
          logger.warn("Invalid input for CRUD API generation", {
            userId: req.user.id,
            errors: errors.array(),
            body: req.body
          });
          return res.status(400).json({
            error: "Validation failed",
            details: errors.array(),
            code: "VALIDATION_ERROR"
          });
        }
        const { name, database, authentication, oauth, framework } = req.body;
        const userId = req.user.id;
        if (!await checkUsageLimit(userId, "api_generation")) {
          return res.status(402).json({
            message: "Free limit reached. Please purchase credits to continue.",
            feature: "api_generation"
          });
        }
        const config = { name, database, authentication, oauth, framework };
        logger.info("API generation started", { name, framework, database }, userId);
        const generatedCode = generateCrudApi(config);
        const project = await storage.createProject({
          userId,
          name,
          type: "api",
          config,
          generatedCode,
          status: "active"
        });
        await deductCreditForUsage(userId, "api_generation");
        logger.apiGeneration(name, framework, true, userId);
        logger.database("insert", "projects", { projectId: project.id, name }, userId);
        res.json({
          project,
          code: generatedCode,
          message: "API generated successfully"
        });
      } catch (error) {
        res.status(500).json({ message: error.message });
      }
    }
  );
  app.post("/api/generate/dockerfile", requireAuth, async (req, res) => {
    try {
      const { language, framework, port, baseImage, envVars } = req.body;
      const userId = req.user.id;
      if (!await checkUsageLimit(userId, "docker_generation")) {
        return res.status(402).json({
          message: "Free limit reached. Please purchase credits to continue.",
          feature: "docker_generation"
        });
      }
      const config = { language, framework, port, baseImage, envVars };
      const dockerfile = generateDockerfile(config);
      const project = await storage.createProject({
        userId,
        name: `${framework}-docker`,
        type: "docker",
        config,
        generatedCode: dockerfile,
        status: "active"
      });
      await deductCreditForUsage(userId, "docker_generation");
      res.json({
        project,
        dockerfile,
        message: "Dockerfile generated successfully"
      });
    } catch (error) {
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/generate/docker-compose", requireAuth, async (req, res) => {
    try {
      const { services } = req.body;
      const userId = req.user.id;
      if (!await checkUsageLimit(userId, "docker_generation")) {
        return res.status(402).json({
          message: "Free limit reached. Please purchase credits to continue.",
          feature: "docker_generation"
        });
      }
      const dockerCompose = generateDockerCompose(services);
      const project = await storage.createProject({
        userId,
        name: "docker-compose-setup",
        type: "docker",
        config: { services },
        generatedCode: dockerCompose,
        status: "active"
      });
      await deductCreditForUsage(userId, "docker_generation");
      res.json({
        project,
        dockerCompose,
        message: "Docker Compose generated successfully"
      });
    } catch (error) {
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/generate/github-actions", requireAuth, async (req, res) => {
    try {
      const { language, framework, testCommand } = req.body;
      const userId = req.user.id;
      if (!await checkUsageLimit(userId, "cicd_generation")) {
        return res.status(402).json({
          message: "Free limit reached. Please purchase credits to continue.",
          feature: "cicd_generation"
        });
      }
      const config = { language, framework, testCommand };
      const workflow = generateGitHubActions(config);
      const project = await storage.createProject({
        userId,
        name: `${framework}-ci-cd`,
        type: "ci-cd",
        config,
        generatedCode: workflow,
        status: "active"
      });
      await deductCreditForUsage(userId, "cicd_generation");
      res.json({
        project,
        workflow,
        message: "GitHub Actions workflow generated successfully"
      });
    } catch (error) {
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/ai/analyze-logs", requireAuth, async (req, res) => {
    try {
      const { logText } = req.body;
      const userId = req.user.id;
      if (!await checkUsageLimit(userId, "ai_assistance")) {
        return res.status(402).json({
          message: "Free limit reached. Please purchase credits to continue.",
          feature: "ai_assistance"
        });
      }
      const analysis = await analyzeLogError(logText);
      await deductCreditForUsage(userId, "ai_assistance");
      res.json(analysis);
    } catch (error) {
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/ai/generate-yaml", requireAuth, async (req, res) => {
    try {
      const { description } = req.body;
      const userId = req.user.id;
      if (!await checkUsageLimit(userId, "ai_assistance")) {
        return res.status(402).json({
          message: "Free limit reached. Please purchase credits to continue.",
          feature: "ai_assistance"
        });
      }
      const result = await generateYamlFromNaturalLanguage(description);
      await deductCreditForUsage(userId, "ai_assistance");
      res.json(result);
    } catch (error) {
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/ai/optimize-dockerfile", requireAuth, async (req, res) => {
    try {
      const { dockerfile } = req.body;
      const userId = req.user.id;
      if (!await checkUsageLimit(userId, "ai_assistance")) {
        return res.status(402).json({
          message: "Free limit reached. Please purchase credits to continue.",
          feature: "ai_assistance"
        });
      }
      const result = await optimizeDockerfile(dockerfile);
      await deductCreditForUsage(userId, "ai_assistance");
      res.json(result);
    } catch (error) {
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/ai/devops-query", requireAuth, async (req, res) => {
    try {
      const { query } = req.body;
      const userId = req.user.id;
      if (!query || typeof query !== "string") {
        return res.status(400).json({ message: "Query is required" });
      }
      const activeSubscription = await storage.getActiveSubscription(userId);
      const isPremium = activeSubscription || req.user.isPremium;
      if (!isPremium) {
        if (!await checkUsageLimit(userId, "ai_assistance")) {
          return res.status(402).json({
            message: "Free limit reached. Please purchase credits or subscribe to AI Pro for unlimited access.",
            feature: "ai_assistance",
            isPremiumFeature: true
          });
        }
      }
      const answer = await generateDevOpsResponse(query);
      if (!isPremium) {
        await deductCreditForUsage(userId, "ai_assistance");
      }
      logger.info(`DevOps AI query processed for user ${userId} (premium: ${isPremium}): ${query.substring(0, 50)}...`);
      res.json({
        answer,
        query,
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        isPremium
      });
    } catch (error) {
      logger.error("DevOps AI query error", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.get("/api/subscription", requireAuth, async (req, res) => {
    try {
      const userId = req.user.id;
      const subscription = await storage.getActiveSubscription(userId);
      res.json({
        hasActiveSubscription: !!subscription,
        subscription: subscription || null,
        isPremium: req.user.isPremium
      });
    } catch (error) {
      logger.error("Error fetching subscription", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/subscription/create", requireAuth, async (req, res) => {
    try {
      const { planType } = req.body;
      const userId = req.user.id;
      if (!planType || !["ai-pro-monthly", "ai-pro-annual"].includes(planType)) {
        return res.status(400).json({ message: "Invalid plan type" });
      }
      const now = /* @__PURE__ */ new Date();
      let endDate;
      let amount;
      if (planType === "ai-pro-monthly") {
        endDate = new Date(now.getTime() + 30 * 24 * 60 * 60 * 1e3);
        amount = 19900;
      } else {
        endDate = new Date(now.getTime() + 365 * 24 * 60 * 60 * 1e3);
        amount = 199900;
      }
      const subscription = await storage.createSubscription({
        userId,
        planType,
        status: "active",
        razorpaySubscriptionId: null,
        startDate: now,
        endDate,
        autoRenew: true
      });
      logger.info(`Subscription created for user ${userId}: ${planType}`);
      res.json({
        success: true,
        subscription,
        message: `Successfully subscribed to ${planType === "ai-pro-monthly" ? "AI Pro Monthly" : "AI Pro Annual"}`
      });
    } catch (error) {
      logger.error("Error creating subscription", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/subscription/cancel", requireAuth, async (req, res) => {
    try {
      const userId = req.user.id;
      const subscription = await storage.getActiveSubscription(userId);
      if (!subscription) {
        return res.status(404).json({ message: "No active subscription found" });
      }
      const cancelled = await storage.cancelSubscription(subscription.id);
      logger.info(`Subscription cancelled for user ${userId}`);
      res.json({
        success: true,
        subscription: cancelled,
        message: "Subscription cancelled. You will still have access until the end of your billing period."
      });
    } catch (error) {
      logger.error("Error cancelling subscription", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/user/tour-complete", requireAuth, async (req, res) => {
    try {
      const userId = req.user.id;
      const updatedUser = await storage.updateUserTourStatus(userId, true);
      logger.info(`Tour completed for user ${userId}`);
      res.json({
        success: true,
        hasSeenTour: updatedUser.hasSeenTour
      });
    } catch (error) {
      logger.error("Error updating tour status", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.patch("/api/user/domain", requireAuth, async (req, res) => {
    try {
      const userId = req.user.id;
      const { domain } = req.body;
      const validDomains = ["devops", "data-engineering", "cybersecurity"];
      if (!domain || !validDomains.includes(domain)) {
        return res.status(400).json({ message: "Invalid domain. Must be one of: devops, data-engineering, cybersecurity" });
      }
      const updatedUser = await storage.updateUserDomain(userId, domain);
      logger.info(`Domain updated to ${domain} for user ${userId}`);
      res.json({
        success: true,
        primaryDomain: updatedUser.primaryDomain
      });
    } catch (error) {
      logger.error("Error updating user domain", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.get("/api/domain-configs", async (_req, res) => {
    try {
      const configs = await storage.getDomainConfigs();
      res.json(configs);
    } catch (error) {
      res.status(500).json({ message: error.message });
    }
  });
  app.get("/api/site-settings", async (_req, res) => {
    try {
      const settings = await storage.getSiteSettings();
      res.json(settings);
    } catch (error) {
      res.status(500).json({ message: error.message });
    }
  });
  app.put("/api/admin/site-settings/:key", requireAuth, async (req, res) => {
    try {
      const user = req.user;
      if (!user?.isAdmin) {
        return res.status(403).json({ message: "Admin access required" });
      }
      const { key } = req.params;
      const { value } = req.body;
      if (typeof value !== "boolean") {
        return res.status(400).json({ message: "Value must be a boolean" });
      }
      const updated = await storage.updateSiteSetting(key, value, user.id);
      res.json(updated);
    } catch (error) {
      res.status(500).json({ message: error.message });
    }
  });
  app.get("/api/incidents", requireAuth, async (req, res) => {
    try {
      const userId = req.user.id;
      const incidents2 = await storage.getIncidentsByUser(userId);
      res.json(incidents2);
    } catch (error) {
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/incidents", requireAuth, async (req, res) => {
    try {
      const userId = req.user.id;
      const { title, description, category, priority } = req.body;
      if (!title || !description || !category) {
        return res.status(400).json({ message: "Title, description, and category are required" });
      }
      const incident = await storage.createIncident(userId, {
        title,
        description,
        category,
        priority: priority || "medium"
      });
      res.json(incident);
    } catch (error) {
      res.status(500).json({ message: error.message });
    }
  });
  app.get("/api/incidents/:id", requireAuth, async (req, res) => {
    try {
      const userId = req.user.id;
      const id = parseInt(req.params.id);
      const incident = await storage.getIncident(id);
      if (!incident || incident.userId !== userId) {
        return res.status(404).json({ message: "Incident not found" });
      }
      const messages = await storage.getIncidentMessages(id);
      res.json({ incident, messages });
    } catch (error) {
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/incidents/:id/message", requireAuth, async (req, res) => {
    try {
      const userId = req.user.id;
      const id = parseInt(req.params.id);
      const { message } = req.body;
      if (!message) {
        return res.status(400).json({ message: "Message is required" });
      }
      const incident = await storage.getIncident(id);
      if (!incident || incident.userId !== userId) {
        return res.status(404).json({ message: "Incident not found" });
      }
      const msg = await storage.addIncidentMessage(id, userId, "user", message);
      res.json(msg);
    } catch (error) {
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/incidents/:id/reopen", requireAuth, async (req, res) => {
    try {
      const userId = req.user.id;
      const id = parseInt(req.params.id);
      const incident = await storage.getIncident(id);
      if (!incident || incident.userId !== userId) {
        return res.status(404).json({ message: "Incident not found" });
      }
      const updated = await storage.updateIncidentStatus(id, "reopened");
      res.json(updated);
    } catch (error) {
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/ai/migration", requireAuth, async (req, res) => {
    try {
      const { type, content } = req.body;
      const userId = req.user.id;
      if (!type || !content) {
        return res.status(400).json({ message: "Type and content are required" });
      }
      if (!await checkUsageLimit(userId, "migration_assistant")) {
        return res.status(402).json({
          message: "Free limit reached. Please purchase credits to continue.",
          feature: "migration_assistant"
        });
      }
      let prompt = "";
      if (type === "dockerize") {
        prompt = `You are an expert DevOps engineer. Convert the following application description or structure into a production-ready Dockerfile and docker-compose.yml. Include best practices like multi-stage builds, security hardening, and proper caching. 

Application:
${content}

Provide the Dockerfile first, then docker-compose.yml. Add helpful comments explaining each section.`;
      } else if (type === "vm-to-container") {
        prompt = `You are an expert DevOps engineer. Convert the following VM configuration into containerized workloads. Create appropriate Dockerfiles, docker-compose.yml, and any necessary configuration files. Consider service dependencies, volumes, networking, and security.

VM Configuration:
${content}

Provide a complete containerization strategy with all necessary files.`;
      } else if (type === "compose-to-k8s") {
        prompt = `You are an expert Kubernetes engineer. Convert the following Docker Compose file into Kubernetes manifests. Include Deployments, Services, ConfigMaps, Secrets, PersistentVolumeClaims, and any other necessary resources. Follow Kubernetes best practices.

Docker Compose:
${content}

Provide complete Kubernetes YAML manifests that can be applied directly.`;
      } else {
        return res.status(400).json({ message: "Invalid migration type" });
      }
      const genAI2 = new GoogleGenerativeAI2(process.env.GOOGLE_API_KEY || process.env.OPENAI_API_KEY || "");
      const model = genAI2.getGenerativeModel({ model: "gemini-1.5-flash" });
      const result = await model.generateContent(prompt);
      const response = await result.response;
      const text2 = response.text();
      await deductCreditForUsage(userId, "migration_assistant");
      logger.info(`Migration assistant used by user ${userId}: ${type}`);
      res.json({
        result: text2,
        type,
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      });
    } catch (error) {
      logger.error("Migration assistant error", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/ai/deployment-simulate", requireAuth, async (req, res) => {
    try {
      const { config, environment, provider, url } = req.body;
      const userId = req.user.id;
      if (!config || typeof config !== "string" || config.trim().length < 10) {
        return res.status(400).json({ message: "Deployment configuration is required (minimum 10 characters)" });
      }
      const validEnvironments = ["development", "staging", "production", "qa", "test"];
      const validProviders = ["aws", "gcp", "azure", "digitalocean", "kubernetes"];
      const safeEnvironment = validEnvironments.includes(environment) ? environment : "production";
      const safeProvider = validProviders.includes(provider) ? provider : "aws";
      if (!await checkUsageLimit(userId, "deployment_simulator")) {
        return res.status(402).json({ message: "Free limit reached", feature: "deployment_simulator" });
      }
      const prompt = `You are an expert DevOps engineer and deployment specialist. Analyze this deployment configuration and predict potential issues.

Deployment Configuration:
${config}

Target Environment: ${safeEnvironment}
Cloud Provider: ${safeProvider}
${url ? `Application URL: ${url}` : ""}

Provide a comprehensive deployment simulation analysis in the following JSON format:
{
  "failures": [{"risk": "description", "severity": "critical|high|medium|low", "recommendation": "how to fix"}],
  "costImpact": {"current": "$X/month", "projected": "$Y/month", "change": "+/-$Z", "details": "explanation"},
  "scalingNeeds": {"recommendation": "text", "metrics": ["CPU", "Memory", "etc"], "autoscaling": "specific rule suggestion"},
  "securityIssues": [{"issue": "description", "severity": "critical|high|medium|low", "fix": "how to fix"}],
  "overallScore": 0-100,
  "deploymentReady": true|false,
  "summary": "brief summary"
}

Be thorough and realistic in your analysis.`;
      const genAI2 = new GoogleGenerativeAI2(process.env.GOOGLE_API_KEY || process.env.OPENAI_API_KEY || "");
      const model = genAI2.getGenerativeModel({ model: "gemini-1.5-flash" });
      const result = await model.generateContent(prompt);
      const response = await result.response;
      const text2 = response.text();
      let parsed;
      try {
        const jsonMatch = text2.match(/\{[\s\S]*\}/);
        parsed = jsonMatch ? JSON.parse(jsonMatch[0]) : { summary: text2 };
      } catch {
        parsed = { summary: text2, overallScore: 75, deploymentReady: true, failures: [], securityIssues: [], costImpact: { current: "N/A", projected: "N/A", change: "N/A", details: text2 }, scalingNeeds: { recommendation: text2, metrics: [], autoscaling: "" } };
      }
      await deductCreditForUsage(userId, "deployment_simulator");
      logger.info(`Deployment simulation by user ${userId}`);
      res.json({ result: parsed });
    } catch (error) {
      logger.error("Deployment simulator error", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/ai/iac-analyze", requireAuth, async (req, res) => {
    try {
      const { code, type, autoFix } = req.body;
      const userId = req.user.id;
      if (!code || typeof code !== "string" || code.trim().length < 10) {
        return res.status(400).json({ message: "Infrastructure code is required (minimum 10 characters)" });
      }
      const validTypes = ["terraform", "pulumi", "cloudformation", "arm", "bicep"];
      const safeType = validTypes.includes(type) ? type : "terraform";
      if (!await checkUsageLimit(userId, "iac_autofix")) {
        return res.status(402).json({ message: "Free limit reached", feature: "iac_autofix" });
      }
      const prompt = `You are an expert Infrastructure-as-Code specialist. Analyze this ${safeType} code for issues.

Code:
${code}

Analyze for:
1. Drift detection hints
2. Syntax errors
3. Security vulnerabilities
4. Missing tags/policies
5. Unused resources
${autoFix ? "6. Provide corrected code" : ""}

Return JSON:
{
  "issues": [{"type": "drift|syntax|security|missing_tags|unused", "severity": "critical|high|medium|low", "line": number, "resource": "name", "description": "issue", "fix": "suggestion"}],
  "fixedCode": "corrected code if autoFix",
  "summary": "brief summary",
  "stats": {"totalIssues": N, "critical": N, "high": N, "medium": N, "low": N}
}`;
      const genAI2 = new GoogleGenerativeAI2(process.env.GOOGLE_API_KEY || process.env.OPENAI_API_KEY || "");
      const model = genAI2.getGenerativeModel({ model: "gemini-1.5-flash" });
      const result = await model.generateContent(prompt);
      const response = await result.response;
      const text2 = response.text();
      let parsed;
      try {
        const jsonMatch = text2.match(/\{[\s\S]*\}/);
        parsed = jsonMatch ? JSON.parse(jsonMatch[0]) : { summary: text2, issues: [], stats: { totalIssues: 0, critical: 0, high: 0, medium: 0, low: 0 } };
      } catch {
        parsed = { summary: text2, issues: [], fixedCode: code, stats: { totalIssues: 0, critical: 0, high: 0, medium: 0, low: 0 } };
      }
      await deductCreditForUsage(userId, "iac_autofix");
      logger.info(`IaC analysis by user ${userId}`);
      res.json({ result: parsed });
    } catch (error) {
      logger.error("IaC autofix error", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/ai/release-notes", requireAuth, async (req, res) => {
    try {
      const { commits, currentVersion, projectName, format } = req.body;
      const userId = req.user.id;
      if (!commits || typeof commits !== "string" || commits.trim().length < 10) {
        return res.status(400).json({ message: "Commits are required (minimum 10 characters)" });
      }
      const validFormats = ["markdown", "html", "plain", "json"];
      const safeFormat = validFormats.includes(format) ? format : "markdown";
      const safeProjectName = typeof projectName === "string" ? projectName.slice(0, 100) : "Project";
      const safeVersion = typeof currentVersion === "string" && /^\d+\.\d+\.\d+/.test(currentVersion) ? currentVersion : "1.0.0";
      if (!await checkUsageLimit(userId, "release_notes")) {
        return res.status(402).json({ message: "Free limit reached", feature: "release_notes" });
      }
      const prompt = `You are a technical writer. Generate release notes from these commits.

Project: ${safeProjectName}
Current Version: ${safeVersion}
Format: ${safeFormat}

Commits:
${commits}

Return JSON:
{
  "version": "suggested new version following semver",
  "versionType": "major|minor|patch",
  "releaseNotes": "formatted release notes",
  "changelog": "changelog entry",
  "impactSummary": "brief impact summary",
  "breakingChanges": ["list of breaking changes"],
  "newFeatures": ["list of new features"],
  "bugFixes": ["list of bug fixes"],
  "improvements": ["list of improvements"]
}`;
      const genAI2 = new GoogleGenerativeAI2(process.env.GOOGLE_API_KEY || process.env.OPENAI_API_KEY || "");
      const model = genAI2.getGenerativeModel({ model: "gemini-1.5-flash" });
      const result = await model.generateContent(prompt);
      const response = await result.response;
      const text2 = response.text();
      let parsed;
      try {
        const jsonMatch = text2.match(/\{[\s\S]*\}/);
        parsed = jsonMatch ? JSON.parse(jsonMatch[0]) : { releaseNotes: text2, version: currentVersion };
      } catch {
        parsed = { releaseNotes: text2, changelog: text2, version: currentVersion || "1.0.1", versionType: "patch", breakingChanges: [], newFeatures: [], bugFixes: [], improvements: [], impactSummary: text2 };
      }
      await deductCreditForUsage(userId, "release_notes");
      logger.info(`Release notes generated by user ${userId}`);
      res.json({ result: parsed });
    } catch (error) {
      logger.error("Release notes error", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/ai/secret-scan", requireAuth, async (req, res) => {
    try {
      const { code, repoUrl, autoRemediate } = req.body;
      const userId = req.user.id;
      const hasCode = code && typeof code === "string" && code.trim().length >= 10;
      const hasValidUrl = repoUrl && typeof repoUrl === "string" && /^https?:\/\/[^\s]+$/.test(repoUrl);
      if (!hasCode && !hasValidUrl) {
        return res.status(400).json({ message: "Valid code (minimum 10 characters) or repository URL is required" });
      }
      if (!await checkUsageLimit(userId, "secret_scanner")) {
        return res.status(402).json({ message: "Free limit reached", feature: "secret_scanner" });
      }
      const prompt = `You are a security expert. Scan this code for exposed secrets and security issues.

${code ? `Code:
${code}` : `Repository: ${repoUrl}`}

Detect:
1. AWS keys, API tokens, passwords
2. Hardcoded credentials
3. Misconfigured IAM policies
4. Over-permissioned roles
5. Exposed certificates

Return JSON:
{
  "findings": [{"type": "aws_key|api_key|password|token|certificate|iam_issue|over_permission", "severity": "critical|high|medium|low", "location": "file:line", "line": number, "description": "issue", "exposed": "partial masked value", "remediation": "how to fix", "autoFixed": boolean}],
  "summary": "brief summary",
  "stats": {"total": N, "critical": N, "high": N, "medium": N, "low": N, "autoFixed": N},
  "securityScore": 0-100
}`;
      const genAI2 = new GoogleGenerativeAI2(process.env.GOOGLE_API_KEY || process.env.OPENAI_API_KEY || "");
      const model = genAI2.getGenerativeModel({ model: "gemini-1.5-flash" });
      const result = await model.generateContent(prompt);
      const response = await result.response;
      const text2 = response.text();
      let parsed;
      try {
        const jsonMatch = text2.match(/\{[\s\S]*\}/);
        parsed = jsonMatch ? JSON.parse(jsonMatch[0]) : { findings: [], securityScore: 100, summary: "No issues found" };
      } catch {
        parsed = { findings: [], summary: text2, stats: { total: 0, critical: 0, high: 0, medium: 0, low: 0, autoFixed: 0 }, securityScore: 85 };
      }
      await deductCreditForUsage(userId, "secret_scanner");
      logger.info(`Secret scan by user ${userId}`);
      res.json({ result: parsed });
    } catch (error) {
      logger.error("Secret scanner error", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/ai/cloud-optimize", requireAuth, async (req, res) => {
    try {
      const { config, currentSpend, provider } = req.body;
      const userId = req.user.id;
      if (!config || typeof config !== "string" || config.trim().length < 10) {
        return res.status(400).json({ message: "Cloud configuration is required (minimum 10 characters)" });
      }
      const validProviders = ["aws", "gcp", "azure", "multi-cloud"];
      const safeProvider = validProviders.includes(provider) ? provider : "aws";
      const safeSpend = typeof currentSpend === "string" ? currentSpend.slice(0, 50) : "Unknown";
      if (!await checkUsageLimit(userId, "cloud_optimizer")) {
        return res.status(402).json({ message: "Free limit reached", feature: "cloud_optimizer" });
      }
      const prompt = `You are a cloud cost optimization expert. Analyze this infrastructure and suggest cost savings.

Cloud Provider: ${safeProvider}
Current Monthly Spend: ${safeSpend}

Infrastructure:
${config}

Provide optimization recommendations in JSON:
{
  "currentMonthlySpend": "$X,XXX",
  "projectedMonthlySpend": "$X,XXX",
  "totalSavings": "$XXX",
  "savingsPercentage": N,
  "recommendations": [{"category": "name", "currentCost": "$X", "projectedSaving": "$X", "savingPercentage": N, "recommendation": "text", "implementation": "steps", "priority": "high|medium|low", "effort": "easy|medium|hard"}],
  "instanceOptimizations": [{"current": "type", "recommended": "type", "monthlySaving": "$X", "reason": "why"}],
  "spotInstanceOpportunities": [{"workload": "name", "currentCost": "$X", "spotCost": "$X", "saving": "$X"}],
  "autoscalingRules": [{"resource": "name", "rule": "specific rule", "expectedImpact": "savings description"}],
  "summary": "brief summary"
}`;
      const genAI2 = new GoogleGenerativeAI2(process.env.GOOGLE_API_KEY || process.env.OPENAI_API_KEY || "");
      const model = genAI2.getGenerativeModel({ model: "gemini-1.5-flash" });
      const result = await model.generateContent(prompt);
      const response = await result.response;
      const text2 = response.text();
      let parsed;
      try {
        const jsonMatch = text2.match(/\{[\s\S]*\}/);
        parsed = jsonMatch ? JSON.parse(jsonMatch[0]) : { summary: text2 };
      } catch {
        parsed = { summary: text2, currentMonthlySpend: currentSpend || "$5,000", projectedMonthlySpend: "$4,000", totalSavings: "$1,000", savingsPercentage: 20, recommendations: [], instanceOptimizations: [], spotInstanceOpportunities: [], autoscalingRules: [] };
      }
      await deductCreditForUsage(userId, "cloud_optimizer");
      logger.info(`Cloud optimization by user ${userId}`);
      res.json({ result: parsed });
    } catch (error) {
      logger.error("Cloud optimizer error", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/ai/infra-chat", requireAuth, async (req, res) => {
    try {
      const { query } = req.body;
      const userId = req.user.id;
      if (!query || typeof query !== "string" || query.trim().length < 3) {
        return res.status(400).json({ message: "Query is required (minimum 3 characters)" });
      }
      const safeQuery = query.slice(0, 2e3);
      if (!await checkUsageLimit(userId, "infra_chat")) {
        return res.status(402).json({ message: "Free limit reached", feature: "infra_chat" });
      }
      const prompt = `You are an intelligent infrastructure assistant that can help manage Kubernetes, cloud resources, and DevOps tasks.

User Query: ${safeQuery}

Respond naturally and helpfully. If the query involves an action, provide:
1. A clear explanation of what you'll do
2. The exact command(s) that would execute this action
3. Expected results

Return JSON:
{
  "response": "natural language response to the user",
  "action": {
    "type": "scale|restart|status|logs|metrics|deploy|rollback|none",
    "command": "kubectl or cloud CLI command if applicable",
    "result": "expected outcome description"
  }
}

Be helpful, concise, and practical.`;
      const genAI2 = new GoogleGenerativeAI2(process.env.GOOGLE_API_KEY || process.env.OPENAI_API_KEY || "");
      const model = genAI2.getGenerativeModel({ model: "gemini-1.5-flash" });
      const result = await model.generateContent(prompt);
      const response = await result.response;
      const text2 = response.text();
      let parsed;
      try {
        const jsonMatch = text2.match(/\{[\s\S]*\}/);
        parsed = jsonMatch ? JSON.parse(jsonMatch[0]) : { response: text2 };
      } catch {
        parsed = { response: text2, action: null };
      }
      await deductCreditForUsage(userId, "infra_chat");
      logger.info(`Infra chat by user ${userId}`);
      res.json({ result: parsed });
    } catch (error) {
      logger.error("Infra chat error", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/ai/blueprint", requireAuth, async (req, res) => {
    try {
      const { requirements, projectType, cloudProvider, scale } = req.body;
      const userId = req.user.id;
      if (!requirements || typeof requirements !== "string" || requirements.trim().length < 20) {
        return res.status(400).json({ message: "Project requirements are required (minimum 20 characters)" });
      }
      const validProjectTypes = ["web-app", "api", "ecommerce", "saas", "mobile-backend", "data-pipeline", "ml-platform"];
      const validProviders = ["aws", "gcp", "azure", "multi-cloud"];
      const validScales = ["small", "medium", "large", "enterprise"];
      const safeProjectType = validProjectTypes.includes(projectType) ? projectType : "web-app";
      const safeCloudProvider = validProviders.includes(cloudProvider) ? cloudProvider : "aws";
      const safeScale = validScales.includes(scale) ? scale : "medium";
      if (!await checkUsageLimit(userId, "blueprint_generator")) {
        return res.status(402).json({ message: "Free limit reached", feature: "blueprint_generator" });
      }
      const prompt = `You are a senior solutions architect. Generate a complete architecture blueprint.

Project Requirements:
${requirements}

Project Type: ${safeProjectType}
Cloud Provider: ${safeCloudProvider}
Scale: ${safeScale}

Generate a comprehensive architecture in JSON:
{
  "projectName": "suggested name",
  "architecture": {
    "overview": "high-level description",
    "diagram": "ASCII art architecture diagram",
    "components": [{"name": "Service Name", "type": "compute|database|storage|network|cache", "description": "purpose"}]
  },
  "infrastructure": {
    "resources": "description of infrastructure",
    "terraform": "basic Terraform code for main resources",
    "estimatedCost": "$X,XXX/month"
  },
  "cicd": {
    "pipeline": "GitHub Actions or similar YAML",
    "stages": ["Build", "Test", "Deploy", etc]
  },
  "security": {
    "recommendations": ["list of security practices"],
    "implementation": "security configuration code"
  },
  "deployment": {
    "steps": ["step-by-step deployment guide"],
    "timeline": "estimated implementation time"
  },
  "summary": "executive summary"
}`;
      const genAI2 = new GoogleGenerativeAI2(process.env.GOOGLE_API_KEY || process.env.OPENAI_API_KEY || "");
      const model = genAI2.getGenerativeModel({ model: "gemini-1.5-flash" });
      const result = await model.generateContent(prompt);
      const response = await result.response;
      const text2 = response.text();
      let parsed;
      try {
        const jsonMatch = text2.match(/\{[\s\S]*\}/);
        parsed = jsonMatch ? JSON.parse(jsonMatch[0]) : { summary: text2, projectName: "New Project" };
      } catch {
        parsed = {
          projectName: "Architecture Blueprint",
          summary: text2,
          architecture: { overview: text2, diagram: "", components: [] },
          infrastructure: { resources: "", terraform: "", estimatedCost: "TBD" },
          cicd: { pipeline: "", stages: [] },
          security: { recommendations: [], implementation: "" },
          deployment: { steps: [], timeline: "TBD" }
        };
      }
      await deductCreditForUsage(userId, "blueprint_generator");
      logger.info(`Blueprint generated by user ${userId}`);
      res.json({ result: parsed });
    } catch (error) {
      logger.error("Blueprint generator error", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/ai/postmortem", requireAuth, async (req, res) => {
    try {
      const { description, url, logs, severity } = req.body;
      const userId = req.user.id;
      if (!description || typeof description !== "string" || description.trim().length < 20) {
        return res.status(400).json({ message: "Incident description is required (minimum 20 characters)" });
      }
      const validSeverities = ["critical", "high", "medium", "low"];
      const safeSeverity = validSeverities.includes(severity) ? severity : "high";
      const safeUrl = url && typeof url === "string" && /^https?:\/\//.test(url) ? url : "";
      const safeLogs = typeof logs === "string" ? logs.slice(0, 1e4) : "";
      if (!await checkUsageLimit(userId, "postmortem_generator")) {
        return res.status(402).json({ message: "Free limit reached", feature: "postmortem_generator" });
      }
      const prompt = `You are an expert in incident management. Generate a comprehensive post-mortem report.

Incident Description:
${description}

${safeUrl ? `Incident URL: ${safeUrl}` : ""}
Severity: ${safeSeverity}
${safeLogs ? `
Logs:
${safeLogs}` : ""}

Generate a detailed post-mortem in JSON:
{
  "incidentTitle": "brief title",
  "severity": "critical|high|medium|low",
  "summary": "executive summary",
  "timeline": [{"time": "HH:MM UTC", "event": "what happened", "type": "detection|response|mitigation|resolution"}],
  "whatHappened": "detailed description",
  "rootCause": "root cause analysis",
  "impact": {
    "users": "X users/Y% of traffic",
    "duration": "X hours Y minutes",
    "services": ["affected services"],
    "financial": "estimated impact if any"
  },
  "teamInvolvement": [{"team": "Team Name", "role": "what they did"}],
  "whatWentWell": ["positive observations"],
  "whatWentWrong": ["areas for improvement"],
  "preventiveMeasures": [{"action": "specific action", "owner": "team/person", "priority": "high|medium|low", "deadline": "date"}],
  "lessonsLearned": ["key takeaways"],
  "fullReport": "complete markdown formatted report"
}`;
      const genAI2 = new GoogleGenerativeAI2(process.env.GOOGLE_API_KEY || process.env.OPENAI_API_KEY || "");
      const model = genAI2.getGenerativeModel({ model: "gemini-1.5-flash" });
      const result = await model.generateContent(prompt);
      const response = await result.response;
      const text2 = response.text();
      let parsed;
      try {
        const jsonMatch = text2.match(/\{[\s\S]*\}/);
        parsed = jsonMatch ? JSON.parse(jsonMatch[0]) : { summary: text2, incidentTitle: "Incident Report" };
      } catch {
        parsed = {
          incidentTitle: "Incident Report",
          severity: severity || "high",
          summary: text2,
          timeline: [],
          whatHappened: text2,
          rootCause: "Analysis pending",
          impact: { users: "Unknown", duration: "Unknown", services: [], financial: "Unknown" },
          teamInvolvement: [],
          whatWentWell: [],
          whatWentWrong: [],
          preventiveMeasures: [],
          lessonsLearned: [],
          fullReport: text2
        };
      }
      await deductCreditForUsage(userId, "postmortem_generator");
      logger.info(`Post-mortem generated by user ${userId}`);
      res.json({ result: parsed });
    } catch (error) {
      logger.error("Post-mortem generator error", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/ai/env-replicate", requireAuth, async (req, res) => {
    try {
      const { repoUrl, branch, includeTests, includeSeedData } = req.body;
      const userId = req.user.id;
      if (!repoUrl || typeof repoUrl !== "string" || !repoUrl.startsWith("https://github.com/")) {
        return res.status(400).json({ message: "Valid GitHub repository URL is required" });
      }
      if (!await checkUsageLimit(userId, "env_replicator")) {
        return res.status(402).json({ message: "Free limit reached", feature: "env_replicator" });
      }
      const safeBranch = typeof branch === "string" ? branch.slice(0, 50) : "main";
      const prompt = `You are an expert DevOps engineer. Analyze this GitHub repository and generate a complete development environment setup.

Repository URL: ${repoUrl}
Branch: ${safeBranch}
Include Tests: ${includeTests ? "Yes" : "No"}
Include Seed Data: ${includeSeedData ? "Yes" : "No"}

Based on analyzing typical repo structures for this URL, infer the tech stack and generate:
1. Docker Compose file with all services (db, cache, app)
2. Dockerfile for the application
3. Environment variables template
4. Database migrations (if applicable)
5. Seed data scripts (if requested)
6. Setup script to bootstrap everything

Return JSON:
{
  "projectName": "name from repo",
  "techStack": [{"language": "Node.js", "framework": "Express", "version": "18"}],
  "osPackages": ["git", "curl", "build-essential"],
  "dockerCompose": "docker-compose.yml content",
  "dockerfile": "Dockerfile content",
  "envFile": ".env.example content",
  "migrations": "SQL migrations if applicable",
  "seedData": "seed data SQL if requested",
  "setupScript": "bash setup script",
  "readme": "DEV_SETUP.md content",
  "summary": "brief description"
}`;
      const genAI2 = new GoogleGenerativeAI2(process.env.GOOGLE_API_KEY || process.env.OPENAI_API_KEY || "");
      const model = genAI2.getGenerativeModel({ model: "gemini-1.5-flash" });
      const result = await model.generateContent(prompt);
      const response = await result.response;
      const text2 = response.text();
      let parsed;
      try {
        const jsonMatch = text2.match(/\{[\s\S]*\}/);
        parsed = jsonMatch ? JSON.parse(jsonMatch[0]) : { summary: text2, projectName: "Project" };
      } catch {
        parsed = {
          projectName: "Project",
          techStack: [],
          osPackages: [],
          dockerCompose: "",
          dockerfile: "",
          envFile: "",
          migrations: "",
          seedData: "",
          setupScript: "",
          readme: text2,
          summary: text2
        };
      }
      await deductCreditForUsage(userId, "env_replicator");
      logger.info(`Environment replicated by user ${userId}: ${repoUrl}`);
      res.json({ result: parsed });
    } catch (error) {
      logger.error("Environment replicator error", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/ai/db-optimize", requireAuth, async (req, res) => {
    try {
      const { slowQueries, schemaDDL, engine } = req.body;
      const userId = req.user.id;
      if ((!slowQueries || typeof slowQueries !== "string" || slowQueries.trim().length < 10) && (!schemaDDL || typeof schemaDDL !== "string" || schemaDDL.trim().length < 10)) {
        return res.status(400).json({ message: "Slow queries or schema DDL required (minimum 10 characters)" });
      }
      const validEngines = ["postgresql", "mysql", "mariadb", "mssql", "oracle"];
      const safeEngine = validEngines.includes(engine) ? engine : "postgresql";
      if (!await checkUsageLimit(userId, "db_optimizer")) {
        return res.status(402).json({ message: "Free limit reached", feature: "db_optimizer" });
      }
      const prompt = `You are an expert DBA (Database Administrator). Analyze these ${safeEngine} queries and schema for optimization.

${slowQueries ? `Slow Queries:
${slowQueries}` : ""}
${schemaDDL ? `
Schema DDL:
${schemaDDL}` : ""}

Provide comprehensive database optimization analysis in JSON:
{
  "summary": "brief summary",
  "overallScore": 0-100,
  "slowQueries": [{"query": "original", "executionTime": "5s", "issue": "problem", "optimizedQuery": "fixed query", "improvement": "10x faster"}],
  "indexRecommendations": [{"table": "name", "columns": ["col1", "col2"], "type": "btree|hash|gin", "createStatement": "CREATE INDEX...", "reason": "why needed", "estimatedImprovement": "50% faster"}],
  "configOptimizations": [{"parameter": "shared_buffers", "currentValue": "128MB", "recommendedValue": "256MB", "reason": "explanation", "impact": "high|medium|low"}],
  "antiPatterns": [{"pattern": "SELECT *", "location": "query 1", "severity": "critical|high|medium|low", "fix": "how to fix"}],
  "queryBottlenecks": [{"query": "problematic query", "bottleneck": "issue", "prediction": "will get worse", "mitigation": "fix"}]
}`;
      const genAI2 = new GoogleGenerativeAI2(process.env.GOOGLE_API_KEY || process.env.OPENAI_API_KEY || "");
      const model = genAI2.getGenerativeModel({ model: "gemini-1.5-flash" });
      const result = await model.generateContent(prompt);
      const response = await result.response;
      const text2 = response.text();
      let parsed;
      try {
        const jsonMatch = text2.match(/\{[\s\S]*\}/);
        parsed = jsonMatch ? JSON.parse(jsonMatch[0]) : { summary: text2, overallScore: 70 };
      } catch {
        parsed = {
          summary: text2,
          overallScore: 70,
          slowQueries: [],
          indexRecommendations: [],
          configOptimizations: [],
          antiPatterns: [],
          queryBottlenecks: []
        };
      }
      await deductCreditForUsage(userId, "db_optimizer");
      logger.info(`Database optimization by user ${userId}`);
      res.json({ result: parsed });
    } catch (error) {
      logger.error("Database optimizer error", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/ai/website-analyze", requireAuth, async (req, res) => {
    try {
      const { url, checks } = req.body;
      const userId = req.user.id;
      if (!url || typeof url !== "string" || url.length > 500) {
        return res.status(400).json({ message: "Valid website URL is required (max 500 characters)" });
      }
      try {
        const parsedUrl = new URL(url);
        if (!["http:", "https:"].includes(parsedUrl.protocol)) {
          return res.status(400).json({ message: "URL must use HTTP or HTTPS protocol" });
        }
      } catch {
        return res.status(400).json({ message: "Invalid URL format" });
      }
      if (!await checkUsageLimit(userId, "website_monitor")) {
        return res.status(402).json({ message: "Free limit reached", feature: "website_monitor" });
      }
      const checksList = Array.isArray(checks) ? checks : ["performance", "security", "seo"];
      const startTime = Date.now();
      let realMetrics = {
        status: 0,
        responseTime: "0ms",
        ttfb: "0ms",
        contentType: "unknown",
        serverInfo: "unknown",
        contentLength: 0,
        https: url.startsWith("https"),
        headers: {},
        html: "",
        errors: []
      };
      try {
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), 15e3);
        const ttfbStart = Date.now();
        const response2 = await fetch(url, {
          method: "GET",
          headers: {
            "User-Agent": "Prometix-Monitor/1.0 (Website Analyzer)",
            "Accept": "text/html,application/xhtml+xml"
          },
          signal: controller.signal,
          redirect: "follow"
        });
        const ttfbEnd = Date.now();
        clearTimeout(timeoutId);
        const html = await response2.text();
        const endTime = Date.now();
        const responseHeaders = {};
        response2.headers.forEach((value, key) => {
          responseHeaders[key.toLowerCase()] = value;
        });
        realMetrics = {
          status: response2.status,
          responseTime: `${endTime - startTime}ms`,
          ttfb: `${ttfbEnd - ttfbStart}ms`,
          contentType: responseHeaders["content-type"] || "unknown",
          serverInfo: responseHeaders["server"] || "unknown",
          contentLength: html.length,
          https: url.startsWith("https"),
          headers: responseHeaders,
          html: html.slice(0, 5e4),
          errors: []
        };
      } catch (fetchError) {
        realMetrics.errors.push(fetchError.message || "Failed to fetch website");
        realMetrics.status = 0;
        realMetrics.responseTime = `${Date.now() - startTime}ms`;
      }
      const securityHeaders2 = {
        "x-frame-options": realMetrics.headers["x-frame-options"] || null,
        "x-content-type-options": realMetrics.headers["x-content-type-options"] || null,
        "strict-transport-security": realMetrics.headers["strict-transport-security"] || null,
        "content-security-policy": realMetrics.headers["content-security-policy"] || null,
        "x-xss-protection": realMetrics.headers["x-xss-protection"] || null,
        "referrer-policy": realMetrics.headers["referrer-policy"] || null
      };
      let titleMatch = realMetrics.html.match(/<title[^>]*>([^<]*)<\/title>/i);
      let descMatch = realMetrics.html.match(/<meta[^>]*name=["']description["'][^>]*content=["']([^"']*)/i);
      if (!descMatch) {
        descMatch = realMetrics.html.match(/<meta[^>]*content=["']([^"']*)[^>]*name=["']description["']/i);
      }
      const seoData = {
        title: titleMatch ? titleMatch[1].trim() : "",
        description: descMatch ? descMatch[1].trim() : "",
        hasH1: /<h1[^>]*>/i.test(realMetrics.html),
        hasCanonical: /<link[^>]*rel=["']canonical["']/i.test(realMetrics.html),
        hasViewport: /<meta[^>]*name=["']viewport["']/i.test(realMetrics.html),
        hasOpenGraph: /<meta[^>]*property=["']og:/i.test(realMetrics.html)
      };
      const prompt = `You are a website analysis expert. Analyze this website based on REAL data collected from the site.

Website URL: ${url}
Analysis Focus: ${checksList.join(", ")}

REAL MEASURED DATA:
- HTTP Status: ${realMetrics.status}
- Response Time (Total): ${realMetrics.responseTime}
- Time to First Byte (TTFB): ${realMetrics.ttfb}
- Content Type: ${realMetrics.contentType}
- Server: ${realMetrics.serverInfo}
- Content Length: ${realMetrics.contentLength} bytes
- HTTPS: ${realMetrics.https}
- Fetch Errors: ${realMetrics.errors.length > 0 ? realMetrics.errors.join(", ") : "None"}

SECURITY HEADERS FOUND:
- X-Frame-Options: ${securityHeaders2["x-frame-options"] || "MISSING"}
- X-Content-Type-Options: ${securityHeaders2["x-content-type-options"] || "MISSING"}
- Strict-Transport-Security: ${securityHeaders2["strict-transport-security"] || "MISSING"}
- Content-Security-Policy: ${securityHeaders2["content-security-policy"] ? "Present" : "MISSING"}
- X-XSS-Protection: ${securityHeaders2["x-xss-protection"] || "MISSING"}
- Referrer-Policy: ${securityHeaders2["referrer-policy"] || "MISSING"}

SEO DATA:
- Title: ${seoData.title || "MISSING"}
- Meta Description: ${seoData.description || "MISSING"}
- Has H1 Tag: ${seoData.hasH1}
- Has Canonical Link: ${seoData.hasCanonical}
- Has Viewport Meta: ${seoData.hasViewport}
- Has Open Graph Tags: ${seoData.hasOpenGraph}

Based on this REAL data, provide a comprehensive analysis. Calculate scores based on actual measurements.
Performance: Score based on actual response times (< 500ms = excellent, < 1000ms = good, < 2000ms = fair, > 2000ms = poor)
Security: Score based on actual headers present (each missing critical header = -15 points from 100)
SEO: Score based on actual meta tags present

Return JSON with this structure:
{
  "overallScore": number (0-100),
  "status": "healthy" | "warning" | "critical",
  "performance": {
    "loadTime": "${realMetrics.responseTime}",
    "ttfb": "${realMetrics.ttfb}",
    "fcp": "estimated from data",
    "lcp": "estimated from data",
    "score": number,
    "recommendations": ["specific recommendations based on actual data"]
  },
  "security": {
    "score": number,
    "https": ${realMetrics.https},
    "headers": [{"name": "header name", "status": "pass|fail|warn", "value": "actual value or MISSING"}],
    "issues": [{"severity": "critical|high|medium|low", "issue": "specific issue", "fix": "how to fix"}]
  },
  "seo": {
    "score": number,
    "title": "${seoData.title.replace(/"/g, '\\"') || "Not found"}",
    "description": "${seoData.description.slice(0, 200).replace(/"/g, '\\"') || "Not found"}",
    "issues": ["specific issues found"],
    "recommendations": ["specific recommendations"]
  },
  "accessibility": {
    "score": number (estimate based on HTML structure),
    "issues": [{"severity": "medium", "issue": "potential issue", "element": "element type"}]
  },
  "summary": "brief summary based on real findings"
}`;
      const genAI2 = new GoogleGenerativeAI2(process.env.GOOGLE_API_KEY || process.env.OPENAI_API_KEY || "");
      const model = genAI2.getGenerativeModel({ model: "gemini-1.5-flash" });
      const result = await model.generateContent(prompt);
      const response = await result.response;
      const text2 = response.text();
      let parsed;
      try {
        const jsonMatch = text2.match(/\{[\s\S]*\}/);
        parsed = jsonMatch ? JSON.parse(jsonMatch[0]) : { url, summary: text2, overallScore: 75 };
      } catch {
        let secScore = 100;
        if (!securityHeaders2["x-frame-options"]) secScore -= 15;
        if (!securityHeaders2["strict-transport-security"]) secScore -= 20;
        if (!securityHeaders2["content-security-policy"]) secScore -= 15;
        if (!realMetrics.https) secScore -= 25;
        let seoScore = 100;
        if (!seoData.title) seoScore -= 30;
        if (!seoData.description) seoScore -= 25;
        if (!seoData.hasH1) seoScore -= 15;
        if (!seoData.hasViewport) seoScore -= 15;
        const responseMs = parseInt(realMetrics.responseTime) || 1e3;
        let perfScore = responseMs < 500 ? 95 : responseMs < 1e3 ? 80 : responseMs < 2e3 ? 60 : 40;
        parsed = {
          url,
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          overallScore: Math.round((perfScore + secScore + seoScore) / 3),
          status: realMetrics.status >= 200 && realMetrics.status < 400 ? "healthy" : "critical",
          performance: {
            loadTime: realMetrics.responseTime,
            ttfb: realMetrics.ttfb,
            fcp: "N/A",
            lcp: "N/A",
            score: perfScore,
            recommendations: []
          },
          security: {
            score: Math.max(0, secScore),
            https: realMetrics.https,
            headers: Object.entries(securityHeaders2).map(([name, value]) => ({
              name: name.toUpperCase().replace(/-/g, "_"),
              status: value ? "pass" : "fail",
              value: value || "MISSING"
            })),
            issues: []
          },
          seo: {
            score: Math.max(0, seoScore),
            title: seoData.title,
            description: seoData.description,
            issues: [],
            recommendations: []
          },
          accessibility: { score: 70, issues: [] },
          logs: {
            status: realMetrics.status,
            responseTime: realMetrics.responseTime,
            contentType: realMetrics.contentType,
            serverInfo: realMetrics.serverInfo,
            errors: realMetrics.errors
          },
          summary: text2
        };
      }
      parsed.logs = {
        status: realMetrics.status,
        responseTime: realMetrics.responseTime,
        contentType: realMetrics.contentType,
        serverInfo: realMetrics.serverInfo,
        errors: realMetrics.errors
      };
      parsed.url = url;
      parsed.timestamp = (/* @__PURE__ */ new Date()).toISOString();
      await deductCreditForUsage(userId, "website_monitor");
      logger.info(`Website analysis by user ${userId}: ${url} - Status: ${realMetrics.status}, Time: ${realMetrics.responseTime}`);
      res.json({ result: parsed });
    } catch (error) {
      logger.error("Website analyzer error", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.get("/api/projects", requireAuth, projectsCacheMiddleware, async (req, res) => {
    try {
      const userId = req.user.id;
      const projects2 = await storage.getProjectsByUserId(userId);
      logger.info(`Retrieved ${projects2.length} projects for user ${userId}`);
      res.json(projects2);
    } catch (error) {
      logger.error("Error fetching projects", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.get("/api/projects/:id", requireAuth, async (req, res) => {
    try {
      const { id } = req.params;
      const project = await storage.getProject(parseInt(id));
      if (!project) {
        return res.status(404).json({ message: "Project not found" });
      }
      if (project.userId !== req.user.id) {
        return res.status(403).json({ message: "Access denied" });
      }
      res.json(project);
    } catch (error) {
      res.status(500).json({ message: error.message });
    }
  });
  app.put("/api/projects/:id", requireAuth, invalidateCacheMiddleware(["projects:user::userId"]), async (req, res) => {
    try {
      const { id } = req.params;
      const updates = req.body;
      const project = await storage.getProject(parseInt(id));
      if (!project || project.userId !== req.user.id) {
        return res.status(404).json({ message: "Project not found" });
      }
      const updatedProject = await storage.updateProject(parseInt(id), updates);
      logger.info(`Updated project ${id} for user ${req.user.id}`);
      res.json(updatedProject);
    } catch (error) {
      logger.error("Error updating project", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.get("/api/credits/packages", (req, res) => {
    res.json(creditPackages);
  });
  app.post("/api/payment/create-order", requireAuth, async (req, res) => {
    try {
      const { packageId } = req.body;
      const userId = req.user.id;
      const order = await createPaymentOrder(userId, packageId);
      res.json(order);
    } catch (error) {
      res.status(500).json({ message: error.message });
    }
  });
  app.post("/api/payment/verify", requireAuth, async (req, res) => {
    try {
      const { razorpay_order_id, razorpay_payment_id, razorpay_signature } = req.body;
      await processSuccessfulPayment(razorpay_payment_id, razorpay_order_id, razorpay_signature);
      res.json({ message: "Payment verified successfully" });
    } catch (error) {
      res.status(400).json({ message: error.message });
    }
  });
  app.post("/api/sonar/analyze", requireAdmin, async (req, res) => {
    try {
      const { sonarService: sonarService2 } = await Promise.resolve().then(() => (init_sonar_service(), sonar_service_exports));
      const result = await sonarService2.runAnalysis();
      logger.info("SonarQube analysis initiated", {
        userId: req.user.id,
        success: result.success,
        taskId: result.taskId
      });
      res.json(result);
    } catch (error) {
      logger.error("SonarQube analysis failed", {
        error: error instanceof Error ? error.message : String(error),
        userId: req.user.id
      });
      trackError("sonar_analysis_failed", "/api/sonar/analyze", String(req.user?.id || "unknown"));
      res.status(500).json({
        error: "Analysis failed",
        message: "Please try again later"
      });
    }
  });
  app.get("/api/sonar/metrics", requireAdmin, async (req, res) => {
    try {
      const { sonarService: sonarService2 } = await Promise.resolve().then(() => (init_sonar_service(), sonar_service_exports));
      const metrics = await sonarService2.getProjectMetrics();
      if (!metrics) {
        return res.status(404).json({
          error: "No metrics available",
          message: "Please run an analysis first"
        });
      }
      logger.info("SonarQube metrics retrieved", {
        userId: req.user.id,
        overallRating: metrics.overallRating
      });
      res.json(metrics);
    } catch (error) {
      logger.error("Failed to get SonarQube metrics", {
        error: error instanceof Error ? error.message : String(error),
        userId: req.user.id
      });
      res.status(500).json({
        error: "Failed to get metrics",
        message: "Please try again later"
      });
    }
  });
  app.get("/api/sonar/issues", requireAdmin, async (req, res) => {
    try {
      const { sonarService: sonarService2 } = await Promise.resolve().then(() => (init_sonar_service(), sonar_service_exports));
      const results = await sonarService2.getAnalysisResults();
      if (!results) {
        return res.status(404).json({
          error: "No analysis results available",
          message: "Please run an analysis first"
        });
      }
      logger.info("SonarQube issues retrieved", {
        userId: req.user.id,
        issueCount: results.issues.length
      });
      res.json(results.issues);
    } catch (error) {
      logger.error("Failed to get SonarQube issues", {
        error: error instanceof Error ? error.message : String(error),
        userId: req.user.id
      });
      res.status(500).json({
        error: "Failed to get issues",
        message: "Please try again later"
      });
    }
  });
  app.get("/api/usage", requireAuth, usageCacheMiddleware, async (req, res) => {
    try {
      const userId = req.user.id;
      const features = ["api_generation", "docker_generation", "cicd_generation", "ai_assistance"];
      const usage2 = await Promise.all(
        features.map(async (feature) => ({
          feature,
          usage: await storage.getUsage(userId, feature)
        }))
      );
      logger.info(`Retrieved usage data for user ${userId}`);
      res.json(usage2);
    } catch (error) {
      logger.error("Error fetching usage data", error);
      res.status(500).json({ message: error.message });
    }
  });
  app.get("/api/logs", requireAdmin, async (req, res) => {
    try {
      const { type = "app", lines = 100 } = req.query;
      const logsDir = join2(process.cwd(), "logs");
      let filename = "";
      switch (type) {
        case "error":
          filename = "error.log";
          break;
        case "access":
          filename = "access.log";
          break;
        default:
          filename = "app.log";
      }
      const logPath = join2(logsDir, filename);
      if (!existsSync2(logPath)) {
        return res.json({ logs: [], message: `No ${type} logs found` });
      }
      const logContent = readFileSync(logPath, "utf-8");
      const logLines = logContent.split("\n").filter((line) => line.trim() !== "");
      const recentLogs = logLines.slice(-Number(lines));
      logger.info("Logs viewed", { type, lines: recentLogs.length }, req.user.id);
      res.json({
        logs: recentLogs,
        total: logLines.length,
        showing: recentLogs.length,
        type: filename
      });
    } catch (error) {
      logger.error("Error reading logs", error, req.user?.id);
      res.status(500).json({ message: "Failed to read logs" });
    }
  });
  app.get("/api/logs/stats", requireAdmin, async (req, res) => {
    try {
      const logsDir = join2(process.cwd(), "logs");
      const stats = {
        app: { exists: false, size: 0, lines: 0 },
        error: { exists: false, size: 0, lines: 0 },
        access: { exists: false, size: 0, lines: 0 }
      };
      ["app.log", "error.log", "access.log"].forEach((filename, index) => {
        const logPath = join2(logsDir, filename);
        const type = ["app", "error", "access"][index];
        if (existsSync2(logPath)) {
          const content = readFileSync(logPath, "utf-8");
          const fileStats = statSync(logPath);
          stats[type] = {
            exists: true,
            size: fileStats.size,
            lines: content.split("\n").length - 1
          };
        }
      });
      res.json(stats);
    } catch (error) {
      logger.error("Error getting log stats", error, req.user?.id);
      res.status(500).json({ message: "Failed to get log statistics" });
    }
  });
  app.get("/metrics", requireAdmin, async (req, res) => {
    try {
      res.set("Content-Type", register2.contentType);
      res.end(await register2.metrics());
    } catch (error) {
      logger.error("Error generating metrics", { error });
      res.status(500).json({ error: "Failed to generate metrics" });
    }
  });
  app.get("/api/admin/logs", requireAdmin, async (req, res) => {
    try {
      const { level = "all", limit = 100 } = req.query;
      const logsDir = join2(process.cwd(), "logs");
      const logPath = join2(logsDir, "app.log");
      let logs = [];
      if (existsSync2(logPath)) {
        const logContent = readFileSync(logPath, "utf-8");
        const logLines = logContent.split("\n").filter((line) => line.trim() !== "");
        logs = logLines.slice(-Number(limit)).map((line, index) => {
          try {
            const logData = JSON.parse(line);
            return {
              id: `${Date.now()}-${index}`,
              timestamp: logData.timestamp || (/* @__PURE__ */ new Date()).toISOString(),
              level: logData.level || "INFO",
              message: logData.message || line,
              userId: logData.userId || null,
              route: logData.route || null,
              duration: logData.duration || null,
              metadata: logData
            };
          } catch {
            return {
              id: `${Date.now()}-${index}`,
              timestamp: (/* @__PURE__ */ new Date()).toISOString(),
              level: "INFO",
              message: line,
              userId: null,
              route: null,
              duration: null
            };
          }
        });
      }
      const filteredLogs = level === "all" ? logs : logs.filter((log2) => log2.level === level);
      res.json(filteredLogs);
    } catch (error) {
      logger.error("Error fetching logs", { error });
      res.status(500).json({ error: "Failed to fetch logs" });
    }
  });
  app.get("/api/admin/metrics", requireAdmin, async (req, res) => {
    try {
      const userCount = await storage.getUserCount();
      const metrics = [
        {
          name: "Total Users",
          value: userCount,
          unit: "",
          status: "good"
        },
        {
          name: "Active Sessions",
          value: 1,
          // Current logged in user
          unit: "",
          status: "good"
        },
        {
          name: "System Uptime",
          value: Math.round(process.uptime() / 3600),
          unit: "hours",
          status: "good"
        },
        {
          name: "Memory Usage",
          value: Math.round(process.memoryUsage().heapUsed / process.memoryUsage().heapTotal * 100),
          unit: "%",
          status: "good"
        },
        {
          name: "API Response Time",
          value: 245,
          unit: "ms",
          change: -12,
          status: "good"
        },
        {
          name: "Error Rate",
          value: 0.5,
          unit: "%",
          status: "good"
        }
      ];
      res.json(metrics);
    } catch (error) {
      logger.error("Error fetching metrics", { error });
      res.status(500).json({ error: "Failed to fetch metrics" });
    }
  });
  app.get("/api/admin/health", requireAdmin, async (req, res) => {
    try {
      const memoryUsage = process.memoryUsage();
      const uptime = process.uptime();
      const memoryPercent = Math.round(memoryUsage.heapUsed / memoryUsage.heapTotal * 100);
      let databaseStatus = "healthy";
      try {
        await storage.getUserCount();
      } catch (error) {
        databaseStatus = "error";
      }
      let redisStatus = "warning";
      try {
        if (redis && redis.isConnected()) {
          redisStatus = "healthy";
        }
      } catch (error) {
        redisStatus = "error";
      }
      const systemHealth = {
        cpu: Math.round(Math.random() * 30 + 20),
        // Mock CPU usage between 20-50%
        memory: memoryPercent,
        disk: 45,
        // Mock value
        database: databaseStatus,
        redis: redisStatus,
        status: databaseStatus === "error" ? "down" : redisStatus === "error" ? "degraded" : "healthy",
        uptime: Math.round(uptime),
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      };
      res.json(systemHealth);
    } catch (error) {
      logger.error("Error checking system health", { error });
      res.status(500).json({ error: "Failed to check system health" });
    }
  });
  app.get("/api/admin/logs/export", requireAdmin, async (req, res) => {
    try {
      const logsDir = join2(process.cwd(), "logs");
      const logPath = join2(logsDir, "app.log");
      let logs = [];
      if (existsSync2(logPath)) {
        const logContent = readFileSync(logPath, "utf-8");
        logs = logContent.split("\n").filter((line) => line.trim() !== "");
      }
      res.setHeader("Content-Type", "application/json");
      res.setHeader("Content-Disposition", `attachment; filename=logs-${(/* @__PURE__ */ new Date()).toISOString().split("T")[0]}.json`);
      res.json({
        exported_at: (/* @__PURE__ */ new Date()).toISOString(),
        exported_by: req.user?.email,
        logs
      });
    } catch (error) {
      logger.error("Error exporting logs", { error });
      res.status(500).json({ error: "Failed to export logs" });
    }
  });
  app.post("/api/jenkins/generate", requireAuth, apiRateLimit, requireCredits(1), async (req, res) => {
    try {
      const { generateJenkinsPipeline: generateJenkinsPipeline2 } = await Promise.resolve().then(() => (init_jenkins_generator(), jenkins_generator_exports));
      const config = req.body;
      const script = generateJenkinsPipeline2(config);
      await deductCreditForUsage(req.user.id, "jenkins_generation");
      logger.info(`Jenkins pipeline generated for user ${req.user.id}`, { projectName: config.projectName });
      res.json({
        script,
        filename: `Jenkinsfile-${config.projectName || "pipeline"}`,
        type: "jenkins-pipeline"
      });
    } catch (error) {
      logger.error("Jenkins generation error", error);
      res.status(500).json({ error: "Failed to generate Jenkins pipeline" });
    }
  });
  app.post("/api/ansible/generate", requireAuth, apiRateLimit, requireCredits(1), async (req, res) => {
    try {
      const { generateAnsiblePlaybook: generateAnsiblePlaybook2 } = await Promise.resolve().then(() => (init_ansible_generator(), ansible_generator_exports));
      const config = req.body;
      const playbook = generateAnsiblePlaybook2(config);
      await deductCreditForUsage(req.user.id, "ansible_generation");
      logger.info(`Ansible playbook generated for user ${req.user.id}`, { playbookName: config.playbookName });
      res.json({
        playbook,
        filename: `${config.playbookName || "playbook"}.yml`,
        type: "ansible-playbook"
      });
    } catch (error) {
      logger.error("Ansible generation error", error);
      res.status(500).json({ error: "Failed to generate Ansible playbook" });
    }
  });
  app.post("/api/sonarqube/setup", requireAuth, apiRateLimit, requireCredits(1), async (req, res) => {
    try {
      const { generateSonarQubeSetup: generateSonarQubeSetup2 } = await Promise.resolve().then(() => (init_sonarqube_generator(), sonarqube_generator_exports));
      const config = req.body;
      const script = generateSonarQubeSetup2(config);
      await deductCreditForUsage(req.user.id, "sonarqube_setup");
      logger.info(`SonarQube setup generated for user ${req.user.id}`, { setupType: config.setupType });
      res.json({
        script,
        filename: `sonarqube-setup-${config.setupType || "docker"}.${config.setupType === "kubernetes" ? "yaml" : "sh"}`,
        type: "sonarqube-setup"
      });
    } catch (error) {
      logger.error("SonarQube setup generation error", error);
      res.status(500).json({ error: "Failed to generate SonarQube setup" });
    }
  });
  app.post("/api/snowflake/generate", requireAuth, apiRateLimit, requireCredits(1), async (req, res) => {
    try {
      const { generateSnowflakeSetup: generateSnowflakeSetup2 } = await Promise.resolve().then(() => (init_snowflake_generator(), snowflake_generator_exports));
      const config = req.body;
      const script = generateSnowflakeSetup2(config);
      await deductCreditForUsage(req.user.id, "snowflake_setup");
      logger.info(`Snowflake setup generated for user ${req.user.id}`, { databaseName: config.databaseName });
      res.json({
        script,
        filename: `snowflake-setup-${config.databaseName || "config"}.sql`,
        type: "snowflake-setup"
      });
    } catch (error) {
      logger.error("Snowflake setup generation error", error);
      res.status(500).json({ error: "Failed to generate Snowflake setup" });
    }
  });
  app.post("/api/airflow/generate", requireAuth, apiRateLimit, requireCredits(1), async (req, res) => {
    try {
      const { generateAirflowDAG: generateAirflowDAG2 } = await Promise.resolve().then(() => (init_airflow_generator(), airflow_generator_exports));
      const config = req.body;
      const script = generateAirflowDAG2(config);
      await deductCreditForUsage(req.user.id, "airflow_generation");
      logger.info(`Airflow DAG generated for user ${req.user.id}`, { dagId: config.dagId });
      res.json({
        script,
        filename: `${config.dagId || "dag"}.py`,
        type: "airflow-dag"
      });
    } catch (error) {
      logger.error("Airflow DAG generation error", error);
      res.status(500).json({ error: "Failed to generate Airflow DAG" });
    }
  });
  const PRIMARY_ADMIN_EMAIL2 = "agrawalmayank200228@gmail.com";
  app.post("/api/admin/impersonate/start", requireAuth, async (req, res) => {
    try {
      const { email, reason } = req.body;
      const impersonatorId = req.user.id;
      const impersonatorEmail = req.user.email;
      if (!email) {
        return res.status(400).json({ error: "Target user email is required" });
      }
      const isPrimaryAdmin = impersonatorEmail === PRIMARY_ADMIN_EMAIL2;
      const isAdmin2 = req.user.isAdmin || isPrimaryAdmin;
      const canImpersonate = req.user.canImpersonate;
      if (!isAdmin2) {
        return res.status(403).json({ error: "Admin access required" });
      }
      if (!isPrimaryAdmin && !canImpersonate) {
        return res.status(403).json({ error: "You don't have impersonation privileges" });
      }
      const targetUser = await storage.getUserByEmail(email);
      if (!targetUser) {
        return res.status(404).json({ error: "User not found" });
      }
      if (targetUser.id === impersonatorId) {
        return res.status(400).json({ error: "Cannot impersonate yourself" });
      }
      await db.insert(impersonationLogs).values({
        impersonatorId,
        targetUserId: targetUser.id,
        reason: reason || null,
        action: "start",
        ipAddress: req.ip || null,
        userAgent: req.headers["user-agent"] || null
      });
      req.session.originalUserId = impersonatorId;
      req.session.originalUserEmail = impersonatorEmail;
      req.session.isImpersonating = true;
      req.login(targetUser, (err) => {
        if (err) {
          logger.error("Failed to switch user", { error: err, impersonatorId, targetUserId: targetUser.id });
          return res.status(500).json({ error: "Failed to switch user" });
        }
        logger.info(`User ${impersonatorEmail} started impersonating ${email}`, { impersonatorId, targetUserId: targetUser.id, reason });
        res.json({ success: true, user: targetUser, isImpersonating: true });
      });
    } catch (error) {
      logger.error("Impersonation start error", error);
      res.status(500).json({ error: "Failed to start impersonation" });
    }
  });
  app.post("/api/admin/impersonate/end", requireAuth, async (req, res) => {
    try {
      const session2 = req.session;
      if (!session2.isImpersonating || !session2.originalUserId) {
        return res.status(400).json({ error: "Not currently impersonating" });
      }
      const originalUserId = session2.originalUserId;
      const targetUserId = req.user.id;
      await db.insert(impersonationLogs).values({
        impersonatorId: originalUserId,
        targetUserId,
        action: "end",
        ipAddress: req.ip || null,
        userAgent: req.headers["user-agent"] || null
      });
      const originalUser = await storage.getUser(originalUserId);
      if (!originalUser) {
        return res.status(500).json({ error: "Original user not found" });
      }
      delete session2.originalUserId;
      delete session2.originalUserEmail;
      delete session2.isImpersonating;
      req.login(originalUser, (err) => {
        if (err) {
          logger.error("Failed to restore session", { error: err, originalUserId });
          return res.status(500).json({ error: "Failed to restore session" });
        }
        logger.info(`User ${originalUser.email} ended impersonation of user ${targetUserId}`);
        res.json({ success: true, user: originalUser });
      });
    } catch (error) {
      logger.error("Impersonation end error", error);
      res.status(500).json({ error: "Failed to end impersonation" });
    }
  });
  app.get("/api/admin/impersonation-status", requireAuth, (req, res) => {
    const session2 = req.session;
    res.json({
      isImpersonating: !!session2.isImpersonating,
      originalUserEmail: session2.originalUserEmail || null
    });
  });
  app.post("/api/admin/grant-impersonate", requireAuth, async (req, res) => {
    try {
      const { userId } = req.body;
      const granterEmail = req.user.email;
      const granterId = req.user.id;
      if (granterEmail !== PRIMARY_ADMIN_EMAIL2) {
        return res.status(403).json({ error: "Only primary admin can grant impersonation privileges" });
      }
      if (!userId || typeof userId !== "number") {
        return res.status(400).json({ error: "Valid user ID is required" });
      }
      if (userId === granterId) {
        return res.status(400).json({ error: "Cannot grant impersonation to yourself" });
      }
      const targetUser = await storage.getUser(userId);
      if (!targetUser) {
        return res.status(404).json({ error: "User not found" });
      }
      if (!targetUser.isAdmin && targetUser.email !== PRIMARY_ADMIN_EMAIL2) {
        return res.status(400).json({ error: "Target user must be an admin" });
      }
      await db.update(users).set({ canImpersonate: true }).where(eq3(users.id, userId));
      logger.info(`Impersonation privilege granted to user ${userId} by ${granterEmail}`);
      res.json({ success: true });
    } catch (error) {
      logger.error("Grant impersonate error", error);
      res.status(500).json({ error: "Failed to grant impersonation privilege" });
    }
  });
  app.post("/api/admin/revoke-impersonate", requireAuth, async (req, res) => {
    try {
      const { userId } = req.body;
      const granterEmail = req.user.email;
      if (granterEmail !== PRIMARY_ADMIN_EMAIL2) {
        return res.status(403).json({ error: "Only primary admin can revoke impersonation privileges" });
      }
      if (!userId || typeof userId !== "number") {
        return res.status(400).json({ error: "Valid user ID is required" });
      }
      const targetUser = await storage.getUser(userId);
      if (!targetUser) {
        return res.status(404).json({ error: "User not found" });
      }
      await db.update(users).set({ canImpersonate: false }).where(eq3(users.id, userId));
      logger.info(`Impersonation privilege revoked from user ${userId} by ${granterEmail}`);
      res.json({ success: true });
    } catch (error) {
      logger.error("Revoke impersonate error", error);
      res.status(500).json({ error: "Failed to revoke impersonation privilege" });
    }
  });
  app.post("/api/admin/ban-user", requireAuth, async (req, res) => {
    try {
      const { userId, reason } = req.body;
      const adminEmail = req.user.email;
      const isAdmin2 = req.user.isAdmin || adminEmail === PRIMARY_ADMIN_EMAIL2;
      if (!isAdmin2) {
        return res.status(403).json({ error: "Admin access required" });
      }
      if (!userId || typeof userId !== "number") {
        return res.status(400).json({ error: "Valid user ID is required" });
      }
      if (userId === req.user.id) {
        return res.status(400).json({ error: "Cannot ban yourself" });
      }
      const targetUser = await storage.getUser(userId);
      if (!targetUser) {
        return res.status(404).json({ error: "User not found" });
      }
      if (targetUser.email === PRIMARY_ADMIN_EMAIL2) {
        return res.status(403).json({ error: "Cannot ban the primary admin" });
      }
      await storage.banUser(userId, reason || "No reason provided");
      logger.info(`User ${userId} banned by ${adminEmail}`, { reason });
      res.json({ success: true });
    } catch (error) {
      logger.error("Ban user error", error);
      res.status(500).json({ error: "Failed to ban user" });
    }
  });
  app.post("/api/admin/unban-user", requireAuth, async (req, res) => {
    try {
      const { userId } = req.body;
      const adminEmail = req.user.email;
      const isAdmin2 = req.user.isAdmin || adminEmail === PRIMARY_ADMIN_EMAIL2;
      if (!isAdmin2) {
        return res.status(403).json({ error: "Admin access required" });
      }
      if (!userId || typeof userId !== "number") {
        return res.status(400).json({ error: "Valid user ID is required" });
      }
      const targetUser = await storage.getUser(userId);
      if (!targetUser) {
        return res.status(404).json({ error: "User not found" });
      }
      await storage.unbanUser(userId);
      logger.info(`User ${userId} unbanned by ${adminEmail}`);
      res.json({ success: true });
    } catch (error) {
      logger.error("Unban user error", error);
      res.status(500).json({ error: "Failed to unban user" });
    }
  });
  app.post("/api/admin/update-credits", requireAuth, async (req, res) => {
    try {
      const { email, credits } = req.body;
      const adminEmail = req.user.email;
      const isAdmin2 = req.user.isAdmin || adminEmail === PRIMARY_ADMIN_EMAIL2;
      if (!isAdmin2) {
        return res.status(403).json({ error: "Admin access required" });
      }
      if (!email || typeof credits !== "number") {
        return res.status(400).json({ error: "Email and credits amount required" });
      }
      const targetUser = await storage.getUserByEmail(email);
      if (!targetUser) {
        return res.status(404).json({ error: "User not found" });
      }
      await storage.updateUserCredits(targetUser.id, credits);
      logger.info(`Credits updated for ${email} to ${credits} by ${adminEmail}`);
      res.json({ success: true });
    } catch (error) {
      logger.error("Update credits error", error);
      res.status(500).json({ error: "Failed to update credits" });
    }
  });
  app.put("/api/admin/domain-configs/:domain", requireAuth, async (req, res) => {
    try {
      const { domain } = req.params;
      const { isEnabled, comingSoonMessage } = req.body;
      const adminEmail = req.user.email;
      const isAdmin2 = req.user.isAdmin || adminEmail === PRIMARY_ADMIN_EMAIL2;
      if (!isAdmin2) {
        return res.status(403).json({ error: "Admin access required" });
      }
      if (typeof isEnabled !== "boolean") {
        return res.status(400).json({ error: "isEnabled must be a boolean" });
      }
      const updatedConfig = await storage.updateDomainConfig(
        domain,
        isEnabled,
        comingSoonMessage || null,
        req.user.id
      );
      logger.info(`Domain ${domain} config updated by ${adminEmail}`, { isEnabled, comingSoonMessage });
      res.json(updatedConfig);
    } catch (error) {
      logger.error("Update domain config error", error);
      res.status(500).json({ error: "Failed to update domain config" });
    }
  });
  app.post("/api/chatbot", requireAuth, async (req, res) => {
    try {
      const { message } = req.body;
      const userId = req.user.id;
      if (!message || typeof message !== "string" || message.length < 2) {
        return res.status(400).json({ error: "Message is required" });
      }
      if (message.length > 2e3) {
        return res.status(400).json({ error: "Message too long (max 2000 characters)" });
      }
      const gemini = await Promise.resolve().then(() => (init_gemini(), gemini_exports));
      const systemPrompt = `You are Prometix AI Assistant, a helpful DevOps and cloud infrastructure expert. You help users with:
- DevOps best practices and tooling
- Docker, Kubernetes, CI/CD pipelines
- Cloud infrastructure (AWS, GCP, Azure)
- Infrastructure as Code (Terraform, Ansible)
- Monitoring and observability
- Security best practices

Be concise, practical, and provide code examples when helpful. Focus on production-ready solutions.`;
      const response = await gemini.generateChatResponse(systemPrompt, message);
      logger.info(`Chatbot query from user ${userId}`, { messageLength: message.length });
      res.json({ response });
    } catch (error) {
      logger.error("Chatbot error", error);
      res.status(500).json({ error: "Failed to generate response" });
    }
  });
  const httpServer = createServer(app);
  return httpServer;
}
var init_routes = __esm({
  "server/routes.ts"() {
    "use strict";
    init_auth();
    init_storage();
    init_db();
    init_schema();
    init_code_generator();
    init_gemini();
    init_payment();
    init_logger();
    init_redis();
    init_cache();
    init_admin();
    init_jobs_service();
    init_interview_questions();
    init_admin_guard();
    init_push_notifications();
    init_prometheus();
    init_security();
    init_auth_guard();
  }
});

// server/utils.ts
function log(message, source = "express") {
  const formattedTime = (/* @__PURE__ */ new Date()).toLocaleTimeString("en-US", {
    hour: "numeric",
    minute: "2-digit",
    second: "2-digit",
    hour12: true
  });
  console.log(`${formattedTime} [${source}] ${message}`);
}
var init_utils = __esm({
  "server/utils.ts"() {
    "use strict";
  }
});

// server/app.ts
var app_exports = {};
__export(app_exports, {
  createApp: () => createApp
});
import express from "express";
async function createApp() {
  const app = express();
  app.use(express.json());
  app.use(express.urlencoded({ extended: false }));
  app.use((req, res, next) => {
    const start = Date.now();
    const path2 = req.path;
    let capturedJsonResponse = void 0;
    const originalResJson = res.json;
    res.json = function(bodyJson, ...args) {
      capturedJsonResponse = bodyJson;
      return originalResJson.apply(res, [bodyJson, ...args]);
    };
    res.on("finish", () => {
      const duration = Date.now() - start;
      if (path2.startsWith("/api")) {
        let logLine = `${req.method} ${path2} ${res.statusCode} in ${duration}ms`;
        if (capturedJsonResponse) {
          logLine += ` :: ${JSON.stringify(capturedJsonResponse)}`;
        }
        if (logLine.length > 80) {
          logLine = logLine.slice(0, 79) + "\u2026";
        }
        log(logLine);
        const userId = req.user?.id;
        logger.access(req.method, path2, res.statusCode, duration, userId, capturedJsonResponse);
      }
    });
    next();
  });
  if (!redis.isConnected()) {
    await redis.connect();
    logger.info("Redis service initialized");
  }
  const server = await registerRoutes(app);
  app.use((err, req, res, _next) => {
    const status = err.status || err.statusCode || 500;
    const message = err.message || "Internal Server Error";
    const userId = req.user?.id;
    logger.error(`${req.method} ${req.path} - ${message}`, err, userId);
    res.status(status).json({ message });
    throw err;
  });
  return { app, server };
}
var init_app = __esm({
  "server/app.ts"() {
    "use strict";
    init_routes();
    init_utils();
    init_logger();
    init_redis();
  }
});

// api/index.ts
var appCache = null;
async function handler(req, res) {
  try {
    if (!appCache) {
      const { createApp: createApp2 } = await Promise.resolve().then(() => (init_app(), app_exports));
      const { app } = await createApp2();
      appCache = app;
    }
    appCache(req, res);
  } catch (error) {
    console.error("Critical Server Startup Error:", error);
    res.status(500).json({
      error: "Server Startup Failed",
      details: error.message,
      code: "STARTUP_ERROR",
      stack: process.env.NODE_ENV === "development" ? error.stack : void 0
    });
  }
}
export {
  handler as default
};
